Dans cet article, nous nous intéressons à un problème d'apprentissage actif consistant à déduire le modèle de transition d'un Processus de Décision Markovien (MDP) en agissant et en observant les transitions résultantes. Ceci est particulièrement utile lorsque la fonction de récompense n'est pas initialement accessible. Notre proposition consiste à formuler ce problème d'apprentissage actif en un problème de maximisation d'utilité dans le cadre de l'apprentissage par renforcement bayésien avec des récompenses dépendant de l'état de croyance. Après avoir présenté trois critères de performance possibles, nous en dérivons des récompenses dépendant de l'état de croyance que l'on pourra utiliser dans le processus de prise de décision. Comme le calcul de la fonction de valeur bayésienne optimale n'est pas envisageable pour de larges horizons, nous utilisons un algorithme simple pour résoudre de manière approchée ce problème d'optimisation. Malgré le fait que la solution est sous- optimale, nous montrons expérimentalement que notre proposition est néanmoins efficace dans un certain nombre de domaines.