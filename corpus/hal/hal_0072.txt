L’effondrement des prix de stockage de l’information, la couverture croissante des usages informatiques et des collectes de données qui y sont associées ainsi que l’accroissement des capacités de traitement de l’information sont autant de bouleversements techniques dans le domaine de l’information. Que l’on parle de Big Data, ou que l’on considère simplement les conséquences de la numérisation lors de la crise sanitaire ces deux dernières années, la collecte généralisée de données sensibles est un nouvel enjeu de notre société. À titre d’exemple, un téléphone récolte le généralement la position instantanée, les relations, les heures de sommeil, les questions et autres données de santé de son utilisateur. La nécessité de sécuriser et d’éviter les fuites de données, qu’elles soient malicieuses ou non, est donc un enjeu clé de la transition numérique. Mais comment peut-on garantir la privacy ? Ce concept a de nombreuses facettes : offuscation, droit à l’oubli, anonymat, confidentialité, minimisation des données. Dans le cadre de l’apprentissage automatique (Machine learning), une métrique s’est imposée au sein de la recherche et des applications des GAFAM pour quantifier le niveau de privacy d’un procédé donné. La confidentialité différentielle (differential privacy) est en effet une définition mathématique qui réduit à un nombre réel le niveau de persistance d’une donnée dans les sorties d’un algorithme. Ce mémoire décrit l’émergence et les facteurs qui ont contribué au succès de cette quantification, ainsi que les conséquences implicites de cette définition sur les attentes de l’apprentissage automatique et le rapport entre l’individu et ses données. Nous abordons donc l’évolution de la notion de privacy face aux nouvelles réalités techniques, nous mettons en contexte la définition de confidentialité différentielle comme une technique de quantification et nous analysons ses variantes comme limites de la définition originelle.