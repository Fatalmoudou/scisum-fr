Nous présentons dans cet article une approche permettant d'apprendre une politique de contrôle à partir de l'observation d'un expert manipulant le système. Dans le cas où les actions peuvent s'avérer critiques pour le système, par exemple dans des applications médicales où bien même en robotique, l'exploration du système peut ou doit être confiée à un humain. Les actions dangereuses pourront ainsi être évitée avec la contrepartie que l'exploration restera partielle et que le nombre de trajectoires utilisables pour l'apprentissage sera limité. Nous nous intéressons ici à l'impacte du choix de l'espace d'états sur l'apprentissage de la politique dans le cas particulier d'un nombre limité d'échantillons d'apprentissage et nous proposons l'utilisation du critère de vraisemblance pour apprendre une discrétisation sous la forme d'un réseau bayésien dynamique. Ce modèle sert ensuite de support à l'apprentissage d'une politique de contrôle. L'algorithme QD-Iteration, qui est une version itérative hors ligne de QLearning, est introduit pour apprendre la politique à partir des trajectoires fournies par l'expert humain. Le problème du pendule sur le chariot est utilisé pour illustrer et tester l'approche.