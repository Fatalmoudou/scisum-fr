Progress in machine learning, more recently accelerated by deep learning, now makes possible to build high-performance models for recognition or generation tasks. However, these models are complex, making it difficult to justify their predictions. To avoid a "black box" effect and for an interpretability reason, the Orpailleur team is interested in model explanations, a research field that is still recent. These explainers manage to spot biased predictions of some models (e.g., decisions mainly based on skin color). The discoveries of the Orpailleur team show that it is possible to make predictions fairer (we generally talk about "fairness") thanks to ensemble methods and variable dropping, without altering the models performance. We particularly focus here on the specific case of textual data, as well as on an adaptation to deep learning models which hold a prominent place in natural language processing. Secondly, we will use these explainers on antibiotic classification models in order to determine how such a classifier reaches the conclusion that a molecule has antibiotic properties. This work will lead to the creation of an interactive web interface to highlight the important patterns learned by these models, through visualizations.