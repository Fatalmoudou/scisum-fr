[
  {
    "title": "Intérêt de la spectrométrie de masse MALDI-TOF couplée aux techniques d’apprentissage automatique « machine learning » dans l’identification des agents pathogènes",
    "authors": [
      "Maxime Muller"
    ],
    "year": 2024,
    "abstract": "Ce travail explore l’intégration des méthodes d’apprentissage automatique avec la technologie de spectrométrie de masse MALDI-TOF pour des applications avancées dans le domaine de la microbiologique. Dans une première partie, je me suis concentré sur les aspects fondamentaux de la spectrométrie de masse MALDI-TOF. J’y présente son histoire, ainsi que ses capacités analytiques. La deuxième partie est dédiée aux notions théoriques de l’apprentissage automatique, avec un focus sur les différents types d’algorithmes (supervisés et non supervisés) et les méthodologies de validation. Enfin, la troisième partie s’appuie sur une revue de la littérature scientifique avec pour problématique : Quel est l’apport de l’apprentissage automatique au spectromètre de masse MALDI-TOF dans l’identification des résistances aux antibiotiques ? Pour répondre à cette question, j’ai consulté des études publiées entre 2016 et 2024, et en particulier celles qui portent sur le Staphylococcus aureus résistant à la méticilline, le Staphylococcus aureus résistant à la vancomycine et le Klebsiella pneumoniae résistant aux carbapénèmes. En conclusion, ce travail démontre que l’apprentissage automatique, combiné au MALDI-TOF, constitue une voie prometteuse pour optimiser les capacités d’analyse des spectres de masse.",
    "uri": "https://dumas.ccsd.cnrs.fr/dumas-05003666v1",
    "texte_nettoye": "Ce travail explore l’intégration des méthodes d’apprentissage automatique avec la technologie de spectrométrie de masse MALDI-TOF pour des applications avancées dans le domaine de la microbiologique. Dans une première partie, je me suis concentré sur les aspects fondamentaux de la spectrométrie de masse MALDI-TOF. J’y présente son histoire, ainsi que ses capacités analytiques. La deuxième partie est dédiée aux notions théoriques de l’apprentissage automatique, avec un focus sur les différents types d’algorithmes (supervisés et non supervisés) et les méthodologies de validation. Enfin, la troisième partie s’appuie sur une revue de la littérature scientifique avec pour problématique : Quel est l’apport de l’apprentissage automatique au spectromètre de masse MALDI-TOF dans l’identification des résistances aux antibiotiques ? Pour répondre à cette question, j’ai consulté des études publiées entre 2016 et 2024, et en particulier celles qui portent sur le Staphylococcus aureus résistant à la méticilline, le Staphylococcus aureus résistant à la vancomycine et le Klebsiella pneumoniae résistant aux carbapénèmes. En conclusion, ce travail démontre que l’apprentissage automatique, combiné au MALDI-TOF, constitue une voie prometteuse pour optimiser les capacités d’analyse des spectres de masse."
  },
  {
    "title": "Les méthodes « apprendre à chercher » en traitement automatique des langues : un état de l'art",
    "authors": [
      "Elena Knyazeva",
      "Guillaume Wisniewski",
      "François Yvon"
    ],
    "year": 2018,
    "abstract": "L'apprentissage structuré est au fondement des méthodes modernes d'apprentissage automatique pour le traitement automatique des langues (TAL). Dans cet article, nous étudions une famille d'algorithmes d'apprentissage structuré, les algorithmes de la famille « apprendre à chercher », qui diffèrent fondamentalement des méthodes classiques telles que les champs markoviens aléatoires, et permettent donc de mettre en évidence certains des compromis de l'apprentissage structuré en TAL. Nous présentons également un panorama des applications de ces techniques en TAL, en discutant les bénéfices découlant de leur utilisation. ABSTRACT. Structured prediction lies at the heart of modern Natural language Processing (NLP). In this paper, we study a specific family of structured learning algorithms, loosely refered to as \"Learning-to-search\" algorithms. They differ in several important ways from more studied methods such as Conditional Random Fields, and their study highlights several important trade-offs of structured learning for NLP. We also present an overview of existing applications of these techniques to NLP problems and discuss their potential benefits. MOTS-CLÉS : traitement automatique des langues, apprentissage structuré, apprendre à chercher.",
    "uri": "https://hal.science/hal-02103318v1",
    "texte_nettoye": "L'apprentissage structuré est au fondement des méthodes modernes d'apprentissage automatique pour le traitement automatique des langues (TAL). Dans cet article, nous étudions une famille d'algorithmes d'apprentissage structuré, les algorithmes de la famille « apprendre à chercher », qui diffèrent fondamentalement des méthodes classiques telles que les champs markoviens aléatoires, et permettent donc de mettre en évidence certains des compromis de l'apprentissage structuré en TAL. Nous présentons également un panorama des applications de ces techniques en TAL, en discutant les bénéfices découlant de leur utilisation."
  },
  {
    "title": "Évaluation de systèmes apprenant tout au long de la vie",
    "authors": [
      "Yevhenii Prokopalo",
      "Sylvain Meignier",
      "Olivier Galibert",
      "Loïc Barrault",
      "Anthony Larcher"
    ],
    "year": 2020,
    "abstract": "Aujourd’hui les systèmes intelligents obtiennent d’excellentes performances dans de nombreux domaines lorsqu’ils sont entraînés par des experts en apprentissage automatique. Lorsque ces systèmes sont mis en production, leurs performances se dégradent au cours du temps du fait de l’évolution de leur environnement réel. Une adaptation de leur modèle par des experts en apprentissage automatique est possible mais très coûteuse alors que les sociétés utilisant ces systèmes disposent d’experts du domaine qui pourraient accompagner ces systèmes dans un apprentissage tout au long de la vie. Dans cet article nous proposons un cadre d’évaluation générique pour des systèmes apprenant tout au long de la vie (SATLV). Nous proposons d’évaluer l’apprentissage assisté par l’humain (actif ou interactif) et l’apprentissage au cours du temps.",
    "uri": "https://hal.science/hal-02798580v3",
    "texte_nettoye": "Aujourd’hui les systèmes intelligents obtiennent d’excellentes performances dans de nombreux domaines lorsqu’ils sont entraînés par des experts en apprentissage automatique. Lorsque ces systèmes sont mis en production, leurs performances se dégradent au cours du temps du fait de l’évolution de leur environnement réel. Une adaptation de leur modèle par des experts en apprentissage automatique est possible mais très coûteuse alors que les sociétés utilisant ces systèmes disposent d’experts du domaine qui pourraient accompagner ces systèmes dans un apprentissage tout au long de la vie. Dans cet article nous proposons un cadre d’évaluation générique pour des systèmes apprenant tout au long de la vie (SATLV). Nous proposons d’évaluer l’apprentissage assisté par l’humain (actif ou interactif) et l’apprentissage au cours du temps."
  },
  {
    "title": "L’apprentissage automatique du rangement d’objets et son application à l’évaluation du lustre des perles de culture de Tahiti",
    "authors": [
      "Gaël Mondonneix"
    ],
    "year": 2019,
    "abstract": "Les problèmes d’apprentissage automatique sont parfois résolus avec des méthodes inadaptées, menant à des résultats sous optimaux. La détermination de la méthode à appliquer à la résolution d’un problème d’apprentissage automatique n’est pas toujours aisée, d’un côté parce qu’elle nécessite d’identifier correctement le problème à résoudre, et d’un autre côté parce que même si le problème en question a été identifié correctement, il n’existe pas nécessairement de méthode spécifique pour le résoudre. La présente thèse part du constat selon lequel il n’existe pas d’algorithme d’apprentissage automatique spécifiquement dédié au rangement d’objets (un type particulier de problèmes d’apprentissage ordinal consistant à apprendre un ordre total à partir d’un ensemble de couples d’objets). Une méthode d’apprentissage automatique destinée à combler ce manque est proposée et implémentée sous forme d’une machine à noyau. Un méta-noyau est développé afin de l’adapter aux bibliothèques standard d’apprentissage automatique. L’algorithme proposé est appliqué à un cas concret de problème de rangement d’objets, l’évaluation du lustre des perles de Tahiti, et comparé avec quatre autres méthodes. Cet algorithme donne de meilleurs résultats (93,6% ± 3,9% de prévisions correctes sans sélection de caractéristiques, 94,3% ± 3,4% avec sélection de caractéristiques) que les meilleures des autres méthodes testées (91,3% ± 3,4% et 92,6% ± 4,3% sans et avec sélection de caractéristiques), cette amélioration étant significative (p < 0,01 et p = 0.02 respectivement). En outre, cette méthode ne présente pas de différence significative entre les résultats obtenus avec et sans sélection de caractéristiques (valeur-p = 0.33), ce qui peut indiquer que son biais d'apprentissage correspond bien au problème à résoudre et peut ainsi réduire la charge de travail en phase de prétraitement des données.",
    "uri": "https://upf.hal.science/tel-03180294v1",
    "texte_nettoye": "Les problèmes d’apprentissage automatique sont parfois résolus avec des méthodes inadaptées, menant à des résultats sous optimaux. La détermination de la méthode à appliquer à la résolution d’un problème d’apprentissage automatique n’est pas toujours aisée, d’un côté parce qu’elle nécessite d’identifier correctement le problème à résoudre, et d’un autre côté parce que même si le problème en question a été identifié correctement, il n’existe pas nécessairement de méthode spécifique pour le résoudre. La présente thèse part du constat selon lequel il n’existe pas d’algorithme d’apprentissage automatique spécifiquement dédié au rangement d’objets (un type particulier de problèmes d’apprentissage ordinal consistant à apprendre un ordre total à partir d’un ensemble de couples d’objets). Une méthode d’apprentissage automatique destinée à combler ce manque est proposée et implémentée sous forme d’une machine à noyau. Un méta-noyau est développé afin de l’adapter aux bibliothèques standard d’apprentissage automatique. L’algorithme proposé est appliqué à un cas concret de problème de rangement d’objets, l’évaluation du lustre des perles de Tahiti, et comparé avec quatre autres méthodes. Cet algorithme donne de meilleurs résultats (93,6% ± 3,9% de prévisions correctes sans sélection de caractéristiques, 94,3% ± 3,4% avec sélection de caractéristiques) que les meilleures des autres méthodes testées (91,3% ± 3,4% et 92,6% ± 4,3% sans et avec sélection de caractéristiques), cette amélioration étant significative (p < 0,01 et p = 0.02 respectivement). En outre, cette méthode ne présente pas de différence significative entre les résultats obtenus avec et sans sélection de caractéristiques (valeur-p = 0.33), ce qui peut indiquer que son biais d'apprentissage correspond bien au problème à résoudre et peut ainsi réduire la charge de travail en phase de prétraitement des données."
  },
  {
    "title": "Structures arborescentes et apprentissage automatique",
    "authors": [
      "Marc Tommasi"
    ],
    "year": 2006,
    "abstract": "Le programme de recherches présenté dans cette synthèse s'inscrit dans la double problématique de l'étude des langages d'arbres et de l'apprentissage automatique à partir de données arborescentes. À la base de ce travail se trouve la question de l'accès et de la manipulation automatique d'informations au format XML au sein d'un réseau d'applications réparties dans internet. La réalisation de ces applications est toujours du ressort de programmeurs spécialistes d'XML et reste hors de portée de l'utilisateur final. De plus, les développements récents d'internet poursuivent l'objectif d'automatiser les communications entre applications s'échangeant des flux de données XML. Le recours à des techniques d'apprentissage automatique est une réponse possible à cette situation. Nous considèrons que les informations sont décrites dans un langage XML, et dans la perspective de ce mémoire, embarquées dans des données structurées sous forme arborescente. Les applications sont basées alors sur des opérations élémentaires que sont l'interrogation ou les requêtes dans ces documents arborescents ou encore la transformation de tels documents. Nous abordons alors la question sous l'angle de la réalisation automatique de programmes d'annotation d'arbres, permettant de dériver des procédures de transformation ou d'exécution de requêtes. Le mémoire décrit les contributions apportées pour la manipulation et l'apprentissage d'ensembles d'arbres d'arité non bornée (comme le sont les arbres XML), et l'annotation par des méthodes de classification supervisée ou d'inférence statistique.",
    "uri": "https://theses.hal.science/tel-00117063v1",
    "texte_nettoye": "Le programme de recherches présenté dans cette synthèse s'inscrit dans la double problématique de l'étude des langages d'arbres et de l'apprentissage automatique à partir de données arborescentes. À la base de ce travail se trouve la question de l'accès et de la manipulation automatique d'informations au format XML au sein d'un réseau d'applications réparties dans internet. La réalisation de ces applications est toujours du ressort de programmeurs spécialistes d'XML et reste hors de portée de l'utilisateur final. De plus, les développements récents d'internet poursuivent l'objectif d'automatiser les communications entre applications s'échangeant des flux de données XML. Le recours à des techniques d'apprentissage automatique est une réponse possible à cette situation. Nous considèrons que les informations sont décrites dans un langage XML, et dans la perspective de ce mémoire, embarquées dans des données structurées sous forme arborescente. Les applications sont basées alors sur des opérations élémentaires que sont l'interrogation ou les requêtes dans ces documents arborescents ou encore la transformation de tels documents. Nous abordons alors la question sous l'angle de la réalisation automatique de programmes d'annotation d'arbres, permettant de dériver des procédures de transformation ou d'exécution de requêtes. Le mémoire décrit les contributions apportées pour la manipulation et l'apprentissage d'ensembles d'arbres d'arité non bornée (comme le sont les arbres XML), et l'annotation par des méthodes de classification supervisée ou d'inférence statistique."
  },
  {
    "title": "Méthodes, techniques et outils d’apprentissage automatique",
    "authors": [
      "Habib Hadj-Mabrouk"
    ],
    "year": 2000,
    "abstract": "Le processus de transfert de connaissances des experts humains vers la machine est complexe et peu étudié. En effet, le goulot d'étranglement du développement d'un système expert ne se limite pas à la seule phase d'extraction de connaissances mais est également lié aux caractéristiques et à la formalisation des connaissances ainsi qu'à la collaboration entre l'expert et le cogniticien. Le savoir-faire des experts repose sur des connaissances subjectives, empiriques et parfois implicites qui peuvent générer plusieurs interprétations. Il n'existe généralement pas d'explication scientifique pour justifier cette expertise compilée. Ces connaissances ne sont pas toujours conscientes chez l'expert, compréhensibles par un novice ou même exprimables par l'intermédiaire d'un langage. La transcription d'un langage verbal (naturel) en langage formel interprétable par une machine provoque souvent une distorsion de la connaissance experte. Ceci introduit un biais entre le modèle cognitif de l'expert et le modèle implémenté. Ce décalage est dû non seulement au fait que les langages de représentation employés en IA ne sont pas d'une richesse suffisante pour expliciter le fonctionnement cognitif de l'expert mais aussi à l'interprétation subjective du cogniticien. Toutes ces contraintes restreignent le champ d'investigation des méthodes d'acquisition de connaissances. L'utilisation des techniques d'apprentissage automatique (Machine Learning) est une solution pour affaiblir ces contraintes. En effet, si la psychologie cognitive et le génie logiciel ont généré des méthodes et outils d'aide à l'acquisition des connaissances, l'exploitation de ces méthodes demeure encore limitée, dans un contexte industriel complexe. Les experts considèrent généralement qu'il est plus simple de décrire des exemples ou des cas expérimentaux plutôt que d'expliciter des processus de prise de décision. L'introduction des systèmes d'apprentissage automatique fonctionnant sur des exemples permet d'engendrer de nouvelles connaissances susceptibles d'aider l'expert à résoudre un problème particulier. L'expertise d'un domaine est non seulement détenue par les experts mais aussi répartie et emmagasinée implicitement dans une masse de données historiques que l'esprit humain éprouve des difficultés à synthétiser. Extraire de cette masse d'informations des connaissances pertinentes dans un but explicatif ou décisionnel constitue l'un des objectifs de l'apprentissage automatique. L'apprentissage automatique (Machine Learning) est une branche importante de la recherche dans le domaine de l'Intelligence Artificielle. L'apprentissage est un terme très général qui décrit le processus selon lequel l'être humain ou la machine peut accroître sa connaissance. Apprendre c'est donc raisonner. Les efforts menés dans ce domaine ont débouché sur une grande variété de méthodes, techniques, algorithmes et systèmes. Ce foisonnement rend difficile la perception du domaine, compte tenu de l'ambiguïté du vocabulaire qui lui est propre et de l'absence de définitions de référence. Sur la base des travaux existants, notre contribution vise à synthétiser les concepts fondamentaux impliqués dans le processus d'apprentissage. Cet article propose donc une caractérisation générale du processus d'apprentissage en précisant ses données d'entrée et de sortie, les contraintes qu'il doit respecter ainsi que les mécanismes employés pour apprendre.",
    "uri": "https://hal.science/hal-03023929v1",
    "texte_nettoye": "Le processus de transfert de connaissances des experts humains vers la machine est complexe et peu étudié. En effet, le goulot d'étranglement du développement d'un système expert ne se limite pas à la seule phase d'extraction de connaissances mais est également lié aux caractéristiques et à la formalisation des connaissances ainsi qu'à la collaboration entre l'expert et le cogniticien. Le savoir-faire des experts repose sur des connaissances subjectives, empiriques et parfois implicites qui peuvent générer plusieurs interprétations. Il n'existe généralement pas d'explication scientifique pour justifier cette expertise compilée. Ces connaissances ne sont pas toujours conscientes chez l'expert, compréhensibles par un novice ou même exprimables par l'intermédiaire d'un langage. La transcription d'un langage verbal (naturel) en langage formel interprétable par une machine provoque souvent une distorsion de la connaissance experte. Ceci introduit un biais entre le modèle cognitif de l'expert et le modèle implémenté. Ce décalage est dû non seulement au fait que les langages de représentation employés en IA ne sont pas d'une richesse suffisante pour expliciter le fonctionnement cognitif de l'expert mais aussi à l'interprétation subjective du cogniticien. Toutes ces contraintes restreignent le champ d'investigation des méthodes d'acquisition de connaissances. L'utilisation des techniques d'apprentissage automatique (Machine Learning) est une solution pour affaiblir ces contraintes. En effet, si la psychologie cognitive et le génie logiciel ont généré des méthodes et outils d'aide à l'acquisition des connaissances, l'exploitation de ces méthodes demeure encore limitée, dans un contexte industriel complexe. Les experts considèrent généralement qu'il est plus simple de décrire des exemples ou des cas expérimentaux plutôt que d'expliciter des processus de prise de décision. L'introduction des systèmes d'apprentissage automatique fonctionnant sur des exemples permet d'engendrer de nouvelles connaissances susceptibles d'aider l'expert à résoudre un problème particulier. L'expertise d'un domaine est non seulement détenue par les experts mais aussi répartie et emmagasinée implicitement dans une masse de données historiques que l'esprit humain éprouve des difficultés à synthétiser. Extraire de cette masse d'informations des connaissances pertinentes dans un but explicatif ou décisionnel constitue l'un des objectifs de l'apprentissage automatique. L'apprentissage automatique (Machine Learning) est une branche importante de la recherche dans le domaine de l'Intelligence Artificielle. L'apprentissage est un terme très général qui décrit le processus selon lequel l'être humain ou la machine peut accroître sa connaissance. Apprendre c'est donc raisonner. Les efforts menés dans ce domaine ont débouché sur une grande variété de méthodes, techniques, algorithmes et systèmes. Ce foisonnement rend difficile la perception du domaine, compte tenu de l'ambiguïté du vocabulaire qui lui est propre et de l'absence de définitions de référence. Sur la base des travaux existants, notre contribution vise à synthétiser les concepts fondamentaux impliqués dans le processus d'apprentissage. Cet article propose donc une caractérisation générale du processus d'apprentissage en précisant ses données d'entrée et de sortie, les contraintes qu'il doit respecter ainsi que les mécanismes employés pour apprendre."
  },
  {
    "title": "Apprentissage des langages réguliers d'arbres à l'aide d'un expert",
    "authors": [
      "Jérôme Besombes",
      "Jean-Yves Marion"
    ],
    "year": 2003,
    "abstract": "Dans le but de modéliser l'apprentissage des langues naturelles, nous nous intéressons à l'apprentissage des langages réguliers d'arbres à partir d'exemples positifs et en interaction avec un expert. L'expert est un oracle qui connaît le langage appris et qui répond à des questions d'appartenance. Nous donnons un algorithme efficace d'apprentissage dans ce paradigme, nous montrons sa correction et l'illustrons sur un exemple détaillé.",
    "uri": "https://inria.hal.science/inria-00099694v1",
    "texte_nettoye": "Dans le but de modéliser l'apprentissage des langues naturelles, nous nous intéressons à l'apprentissage des langages réguliers d'arbres à partir d'exemples positifs et en interaction avec un expert. L'expert est un oracle qui connaît le langage appris et qui répond à des questions d'appartenance. Nous donnons un algorithme efficace d'apprentissage dans ce paradigme, nous montrons sa correction et l'illustrons sur un exemple détaillé."
  },
  {
    "title": "Apprentissage des grammaires catégorielles à partir de structures",
    "authors": [
      "Jérôme Besombes",
      "Jean-Yves Marion"
    ],
    "year": 2004,
    "abstract": "Nous présentons Alfa, un algorithme d'apprentissage des grammaires catégorielles. Nous nous intéressons à l'apprentissage à la limite à partir d'exemples positifs et les exemples sont des arbres de dérivations partiellement désetiquetés. Nous montrons qu'Alfa identifie la classe des grammaires catérogielles réversibles, une classe contenant strictement les grammaires rigides. En ce sens, ce résultat constitue une extension de l'algorithme de Kanazawa d'apprentissage des grammaires rigides.",
    "uri": "https://inria.hal.science/inria-00100100v1",
    "texte_nettoye": "Nous présentons Alfa, un algorithme d'apprentissage des grammaires catégorielles. Nous nous intéressons à l'apprentissage à la limite à partir d'exemples positifs et les exemples sont des arbres de dérivations partiellement désetiquetés. Nous montrons qu'Alfa identifie la classe des grammaires catérogielles réversibles, une classe contenant strictement les grammaires rigides. En ce sens, ce résultat constitue une extension de l'algorithme de Kanazawa d'apprentissage des grammaires rigides."
  },
  {
    "title": "Apprentissage des langages réguliers d'arbres et applications",
    "authors": [
      "Jérôme Besombes",
      "Jean-Yves Marion"
    ],
    "year": 2003,
    "abstract": "Nous nous intéressons à l'apprentissage des langues naturelles. Notre modèle est l'identification à la limite de Gold et les exemples sont des arbres. La principale contribution est la démonstration du fait que les langages réguliers d'arbres réversibles sont identifiables. Ce résultat nous permet de disposer d'un cadre unifié pour l'apprentissage à partir d'exemples semi-structurés. En particulier, nous présentons trois conséquences de ce résultat. (1) Nous étendons le résultat de Sakakibara concernant l'apprentissage de grammaires algébriques à partir des arbres de dérivation. (2) Nous montrons que les langages d'arbres de dépendances sont identifiables. (3) Nous donnons une nouvelle démonstration de l'apprentissage des grammaires catégorielles de Kanazawa.",
    "uri": "https://inria.hal.science/inria-00099629v1",
    "texte_nettoye": "Nous nous intéressons à l'apprentissage des langues naturelles. Notre modèle est l'identification à la limite de Gold et les exemples sont des arbres. La principale contribution est la démonstration du fait que les langages réguliers d'arbres réversibles sont identifiables. Ce résultat nous permet de disposer d'un cadre unifié pour l'apprentissage à partir d'exemples semi-structurés. En particulier, nous présentons trois conséquences de ce résultat. (1) Nous étendons le résultat de Sakakibara concernant l'apprentissage de grammaires algébriques à partir des arbres de dérivation. (2) Nous montrons que les langages d'arbres de dépendances sont identifiables. (3) Nous donnons une nouvelle démonstration de l'apprentissage des grammaires catégorielles de Kanazawa."
  },
  {
    "title": "Optimisation automatique de l'apprentissage en ligne",
    "authors": [
      "Émile Royer"
    ],
    "year": 2024,
    "abstract": "Les méthodes d'optimisation de modèles d'apprentissage automatique en ligne utilisent exclusivement des algorithmes génétiques. Il est raisonnable de penser que des méthodes plus efficaces encore existent en faisant un parallèle avec les autres domaines de l'optimisation. Nous montrons qu'il est possible d'utiliser l'optimisation bayésienne pour l'optimisation en ligne, avec des résultats équivalents à l'état de l'art. Cette nouvelle technique a le potentiel de créer de nouveaux optimiseurs en ligne plus efficaces.",
    "uri": "https://inria.hal.science/hal-04921796v1",
    "texte_nettoye": "Les méthodes d'optimisation de modèles d'apprentissage automatique en ligne utilisent exclusivement des algorithmes génétiques. Il est raisonnable de penser que des méthodes plus efficaces encore existent en faisant un parallèle avec les autres domaines de l'optimisation. Nous montrons qu'il est possible d'utiliser l'optimisation bayésienne pour l'optimisation en ligne, avec des résultats équivalents à l'état de l'art. Cette nouvelle technique a le potentiel de créer de nouveaux optimiseurs en ligne plus efficaces."
  },
  {
    "title": "Adaptation de domaine multisource sur données déséquilibrées : application à l'amélioration de la sécurité des télésièges",
    "authors": [
      "Kevin Bascol"
    ],
    "year": 2019,
    "abstract": "Bluecime a mis au point un système de vidéosurveillance à l'embarquement de télésièges qui a pour but d'améliorer la sécurité des passagers. Ce système est déjà performant, mais il n'utilise pas de techniques d'apprentissage automatique et nécessite une phase de configuration chronophage. L’apprentissage automatique est un sous-domaine de l'intelligence artificielle qui traite de l'étude et de la conception d'algorithmes pouvant apprendre et acquérir des connaissances à partir d'exemples pour une tâche donnée. Une telle tâche pourrait consister à classer les situations sûres ou dangereuses dans les télésièges à partir d'exemples d'images déjà étiquetées dans ces deux catégories, appelés exemples d’entraînement. L'algorithme d'apprentissage automatique apprend un modèle capable de prédire la catégories de nouveaux cas. Depuis 2012, il a été démontré que les modèles d'apprentissage profond sont les modèles d'apprentissage machine les mieux adaptés pour traiter les problèmes de classification d'images lorsque de nombreuses données d’entraînement sont disponibles. Dans ce contexte, cette thèse, financée par Bluecime, vise à améliorer à la fois le coût et l'efficacité du système actuel de Bluecime grâce à l'apprentissage profond.",
    "uri": "https://theses.hal.science/tel-02497272v1",
    "texte_nettoye": "Bluecime a mis au point un système de vidéosurveillance à l'embarquement de télésièges qui a pour but d'améliorer la sécurité des passagers. Ce système est déjà performant, mais il n'utilise pas de techniques d'apprentissage automatique et nécessite une phase de configuration chronophage. L’apprentissage automatique est un sous-domaine de l'intelligence artificielle qui traite de l'étude et de la conception d'algorithmes pouvant apprendre et acquérir des connaissances à partir d'exemples pour une tâche donnée. Une telle tâche pourrait consister à classer les situations sûres ou dangereuses dans les télésièges à partir d'exemples d'images déjà étiquetées dans ces deux catégories, appelés exemples d’entraînement. L'algorithme d'apprentissage automatique apprend un modèle capable de prédire la catégories de nouveaux cas. Depuis 2012, il a été démontré que les modèles d'apprentissage profond sont les modèles d'apprentissage machine les mieux adaptés pour traiter les problèmes de classification d'images lorsque de nombreuses données d’entraînement sont disponibles. Dans ce contexte, cette thèse, financée par Bluecime, vise à améliorer à la fois le coût et l'efficacité du système actuel de Bluecime grâce à l'apprentissage profond."
  },
  {
    "title": "Apprendre par imitation : applications à quelques problèmes d'apprentissage structuré en traitement des langues",
    "authors": [
      "Elena Knyazeva"
    ],
    "year": 2018,
    "abstract": "L’apprentissage structuré est devenu omniprésent dans le traitement automatique des langues naturelles. De nombreuses applications qui font maintenant partie de notre vie telles que des assistants personnels, la traduction automatique, ou encore la reconnaissance vocale, reposent sur ces techniques. Les problèmes d'apprentissage structuré qu’il est nécessaire de résoudre sont de plus en plus complexes et demandent de prendre en compte de plus en plus d’informations à des niveaux linguistiques variés (morphologique, syntaxique, etc.) et reposent la question du meilleurs compromis entre la finesse de la modélisation et l’exactitude des algorithmes d’apprentissage et d’inférence. L’apprentissage par imitation propose de réaliser les procédures d’apprentissage et d’inférence de manière approchée afin de pouvoir exploiter pleinement des structures de dépendance plus riches. Cette thèse explore ce cadre d’apprentissage, en particulier l’algorithme SEARN, à la fois sur le plan théorique ainsi que ses possibilités d’application aux tâches de traitement automatique des langues, notamment aux plus complexes telles que la traduction. Concernant les aspects théoriques, nous présentons un cadre unifié pour les différentes familles d’apprentissage par imitation, qui permet de redériver de manière simple les propriétés de convergence de ces algorithmes; concernant les aspects plus appliqués, nous utilisons l’apprentissage par imitation d’une part pour explorer l’étiquetage de séquences en ordre libre; d’autre part pour étudier des stratégies de décodage en deux étapes pour la traduction automatique.",
    "uri": "https://theses.hal.science/tel-01906278v1",
    "texte_nettoye": "L’apprentissage structuré est devenu omniprésent dans le traitement automatique des langues naturelles. De nombreuses applications qui font maintenant partie de notre vie telles que des assistants personnels, la traduction automatique, ou encore la reconnaissance vocale, reposent sur ces techniques. Les problèmes d'apprentissage structuré qu’il est nécessaire de résoudre sont de plus en plus complexes et demandent de prendre en compte de plus en plus d’informations à des niveaux linguistiques variés (morphologique, syntaxique, etc.) et reposent la question du meilleurs compromis entre la finesse de la modélisation et l’exactitude des algorithmes d’apprentissage et d’inférence. L’apprentissage par imitation propose de réaliser les procédures d’apprentissage et d’inférence de manière approchée afin de pouvoir exploiter pleinement des structures de dépendance plus riches. Cette thèse explore ce cadre d’apprentissage, en particulier l’algorithme SEARN, à la fois sur le plan théorique ainsi que ses possibilités d’application aux tâches de traitement automatique des langues, notamment aux plus complexes telles que la traduction. Concernant les aspects théoriques, nous présentons un cadre unifié pour les différentes familles d’apprentissage par imitation, qui permet de redériver de manière simple les propriétés de convergence de ces algorithmes; concernant les aspects plus appliqués, nous utilisons l’apprentissage par imitation d’une part pour explorer l’étiquetage de séquences en ordre libre; d’autre part pour étudier des stratégies de décodage en deux étapes pour la traduction automatique."
  },
  {
    "title": "Définition autonome de sous-problèmes dans l'apprentissage par renforcement",
    "authors": [
      "Daniel Szer"
    ],
    "year": 2003,
    "abstract": "Plusieurs approches ont été développées pour structurer et faciliter l'apprentissage d'une tâche en utilisant des solutions de tâches plus simples ou moins complexes. Ces approches, souvent nommées apprentissage hiérarchique ou incrémental, nécessitent normalement une décomposition de la tâche par un humain, ou bien la définition de sous-tâches pouvant être utilisées, mais il n'existe pratiquement pas d'algorithmes qui procèdent à la décomposition des problèmes complexes en des tâches plus simples de façon autonome. Nous allons proposer un moyen général permettant la définition de sous-problèmes dans le cadre de l'apprentissage par renforcement, basé sur l'observation de changements dans l'environnement. Nous allons ensuite montrer comment résoudre ces sous-problèmes et comment les solutions à ces derniers peuvent être utilisées pour résoudre le problème initial.",
    "uri": "https://inria.hal.science/inria-00099688v1",
    "texte_nettoye": "Plusieurs approches ont été développées pour structurer et faciliter l'apprentissage d'une tâche en utilisant des solutions de tâches plus simples ou moins complexes. Ces approches, souvent nommées apprentissage hiérarchique ou incrémental, nécessitent normalement une décomposition de la tâche par un humain, ou bien la définition de sous-tâches pouvant être utilisées, mais il n'existe pratiquement pas d'algorithmes qui procèdent à la décomposition des problèmes complexes en des tâches plus simples de façon autonome. Nous allons proposer un moyen général permettant la définition de sous-problèmes dans le cadre de l'apprentissage par renforcement, basé sur l'observation de changements dans l'environnement. Nous allons ensuite montrer comment résoudre ces sous-problèmes et comment les solutions à ces derniers peuvent être utilisées pour résoudre le problème initial."
  },
  {
    "title": "REDUCTION DE LA DIMENSION DE L'ESPACE VECTORIEL LORS DE LA PHASE DE PRETRAITEMENT POUR L'APPRENTISSAGE AUTOMATIQUE",
    "authors": [
      "Marc Aurele Djeguede"
    ],
    "year": 2019,
    "abstract": "L'utilisation des techniques d'apprentissage automatique plus connu sous son anglicisme « Machine Learning » se repend de plus en plus. Elle s'applique à des domaines très divers et variés, du traitement de l'image au traitement du langage humain en passant par la sécurité informatique et l'industrie des jeux vidéo. L'un des principaux buts de l'application des techniques d'apprentissage automatique est la prédiction. Tout procédé d'apprentissage automatique peut être subdivisé en les étapes suivantes : le prétraitement des données, la conception du modèle et son entrainement puis l'évaluation du modèle. La phase de prétraitement encore connue sous le nom de « Pre-processing » consiste très souvent à choisir les variables les plus informatives du jeu de vecteurs entrants et à les normaliser si nécessaire. Cette technique s'appelle encore « feature selection / feature engineering ». La finalité de la sélection des variables informatives est d'éliminer les variables non essentielles qui peuvent être considérées comme du bruit par le modèle et réduire ainsi sa précision de prédiction. Plusieurs travaux de recherche ont été menés ces dernières décennies dans le but de proposer soit des algorithmes, soit des critères de sélection des variables informatives.",
    "uri": "https://hal.science/hal-02118326v1",
    "texte_nettoye": "L'utilisation des techniques d'apprentissage automatique plus connu sous son anglicisme « Machine Learning » se repend de plus en plus. Elle s'applique à des domaines très divers et variés, du traitement de l'image au traitement du langage humain en passant par la sécurité informatique et l'industrie des jeux vidéo. L'un des principaux buts de l'application des techniques d'apprentissage automatique est la prédiction. Tout procédé d'apprentissage automatique peut être subdivisé en les étapes suivantes : le prétraitement des données, la conception du modèle et son entrainement puis l'évaluation du modèle. La phase de prétraitement encore connue sous le nom de « Pre-processing » consiste très souvent à choisir les variables les plus informatives du jeu de vecteurs entrants et à les normaliser si nécessaire. Cette technique s'appelle encore « feature selection / feature engineering ». La finalité de la sélection des variables informatives est d'éliminer les variables non essentielles qui peuvent être considérées comme du bruit par le modèle et réduire ainsi sa précision de prédiction. Plusieurs travaux de recherche ont été menés ces dernières décennies dans le but de proposer soit des algorithmes, soit des critères de sélection des variables informatives."
  },
  {
    "title": "Apprentissage discriminant de modèles neuronaux pour la traduction automatique",
    "authors": [
      "Quoc Khanh Do",
      "Alexandre Allauzen",
      "François Yvon"
    ],
    "year": 2016,
    "abstract": "Les méthodes utilisées pour entraîner des réseaux de neurones en traitement des langues reposent, pour la plupart, sur l'optimisation de critères qui sont décorrélés de l'application finale. Nous proposons un nouveau cadre d'apprentissage discriminant pour l'estimation des modèles neuronaux en traduction automatique. Ce cadre s'appuie sur la définition d'un critère d'apprentissage qui prend en compte, d'une part, la métrique utilisée pour l'évaluation automatique de la traduction et, d'autre part, le processus d'intégration de ces modèles au sein des systèmes de traduction automatique. Cette méthode est comparée aux critères d'apprentissage usuels que sont le maximum de vraisemblance et l'estimation contrastive bruitée. Les expériences menées sur les tâches de traduction des séminaires Tedtalks et de textes médicaux, depuis l'anglais vers le français, montrent la pertinence d'un cadre d'apprentissage discriminant et l'importance d'une initialisation judicieuse, en particulier dans une perspective d'adaptation au domaine.",
    "uri": "https://hal.science/hal-01620906v1",
    "texte_nettoye": "Les méthodes utilisées pour entraîner des réseaux de neurones en traitement des langues reposent, pour la plupart, sur l'optimisation de critères qui sont décorrélés de l'application finale. Nous proposons un nouveau cadre d'apprentissage discriminant pour l'estimation des modèles neuronaux en traduction automatique. Ce cadre s'appuie sur la définition d'un critère d'apprentissage qui prend en compte, d'une part, la métrique utilisée pour l'évaluation automatique de la traduction et, d'autre part, le processus d'intégration de ces modèles au sein des systèmes de traduction automatique. Cette méthode est comparée aux critères d'apprentissage usuels que sont le maximum de vraisemblance et l'estimation contrastive bruitée. Les expériences menées sur les tâches de traduction des séminaires Tedtalks et de textes médicaux, depuis l'anglais vers le français, montrent la pertinence d'un cadre d'apprentissage discriminant et l'importance d'une initialisation judicieuse, en particulier dans une perspective d'adaptation au domaine."
  },
  {
    "title": "Vers une approche multimodale basée sur l'apprentissage automatique et profond pour la détection d'impact pendant la chute",
    "authors": [
      "Tresor Y. Koffi",
      "Youssef Mourchid",
      "Mohammed Hindawi",
      "Yohan Dupuis"
    ],
    "year": 2025,
    "abstract": "Vers une approche multimodale basée sur l'apprentissage automatique et profond pour la détection d'impact pendant la chute",
    "uri": "https://hal.science/hal-05131292v1",
    "texte_nettoye": "Vers une approche multimodale basée sur l'apprentissage automatique et profond pour la détection d'impact pendant la chute"
  },
  {
    "title": "Apprentissage par renforcement développemental pour la robotique autonome",
    "authors": [
      "Luc Sarzyniec"
    ],
    "year": 2010,
    "abstract": "La problématique du stage porte sur l'apprentissage par renforcement appliqué à la robotique et plus particulièrement, l'apprentissage d'une tâche complexe divisée en plusieurs tâches plus simples.",
    "uri": "https://inria.hal.science/inria-00546983v1",
    "texte_nettoye": "La problématique du stage porte sur l'apprentissage par renforcement appliqué à la robotique et plus particulièrement, l'apprentissage d'une tâche complexe divisée en plusieurs tâches plus simples."
  },
  {
    "title": "Explicabilité de l'IA",
    "authors": [
      "Romain Giot"
    ],
    "year": 2023,
    "abstract": "L'intelligence artificielle fait partie de notre quotidien. Dans les fait ce terme aujourd'hui représente une sous partie de l'intelligence artificielle : l'apprentissage automatique. Cette présentation se focalise sur les aspects apprentissage automatique et leur explicabilité. Nous verrons les bases de fonctionnement des techniques d'apprentissage automatique, les spécificités de l'apprentissage profond et leurs problèmes. Nous verrons ensuite comment l'explicabilité permet de résoudre certains d'entre eux en présentant des méthodes issues des communautés d'apprentissage automatique et des communautés de visualisation d'informations.",
    "uri": "https://hal.science/hal-04353243v1",
    "texte_nettoye": "L'intelligence artificielle fait partie de notre quotidien. Dans les fait ce terme aujourd'hui représente une sous partie de l'intelligence artificielle : l'apprentissage automatique. Cette présentation se focalise sur les aspects apprentissage automatique et leur explicabilité. Nous verrons les bases de fonctionnement des techniques d'apprentissage automatique, les spécificités de l'apprentissage profond et leurs problèmes. Nous verrons ensuite comment l'explicabilité permet de résoudre certains d'entre eux en présentant des méthodes issues des communautés d'apprentissage automatique et des communautés de visualisation d'informations."
  },
  {
    "title": "Etiquetage morpho-syntaxique du français à base d'apprentissage supervisé",
    "authors": [
      "Julien Bourdaillet",
      "Jean-Gabriel Ganascia"
    ],
    "year": 2005,
    "abstract": "Nous présentons un étiqueteur morpho-syntaxique du français. Celui-ci utilise l’apprentissage supervisé à travers un modèle de Markov caché. Le modèle de langage est appris à partir d’un corpus étiqueté. Nous décrivons son fonctionnement et la méthode d’apprentissage. L’étiqueteur atteint un score de précision de 89 % avec un jeu d’étiquettes très riche. Nous présentons ensuite des résultats détaillés pour chaque classe grammaticale et étudions en particulier la reconnaissance des homographes.",
    "uri": "https://hal.science/hal-01489192v1",
    "texte_nettoye": "Nous présentons un étiqueteur morpho-syntaxique du français. Celui-ci utilise l’apprentissage supervisé à travers un modèle de Markov caché. Le modèle de langage est appris à partir d’un corpus étiqueté. Nous décrivons son fonctionnement et la méthode d’apprentissage. L’étiqueteur atteint un score de précision de 89 % avec un jeu d’étiquettes très riche. Nous présentons ensuite des résultats détaillés pour chaque classe grammaticale et étudions en particulier la reconnaissance des homographes."
  },
  {
    "title": "Apprentissage automatique pour le TAL : Préface",
    "authors": [
      "Isabelle Tellier"
    ],
    "year": 2009,
    "abstract": "L'apprentissage automatique et le traitement automatique des langues sont de plus en plus fréquemment amenés à collaborer. Nous montrons, en parcourant 60 ans d'histoire de l'intelligence artificielle, que les deux domaines ont une origine commune mais qu'ils ont longtemps suivi des évolutions parrallèles. Leur rapprochement date d'une vingtaine d'année et s'est accompagné de mutations profondes qui les affectent tous les deux, tout en permettant de nouvelles hybridations. Les connaissances linguistiques, longtemps conçues comme autonomes ou comme horizon indépassable, sont maintenant intégrées dans les programmes d'apprentissage automatique eux-mêmes. Les articles de ce numéro en sont une très bonne illustration.",
    "uri": "https://inria.hal.science/inria-00514535v1",
    "texte_nettoye": "L'apprentissage automatique et le traitement automatique des langues sont de plus en plus fréquemment amenés à collaborer. Nous montrons, en parcourant 60 ans d'histoire de l'intelligence artificielle, que les deux domaines ont une origine commune mais qu'ils ont longtemps suivi des évolutions parrallèles. Leur rapprochement date d'une vingtaine d'année et s'est accompagné de mutations profondes qui les affectent tous les deux, tout en permettant de nouvelles hybridations. Les connaissances linguistiques, longtemps conçues comme autonomes ou comme horizon indépassable, sont maintenant intégrées dans les programmes d'apprentissage automatique eux-mêmes. Les articles de ce numéro en sont une très bonne illustration."
  },
  {
    "title": "Apprentissage collectif par renforcement d'inspiration biologique ? Le système hamelin",
    "authors": [
      "Vincent Thomas"
    ],
    "year": 2003,
    "abstract": "L'apprentissage par renforcement devient difficile dans le cadre multi-agent car il faut prendre en compte la multiplicité des points de vue. Cet article présente une simulation inspiré d'un phénomène de spécialisation observé dans des groupes de rats. La simulation, nommée Hamelin, modèlise un certains nombre de mécanismes permettant à une collectivité de s'organiser. Nous présentons dans cet article ces mécanismes et mettons en évidence les liens qui peuvent exister entre cette simulation et le formalisme des MDP et l'apprentissage par renforcement avec l'idée que l'inspiration biologique peut aider à découvrir de nouveaux mécanismes pour un apprentissage par renforcement collectif.",
    "uri": "https://inria.hal.science/inria-00099583v1",
    "texte_nettoye": "L'apprentissage par renforcement devient difficile dans le cadre multi-agent car il faut prendre en compte la multiplicité des points de vue. Cet article présente une simulation inspiré d'un phénomène de spécialisation observé dans des groupes de rats. La simulation, nommée Hamelin, modèlise un certains nombre de mécanismes permettant à une collectivité de s'organiser. Nous présentons dans cet article ces mécanismes et mettons en évidence les liens qui peuvent exister entre cette simulation et le formalisme des MDP et l'apprentissage par renforcement avec l'idée que l'inspiration biologique peut aider à découvrir de nouveaux mécanismes pour un apprentissage par renforcement collectif."
  },
  {
    "title": "Apprentissage par Renforcement Développemental en Robotique Autonome",
    "authors": [
      "Luc Sarzyniec",
      "Olivier Buffet",
      "Alain Dutech"
    ],
    "year": 2011,
    "abstract": "Cet article présente une approche développementale de l'apprentissage par renforcement dans un cadre de robotique autonome. Le but est de permettre au robot de tirer parti de la richesse de son environnement sans que cette richesse ne le noie sous trop d'information à traiter. L'idée principale testée ici est de faire croître les capacités perceptives et motrices de l'agent au fur et à mesure de l'apprentissage. Cette approche est combinée avec un apprentissage par renforcement classique (Q-Learning) s'appuyant sur un approximateur de fonction non-linéaire (perceptron multi-couche). Au travers d'expériences encore préliminaires, nous montrons le potentiel de cette approche.",
    "uri": "https://inria.hal.science/inria-00633426v1",
    "texte_nettoye": "Cet article présente une approche développementale de l'apprentissage par renforcement dans un cadre de robotique autonome. Le but est de permettre au robot de tirer parti de la richesse de son environnement sans que cette richesse ne le noie sous trop d'information à traiter. L'idée principale testée ici est de faire croître les capacités perceptives et motrices de l'agent au fur et à mesure de l'apprentissage. Cette approche est combinée avec un apprentissage par renforcement classique (Q-Learning) s'appuyant sur un approximateur de fonction non-linéaire (perceptron multi-couche). Au travers d'expériences encore préliminaires, nous montrons le potentiel de cette approche."
  },
  {
    "title": "Apprentissage symbolique et interprétation de gels d'électrophorèse bidimensionnelle",
    "authors": [
      "Pierre Nugues",
      "Robert Whalen",
      "Jean-Paul Haton"
    ],
    "year": 1989,
    "abstract": "Nous présentons une application des techniques d'apprentissage symbolique. Cette application permet l'extraction automatique et la formalisation d'expertise biologique. Elle a pour support une méthode d'analyse de protéines- Ê: l'électrophorèse bidimensionnelle. Cette dernière produit des gels plans dont on extrait les paramètres numériques par traitement d'image. Grâce à la méthode que nous exposons, nous identifions, à partir des paramètres, des protéines qui peuvent jouer un rôle dans les transitions entre les différentes étapes de la maturation musculaire.",
    "uri": "https://inria.hal.science/inria-00075431v1",
    "texte_nettoye": "Nous présentons une application des techniques d'apprentissage symbolique. Cette application permet l'extraction automatique et la formalisation d'expertise biologique. Elle a pour support une méthode d'analyse de protéines- Ê: l'électrophorèse bidimensionnelle. Cette dernière produit des gels plans dont on extrait les paramètres numériques par traitement d'image. Grâce à la méthode que nous exposons, nous identifions, à partir des paramètres, des protéines qui peuvent jouer un rôle dans les transitions entre les différentes étapes de la maturation musculaire."
  },
  {
    "title": "Les avis sur les restaurants à l’épreuve de l’apprentissage automatique",
    "authors": [
      "Hyun Jung Kang",
      "Iris Eshkol-Taravella"
    ],
    "year": 2020,
    "abstract": "Dans la fouille d’opinions, de nombreuses études portent sur l’extraction automatique des opinions positives ou négatives. Cependant les recherches ayant pour objet la fouille de suggestions et d’intentions sont moins importantes, malgré leur lien profond avec l’opinion. Cet article vise à détecter six catégories (opinion positive/mixte/négative, suggestion, intention, description) dans les avis en ligne sur les restaurants en exploitant deux méthodes : l’apprentissage de surface et l’apprentissage profond supervisés. Les performances obtenues pour chaque catégorie sont interprétées ensuite en tenant compte des spécificités du corpus traité.",
    "uri": "https://hal.science/hal-02784774v3",
    "texte_nettoye": "Dans la fouille d’opinions, de nombreuses études portent sur l’extraction automatique des opinions positives ou négatives. Cependant les recherches ayant pour objet la fouille de suggestions et d’intentions sont moins importantes, malgré leur lien profond avec l’opinion. Cet article vise à détecter six catégories (opinion positive/mixte/négative, suggestion, intention, description) dans les avis en ligne sur les restaurants en exploitant deux méthodes : l’apprentissage de surface et l’apprentissage profond supervisés. Les performances obtenues pour chaque catégorie sont interprétées ensuite en tenant compte des spécificités du corpus traité."
  },
  {
    "title": "Classification par défaut à base de stéréotypes",
    "authors": [
      "Julien Velcin"
    ],
    "year": 2006,
    "abstract": "Je présente dans cet article un modèle de classification automatique adapté aux données catégorielles présentant un fort taux de données manquantes. Ce modèle s’appuie sur le concept de stéréotype et cherche, en utilisant une méta-heuristique de recherche taboue, à trouver l’ensemble de stéréotypes qui caractérise le mieux l’ensemble d’apprentissage. Une première évaluation utilise des jeux de données artificiels et permet de montrer la robustesse des résultats obtenus comparativement à trois algorithmes classiques de clustering. L’application réelle considérée consiste à extraire automatiquement des stéréotypes à partir d’articles de presse. Elle montre la pertinence de notre modèle par rapport aux autres approches existantes.",
    "uri": "https://hal.science/hal-01352072v1",
    "texte_nettoye": "Je présente dans cet article un modèle de classification automatique adapté aux données catégorielles présentant un fort taux de données manquantes. Ce modèle s’appuie sur le concept de stéréotype et cherche, en utilisant une méta-heuristique de recherche taboue, à trouver l’ensemble de stéréotypes qui caractérise le mieux l’ensemble d’apprentissage. Une première évaluation utilise des jeux de données artificiels et permet de montrer la robustesse des résultats obtenus comparativement à trois algorithmes classiques de clustering. L’application réelle considérée consiste à extraire automatiquement des stéréotypes à partir d’articles de presse. Elle montre la pertinence de notre modèle par rapport aux autres approches existantes."
  },
  {
    "title": "Classification automatique pour la compréhension de la parole : vers des systèmes semi-supervisés et auto-évolutifs",
    "authors": [
      "Pierre Gotab"
    ],
    "year": 2012,
    "abstract": "La compréhension automatique de la parole est au confluent des deux grands domaines que sont la reconnaissance automatique de la parole et l'apprentissage automatique. Un des problèmes majeurs dans ce domaine est l'obtention d'un corpus de données conséquent afin d'obtenir des modèles statistiques performants. Les corpus de parole pour entraîner des modèles de compréhension nécessitent une intervention humaine importante, notamment dans les tâches de transcription et d'annotation sémantique. Leur coût de production est élevé et c'est la raison pour laquelle ils sont disponibles en quantité limitée.Cette thèse vise principalement à réduire ce besoin d'intervention humaine de deux façons : d'une part en réduisant la quantité de corpus annoté nécessaire à l'obtention d'un modèle grâce à des techniques d'apprentissage semi-supervisé (Self-Training, Co-Training et Active-Learning) ; et d'autre part en tirant parti des réponses de l'utilisateur du système pour améliorer le modèle de compréhension.Ce dernier point touche à un second problème rencontré par les systèmes de compréhension automatique de la parole et adressé par cette thèse : le besoin d'adapter régulièrement leurs modèles aux variations de comportement des utilisateurs ou aux modifications de l'offre de services du système",
    "uri": "https://theses.hal.science/tel-00858980v1",
    "texte_nettoye": "La compréhension automatique de la parole est au confluent des deux grands domaines que sont la reconnaissance automatique de la parole et l'apprentissage automatique. Un des problèmes majeurs dans ce domaine est l'obtention d'un corpus de données conséquent afin d'obtenir des modèles statistiques performants. Les corpus de parole pour entraîner des modèles de compréhension nécessitent une intervention humaine importante, notamment dans les tâches de transcription et d'annotation sémantique. Leur coût de production est élevé et c'est la raison pour laquelle ils sont disponibles en quantité limitée.Cette thèse vise principalement à réduire ce besoin d'intervention humaine de deux façons : d'une part en réduisant la quantité de corpus annoté nécessaire à l'obtention d'un modèle grâce à des techniques d'apprentissage semi-supervisé (Self-Training, Co-Training et Active-Learning) ; et d'autre part en tirant parti des réponses de l'utilisateur du système pour améliorer le modèle de compréhension.Ce dernier point touche à un second problème rencontré par les systèmes de compréhension automatique de la parole et adressé par cette thèse : le besoin d'adapter régulièrement leurs modèles aux variations de comportement des utilisateurs ou aux modifications de l'offre de services du système"
  },
  {
    "title": "Apprentissage progressif pour la reconnaissance de symboles dans les documents graphiques",
    "authors": [
      "Sabine Barrat",
      "Salvatore Tabbone"
    ],
    "year": 2006,
    "abstract": "Les méthodes actuelles de reconnaissance de symboles donnent de bons résultats quand il s'agit de reconnaître peu de symboles différents qui sont peu bruités et souvent déconnectés du graphique. Cependant, dans le cas d'applications réelles, les méthodes sont encore mal maîtrisées quand il s'agit de discriminer dans de grandes bases entre plusieurs centaines de symboles différents, souvent complexes et bruités et encapsulés dans les couches graphiques. Dans ce contexte il est nécessaire de mettre en oeuvre des méthodes d'apprentissage. Nous présentons dans cet article une méthode d'apprentissage progressif pour la reconnaissance de symboles qui améliore son propre taux de reconnaissance au fur et à mesure que de nouveaux symboles sont reconnus dans les documents. Pour ce faire, nous proposons une nouvelle exploitation de l'analyse discriminante qui fournit des règles d'affectation à partir d'un échantillon d'apprentissage sur lequel les appartenances aux classes sont connues (apprentissage supervisé). Mais cette méthode ne se révèle efficace que si l'échantillon d'apprentissage et les données ultérieures sont observés dans les mêmes conditions. Or cette hypothèse est rarement vérifiée dans les conditions réelles. Pour pallier ce problème, nous avons adapté une approche récente d'analyse discriminante conditionnelle qui ajoute à chaque observation l'observation d'un vecteur aléatoire, représentatif des effets parasites observés dans l'analyse discriminante classique.",
    "uri": "https://inria.hal.science/inria-00118259v1",
    "texte_nettoye": "Les méthodes actuelles de reconnaissance de symboles donnent de bons résultats quand il s'agit de reconnaître peu de symboles différents qui sont peu bruités et souvent déconnectés du graphique. Cependant, dans le cas d'applications réelles, les méthodes sont encore mal maîtrisées quand il s'agit de discriminer dans de grandes bases entre plusieurs centaines de symboles différents, souvent complexes et bruités et encapsulés dans les couches graphiques. Dans ce contexte il est nécessaire de mettre en oeuvre des méthodes d'apprentissage. Nous présentons dans cet article une méthode d'apprentissage progressif pour la reconnaissance de symboles qui améliore son propre taux de reconnaissance au fur et à mesure que de nouveaux symboles sont reconnus dans les documents. Pour ce faire, nous proposons une nouvelle exploitation de l'analyse discriminante qui fournit des règles d'affectation à partir d'un échantillon d'apprentissage sur lequel les appartenances aux classes sont connues (apprentissage supervisé). Mais cette méthode ne se révèle efficace que si l'échantillon d'apprentissage et les données ultérieures sont observés dans les mêmes conditions. Or cette hypothèse est rarement vérifiée dans les conditions réelles. Pour pallier ce problème, nous avons adapté une approche récente d'analyse discriminante conditionnelle qui ajoute à chaque observation l'observation d'un vecteur aléatoire, représentatif des effets parasites observés dans l'analyse discriminante classique."
  },
  {
    "title": "Suivis de biodiversité par la rec﻿onnaissance automatique des espèces sur photographies : perspectives et défis",
    "authors": [
      "Hélène Le Borgne",
      "Christophe Bouget"
    ],
    "year": 2023,
    "abstract": "La reconnaissance d’espèces basée sur des données d’images analysées par l’intelligence artificielle est de plus en plus populaire dans les suivis de biodiversité, pour faire face aux limites des méthodes plus traditionnelles et à l’émergence de considérations déontologiques préconisant le développement de pièges non destructifs (i.e. non létaux, « no kill »). Cette augmentation dans l’utilisation de nouvelles technologies peut largement s’expliquer par un besoin de gain en temps et en précision. Ce type de méthodologie est particulièrement intéressant pour les personnes qui n’ont pas l’expertise nécessaire pour distinguer de nombreuses espèces telles que les Insectes. De plus, les données photographiques sont moins susceptibles de créer un biais observateur que l’observation directe, car elles sont réutilisables et vérifiables. Dans ce document nous allons voir comment les données peuvent être acquises en milieu terrestre (i.e. méthodologies et outils de capture) et la manière dont les images sont ensuite traitées pour la classification des espèces (i.e. gestion des données et analyses). En particulier, nous avons considéré la possibilité d’automatiser les grands volumes de données collectées à l’aide de techniques d’apprentissage automatique et d’apprentissage profond afin de réaliser l’identification des espèces. Cette étude présente également les avantages et les limites de l’utilisation de ces outils pour l’identification automatique des espèces dans un contexte de suivi de biodiversité en milieu terrestre.",
    "uri": "https://hal.inrae.fr/hal-04184907v1",
    "texte_nettoye": "La reconnaissance d’espèces basée sur des données d’images analysées par l’intelligence artificielle est de plus en plus populaire dans les suivis de biodiversité, pour faire face aux limites des méthodes plus traditionnelles et à l’émergence de considérations déontologiques préconisant le développement de pièges non destructifs (i.e. non létaux, « no kill »). Cette augmentation dans l’utilisation de nouvelles technologies peut largement s’expliquer par un besoin de gain en temps et en précision. Ce type de méthodologie est particulièrement intéressant pour les personnes qui n’ont pas l’expertise nécessaire pour distinguer de nombreuses espèces telles que les Insectes. De plus, les données photographiques sont moins susceptibles de créer un biais observateur que l’observation directe, car elles sont réutilisables et vérifiables. Dans ce document nous allons voir comment les données peuvent être acquises en milieu terrestre (i.e. méthodologies et outils de capture) et la manière dont les images sont ensuite traitées pour la classification des espèces (i.e. gestion des données et analyses). En particulier, nous avons considéré la possibilité d’automatiser les grands volumes de données collectées à l’aide de techniques d’apprentissage automatique et d’apprentissage profond afin de réaliser l’identification des espèces. Cette étude présente également les avantages et les limites de l’utilisation de ces outils pour l’identification automatique des espèces dans un contexte de suivi de biodiversité en milieu terrestre."
  },
  {
    "title": "L'apprentissage automatique : principes et exemple d'application au domaine de la sécurité",
    "authors": [
      "Habib Hadj-Mabrouk"
    ],
    "year": 1995,
    "abstract": "Ce papier présente une étude bibliographique des diverses méthodes, techniques et systèmes d'apprentissage automatique. Le but est de montrer dans quelle mesure les travaux effectués dans ce domaine ont pu contribuer au développement de deux outils d'apprentissage \"Clasca\" et \"Evalsca\" dédiés respectivement à la classification et à l'évaluation des scénarios contraires à la sécurité. L'objectif visé est d'alléger la tâche du spécialiste préposé à l'examen de la complétude et de la cohérence des analyses de sécurité des systèmes de transport terrestres guidés.",
    "uri": "https://hal.science/hal-03027893v1",
    "texte_nettoye": "Ce papier présente une étude bibliographique des diverses méthodes, techniques et systèmes d'apprentissage automatique. Le but est de montrer dans quelle mesure les travaux effectués dans ce domaine ont pu contribuer au développement de deux outils d'apprentissage \"Clasca\" et \"Evalsca\" dédiés respectivement à la classification et à l'évaluation des scénarios contraires à la sécurité. L'objectif visé est d'alléger la tâche du spécialiste préposé à l'examen de la complétude et de la cohérence des analyses de sécurité des systèmes de transport terrestres guidés."
  },
  {
    "title": "Un modèle d'apprentissage multimodal pour un substrat distribué d'inspiration corticale",
    "authors": [
      "Thomas Girod"
    ],
    "year": 2010,
    "abstract": "Le domaine des neurosciences computationnelles s'intéresse à la modélisation des fonctions cognitives à travers des modèles numériques bio-inspirés. Dans cette thèse, nous nous intéressons en particulier à l'apprentissage dans un contexte multimodal, c'est à dire à la formation de représentations cohérentes à partir de plusieurs modalités sensorielles et/ou motrices. Notre modèle s'inspire du cortex cérébral, lieu supposé de la fusion multimodale dans le cerveau, et le représente à une échelle mésoscopique par des colonnes corticales regroupées en cartes et des projections axoniques entre ces cartes. Pour effectuer nos simulations, nous proposons une bibliothèque simplifiant la construction et l'évaluation de modèles mésoscopiques. Notre modèle d'apprentissage se base sur le modèle BCM (Bienenstock-Cooper-Munro), qui propose un algorithme d'apprentissage non-supervisé local (une unité apprend à partir de ses entrées de manière autonome) et biologiquement plausible. Nous adaptons BCM en introduisant la notion d'apprentissage guidé, un moyen de biaiser la convergence de l'apprentissage BCM en faveur d'un stimulus choisi. Puis, nous mettons ce mécanisme à profit pour effectuer un co-apprentissage entre plusieurs modalités. Grâce au co-apprentissage, les sélectivités développées sur chaque modalité tendent à représenter le même phénomène, perçu à travers différentes modalités, élaborant ainsi une représentation multimodale cohérente dudit phénomène.",
    "uri": "https://theses.hal.science/tel-00547941v1",
    "texte_nettoye": "Le domaine des neurosciences computationnelles s'intéresse à la modélisation des fonctions cognitives à travers des modèles numériques bio-inspirés. Dans cette thèse, nous nous intéressons en particulier à l'apprentissage dans un contexte multimodal, c'est à dire à la formation de représentations cohérentes à partir de plusieurs modalités sensorielles et/ou motrices. Notre modèle s'inspire du cortex cérébral, lieu supposé de la fusion multimodale dans le cerveau, et le représente à une échelle mésoscopique par des colonnes corticales regroupées en cartes et des projections axoniques entre ces cartes. Pour effectuer nos simulations, nous proposons une bibliothèque simplifiant la construction et l'évaluation de modèles mésoscopiques. Notre modèle d'apprentissage se base sur le modèle BCM (Bienenstock-Cooper-Munro), qui propose un algorithme d'apprentissage non-supervisé local (une unité apprend à partir de ses entrées de manière autonome) et biologiquement plausible. Nous adaptons BCM en introduisant la notion d'apprentissage guidé, un moyen de biaiser la convergence de l'apprentissage BCM en faveur d'un stimulus choisi. Puis, nous mettons ce mécanisme à profit pour effectuer un co-apprentissage entre plusieurs modalités. Grâce au co-apprentissage, les sélectivités développées sur chaque modalité tendent à représenter le même phénomène, perçu à travers différentes modalités, élaborant ainsi une représentation multimodale cohérente dudit phénomène."
  },
  {
    "title": "Introduction des techniques d'apprentissage automatique et d'acquisition des connaissances dans l'analyse de sécurité des transports guidés",
    "authors": [
      "Habib Hadj-Mabrouk"
    ],
    "year": 1994,
    "abstract": "Cet article présente une contribution au renforcement des méthodes usuelles d'analyse de la sécurité employées dans le cadre de la certification des systèmes de transport automatisés (STA). La méthodologie d'aide à l'analyse de sécurité développée repose sur l'utilisation conjointe et complémentaire des techniques d'acquisition des connaissances et d'apprentissage automatique. ACASYA est l'environnement logiciel développé pour supporter cette méthodologie. Il est composé de deux modules principaux : \"Clasca\" et \"Evalsca\", respectivement dédiés à la classification et à l'évaluation des scénarios d'accidents. \"Clasca\" est un système d'apprentissage symbolique-numérique, inductif, incrémental, non monotone et interactif. \"Evalsca\", développé autour du système d'apprentissage de règles \"Charade\", a pour objectif de suggérer aux analystes d'éventuelles pannes non considérées par le constructeur et susceptibles de mettre en défaut la sécurité d'un nouveau STA. Par opposition aux systèmes d'aide au diagnostic, \"Acasya\" peut être perçu comme un outil d'aide à la prévention des accidents dès le stade de conception du STA.",
    "uri": "https://hal.science/hal-03022317v1",
    "texte_nettoye": "Cet article présente une contribution au renforcement des méthodes usuelles d'analyse de la sécurité employées dans le cadre de la certification des systèmes de transport automatisés (STA). La méthodologie d'aide à l'analyse de sécurité développée repose sur l'utilisation conjointe et complémentaire des techniques d'acquisition des connaissances et d'apprentissage automatique. ACASYA est l'environnement logiciel développé pour supporter cette méthodologie. Il est composé de deux modules principaux : \"Clasca\" et \"Evalsca\", respectivement dédiés à la classification et à l'évaluation des scénarios d'accidents. \"Clasca\" est un système d'apprentissage symbolique-numérique, inductif, incrémental, non monotone et interactif. \"Evalsca\", développé autour du système d'apprentissage de règles \"Charade\", a pour objectif de suggérer aux analystes d'éventuelles pannes non considérées par le constructeur et susceptibles de mettre en défaut la sécurité d'un nouveau STA. Par opposition aux systèmes d'aide au diagnostic, \"Acasya\" peut être perçu comme un outil d'aide à la prévention des accidents dès le stade de conception du STA."
  },
  {
    "title": "Accidents du travail dans l’UE-15 et méthodes d’apprentissage automatique »",
    "authors": [
      "Marie Germaine Mbome"
    ],
    "year": 2020,
    "abstract": "Les méthodes d’apprentissage automatique ont été utilisées comme outil de prédiction dans de nombreux domaines, mais leur utilisation en santé et sécurité au travail est relativement nouvelle. C’est la raison pour laquelle, il serait intéressant d’utiliser ces méthodes dans la prévision des accidents du travail, sur les données d’enquêtes européennes sur les conditions de travail. L’objectif de ce travail est de tester les performances des techniques d’apprentissage automatique dans la modélisation et la prédiction d’accidents du travail. A cette fin, nous utiliserons trois modèles : (les forêts aléatoires) (RF), Support Vector Machine (SVM), modèle logistique. Nous observons que, la performance des modèles dépend des critères d’évaluation choisis",
    "uri": "https://ube.hal.science/hal-03722488v1",
    "texte_nettoye": "Les méthodes d’apprentissage automatique ont été utilisées comme outil de prédiction dans de nombreux domaines, mais leur utilisation en santé et sécurité au travail est relativement nouvelle. C’est la raison pour laquelle, il serait intéressant d’utiliser ces méthodes dans la prévision des accidents du travail, sur les données d’enquêtes européennes sur les conditions de travail. L’objectif de ce travail est de tester les performances des techniques d’apprentissage automatique dans la modélisation et la prédiction d’accidents du travail. A cette fin, nous utiliserons trois modèles : (les forêts aléatoires) (RF), Support Vector Machine (SVM), modèle logistique. Nous observons que, la performance des modèles dépend des critères d’évaluation choisis"
  },
  {
    "title": "Procédé d’apprentissage automatique et réseau connexionniste multi-couches pour la mise en œuvre de ce procédé",
    "authors": [
      "Gilles Burel"
    ],
    "year": 1990,
    "abstract": "Le procédé d’apprentissage automatique consistant à modifier des coefficients de pondération d'un réseau de neurones multi-couches de façon à ce que les sorties du réseau de neurones soient représentatives d’un groupe de données, est caractérisé en ce que la vitesse d’apprentissage est ajustée en début d'apprentissage en fixant une valeur de décroissance relative d'une fonction d'erreur entre les valeurs de sorties obtenues et les valeurs de sorties désirées, puis gelée en fin d'apprentissage. En outre, le procédé consiste à imposer une limite supérieure à l’affluence des modifications des coefficients de pondération dues à la présentation d’un exemple sur une quantité, appelée potentiel neuronal, correspondant à un autre exemple et à interdire les modifications des coefficients de pondération qui tendent à accroître de façon trop importante la valeur absolue du potentiel neuronal. Application au traitement de la parole et de l'image, à la robotique, au traitement du signal.",
    "uri": "https://hal.science/hal-04049517v1",
    "texte_nettoye": "Le procédé d’apprentissage automatique consistant à modifier des coefficients de pondération d'un réseau de neurones multi-couches de façon à ce que les sorties du réseau de neurones soient représentatives d’un groupe de données, est caractérisé en ce que la vitesse d’apprentissage est ajustée en début d'apprentissage en fixant une valeur de décroissance relative d'une fonction d'erreur entre les valeurs de sorties obtenues et les valeurs de sorties désirées, puis gelée en fin d'apprentissage. En outre, le procédé consiste à imposer une limite supérieure à l’affluence des modifications des coefficients de pondération dues à la présentation d’un exemple sur une quantité, appelée potentiel neuronal, correspondant à un autre exemple et à interdire les modifications des coefficients de pondération qui tendent à accroître de façon trop importante la valeur absolue du potentiel neuronal. Application au traitement de la parole et de l'image, à la robotique, au traitement du signal."
  },
  {
    "title": "Apprentissage d'une discrétisation pour construire une politique à partir d'exemples",
    "authors": [
      "Cédric Rose",
      "François Charpillet"
    ],
    "year": 2009,
    "abstract": "Nous présentons dans cet article une approche permettant d'apprendre une politique de contrôle à partir de l'observation d'un expert manipulant le système. Dans le cas où les actions peuvent s'avérer critiques pour le système, par exemple dans des applications médicales où bien même en robotique, l'exploration du système peut ou doit être confiée à un humain. Les actions dangereuses pourront ainsi être évitée avec la contrepartie que l'exploration restera partielle et que le nombre de trajectoires utilisables pour l'apprentissage sera limité. Nous nous intéressons ici à l'impacte du choix de l'espace d'états sur l'apprentissage de la politique dans le cas particulier d'un nombre limité d'échantillons d'apprentissage et nous proposons l'utilisation du critère de vraisemblance pour apprendre une discrétisation sous la forme d'un réseau bayésien dynamique. Ce modèle sert ensuite de support à l'apprentissage d'une politique de contrôle. L'algorithme QD-Iteration, qui est une version itérative hors ligne de QLearning, est introduit pour apprendre la politique à partir des trajectoires fournies par l'expert humain. Le problème du pendule sur le chariot est utilisé pour illustrer et tester l'approche.",
    "uri": "https://inria.hal.science/inria-00439677v1",
    "texte_nettoye": "Nous présentons dans cet article une approche permettant d'apprendre une politique de contrôle à partir de l'observation d'un expert manipulant le système. Dans le cas où les actions peuvent s'avérer critiques pour le système, par exemple dans des applications médicales où bien même en robotique, l'exploration du système peut ou doit être confiée à un humain. Les actions dangereuses pourront ainsi être évitée avec la contrepartie que l'exploration restera partielle et que le nombre de trajectoires utilisables pour l'apprentissage sera limité. Nous nous intéressons ici à l'impacte du choix de l'espace d'états sur l'apprentissage de la politique dans le cas particulier d'un nombre limité d'échantillons d'apprentissage et nous proposons l'utilisation du critère de vraisemblance pour apprendre une discrétisation sous la forme d'un réseau bayésien dynamique. Ce modèle sert ensuite de support à l'apprentissage d'une politique de contrôle. L'algorithme QD-Iteration, qui est une version itérative hors ligne de QLearning, est introduit pour apprendre la politique à partir des trajectoires fournies par l'expert humain. Le problème du pendule sur le chariot est utilisé pour illustrer et tester l'approche."
  },
  {
    "title": "Extraction d’information de spécialité avec un système commercial générique",
    "authors": [
      "Clothilde Royan",
      "Jean-Marc Langé",
      "Zied Abidi"
    ],
    "year": 2020,
    "abstract": "Nous avons participé à la tâche 3 du Défi Fouille de texte 2020, dédiée à l’extraction d’information de spécialité, dans le but de tester notre produit commercial d’extraction d’information, Watson Knowledge Studio (WKS), face à des équipes académiques et industrielles. Outre la quantité réduite de données d’apprentissage, la nature des annotations des corpus de référence posait des problèmes d’adaptation à notre produit. Aussi avons-nous dû modifier le schéma d’annotation du corpus d’apprentissage, exécuter l’apprentissage, puis appliquer des règles aux résultats obtenus afin d’obtenir des annotations conformes au schéma initial. Nous avons également appliqué des dictionnaires de spécialité (anatomie, pathologie, etc.) pour injecter de la connaissance du domaine et renforcer les modèles d’apprentissage automatique. Au final, nos résultats lors de la phase de test se situent dans la moyenne de l’ensemble des équipes, avec des F-mesures de 0,43 pour la sous-tâche 1 et 0,63 pour la sous-tâche 2.",
    "uri": "https://hal.science/hal-02784744v3",
    "texte_nettoye": "Nous avons participé à la tâche 3 du Défi Fouille de texte 2020, dédiée à l’extraction d’information de spécialité, dans le but de tester notre produit commercial d’extraction d’information, Watson Knowledge Studio (WKS), face à des équipes académiques et industrielles. Outre la quantité réduite de données d’apprentissage, la nature des annotations des corpus de référence posait des problèmes d’adaptation à notre produit. Aussi avons-nous dû modifier le schéma d’annotation du corpus d’apprentissage, exécuter l’apprentissage, puis appliquer des règles aux résultats obtenus afin d’obtenir des annotations conformes au schéma initial. Nous avons également appliqué des dictionnaires de spécialité (anatomie, pathologie, etc.) pour injecter de la connaissance du domaine et renforcer les modèles d’apprentissage automatique. Au final, nos résultats lors de la phase de test se situent dans la moyenne de l’ensemble des équipes, avec des F-mesures de 0,43 pour la sous-tâche 1 et 0,63 pour la sous-tâche 2."
  },
  {
    "title": "Génération et Évaluation de Visualisations avec des techniques d'Apprentissage Automatique",
    "authors": [
      "Loann Giovannangeli"
    ],
    "year": 2023,
    "abstract": "L'essor de l'Internet des Objets, des modèles de stockage et de traitement des données a conduit à une explosion de la quantité et de la complexité des données que nous collectons aujourd'hui. Pour mieux les comprendre et les manipuler, les experts ont recours à des visualisations de l'information. Cependant, la complexité nouvelle des données rend inefficace des techniques de visualisation utilisées jusqu'alors. Il est donc nécessaire de concevoir de nouvelles techniques de visualisation et de revoir les méthodes d'évaluation qui permettent de mesurer leur efficacité.Cette thèse présente des contributions sur deux aspects principaux du domaine de la Visualisation d'Information : la génération et l'évaluation automatique de visualisations. Pour ces deux axes, nos travaux tirent parti des techniques d'apprentissage automatique, notamment profond, qui ont démontré leurs capacités à traiter efficacement des grands volumes de données. Les cas d'applications des contributions présentées dans ce manuscrit utilisent des modèles d'apprentissage automatique pour le Dessin de Graphe par représentation Nœud-Lien, la Suppression de Chevauchements dans des nuages de points, et l'Évaluation automatique de Visualisations.",
    "uri": "https://theses.hal.science/tel-04312123v1",
    "texte_nettoye": "L'essor de l'Internet des Objets, des modèles de stockage et de traitement des données a conduit à une explosion de la quantité et de la complexité des données que nous collectons aujourd'hui. Pour mieux les comprendre et les manipuler, les experts ont recours à des visualisations de l'information. Cependant, la complexité nouvelle des données rend inefficace des techniques de visualisation utilisées jusqu'alors. Il est donc nécessaire de concevoir de nouvelles techniques de visualisation et de revoir les méthodes d'évaluation qui permettent de mesurer leur efficacité.Cette thèse présente des contributions sur deux aspects principaux du domaine de la Visualisation d'Information : la génération et l'évaluation automatique de visualisations. Pour ces deux axes, nos travaux tirent parti des techniques d'apprentissage automatique, notamment profond, qui ont démontré leurs capacités à traiter efficacement des grands volumes de données. Les cas d'applications des contributions présentées dans ce manuscrit utilisent des modèles d'apprentissage automatique pour le Dessin de Graphe par représentation Nœud-Lien, la Suppression de Chevauchements dans des nuages de points, et l'Évaluation automatique de Visualisations."
  },
  {
    "title": "Un cadre conceptuel et logiciel pour la construction d'environnements d'apprentissage collaboratifs",
    "authors": [
      "Jacques Lonchamp"
    ],
    "year": 2008,
    "abstract": "La diffusion effective de l'apprentissage collaboratif assisté par ordinateur exige de passer d'une première génération d'outils ad hoc, spécialisés et fermés à des environnements beaucoup plus génériques, ouverts et malléables. Ces environnements doivent en outre s'intégrer à des ensembles plus vastes dédiés à l'accompagnement des communautés d'intérêt et de pratique indispensables pour former et guider concrètement les enseignants concernés. Cet article définit un cadre conceptuel pour l'apprentissage collaboratif puis une architecture fonctionnelle reflétant cette vision et enfin un système qui implante cette architecture.",
    "uri": "https://inria.hal.science/inria-00291828v1",
    "texte_nettoye": "La diffusion effective de l'apprentissage collaboratif assisté par ordinateur exige de passer d'une première génération d'outils ad hoc, spécialisés et fermés à des environnements beaucoup plus génériques, ouverts et malléables. Ces environnements doivent en outre s'intégrer à des ensembles plus vastes dédiés à l'accompagnement des communautés d'intérêt et de pratique indispensables pour former et guider concrètement les enseignants concernés. Cet article définit un cadre conceptuel pour l'apprentissage collaboratif puis une architecture fonctionnelle reflétant cette vision et enfin un système qui implante cette architecture."
  },
  {
    "title": "Un (petit) pas vers l’explicabilité de l’intelligence artificielle",
    "authors": [
      "Romain Giot"
    ],
    "year": 2024,
    "abstract": "Ce manuscrit retrace une partie de mes contributions depuis mon arrivée au LaBRI en septembre 2013, ainsi que les perspectives de recherche pour les prochaines années. Les aspects principaux de ma recherche concernent : - L'analyse de données et l'apprentissage automatique. Je présente des travaux liés à l'authentification biométrique et l'attribution de caractéristiques dans les réseaux de neurones profonds. - La visualisation d'informations. Je décris des travaux sur le dessin de grands graphes et la représentation de données dans des grilles. - La symbiose entre la visualisation d'informations et l'apprentissage automatique. Je discute des travaux sur la visualisation pour l'apprentissage (en authentification biométrique, analyse de base étiquetée et visualisation pour les réseaux de neurones profonds), et l'apprentissage pour la visualisation (pour la comparaison automatique de techniques de visualisation, le dessin de graphes et la suppression de chevauchements dans les nœuds en dessin de graphe). Les perspectives de recherche se focalisent sur cette thématique.",
    "uri": "https://theses.hal.science/tel-04463271v2",
    "texte_nettoye": "Ce manuscrit retrace une partie de mes contributions depuis mon arrivée au LaBRI en septembre 2013, ainsi que les perspectives de recherche pour les prochaines années. Les aspects principaux de ma recherche concernent : - L'analyse de données et l'apprentissage automatique. Je présente des travaux liés à l'authentification biométrique et l'attribution de caractéristiques dans les réseaux de neurones profonds. - La visualisation d'informations. Je décris des travaux sur le dessin de grands graphes et la représentation de données dans des grilles. - La symbiose entre la visualisation d'informations et l'apprentissage automatique. Je discute des travaux sur la visualisation pour l'apprentissage (en authentification biométrique, analyse de base étiquetée et visualisation pour les réseaux de neurones profonds), et l'apprentissage pour la visualisation (pour la comparaison automatique de techniques de visualisation, le dessin de graphes et la suppression de chevauchements dans les nœuds en dessin de graphe). Les perspectives de recherche se focalisent sur cette thématique."
  },
  {
    "title": "Revue des principales approches de résolution du problème de déséquilibre des classes",
    "authors": [
      "Emmanuel Remy",
      "Vanessa Verges",
      "Emilie Dautreme",
      "Bruna Martin-Cabanas"
    ],
    "year": 2020,
    "abstract": "Cet article propose une revue bibliographique des principales approches de résolution du problème de déséquilibre des classes en apprentissage automatique.",
    "uri": "https://hal.science/hal-03480661v1",
    "texte_nettoye": "Cet article propose une revue bibliographique des principales approches de résolution du problème de déséquilibre des classes en apprentissage automatique."
  },
  {
    "title": "Contributions aux ombres et jumeaux numériques dans l’industrie : proposition d’une stratégie de couplage entre modèles de simulation et d’apprentissage automatique appliquée aux scieries",
    "authors": [
      "Sylvain Chabanet"
    ],
    "year": 2023,
    "abstract": "Ces travaux de thèses s'inscrivent dans le projet ANR Lorraine-Intelligence Artificielle qui se veut un projet multi-disciplinaire promouvant la recherche à la fois sur l'intelligence artificielle elle-même et sur ses applications à d'autres domaines de recherche. A ce titre, cette thèse s'intéresse au développement et à l'utilisation de modèles d'apprentissage automatique comme modèles de substitution a des modèles de simulation. L'intérêt pour ce sujet de recherche est, en particulier, porté par l'engouement des milieux académiques et industriel pour le concept d'ombres et jumeaux numériques, vus comme une évolution des modèles de simulation pour une utilisation pérenne au cœur des systèmes et des processus. La contribution principale de ces travaux de thèse est la proposition d'une stratégie de couplage entre un modèle de simulation et un modèle de substitution réalisant une même tâche de prédiction de manière répétée sur un flux de données. Le modèle de simulation est supposé avoir un haut niveau de fidélité mais être trop lent ou coûteux en calcul pour être utilisé seul pour réaliser l'intégralité des prédictions requises. Le modèle de substitution est un modèle d'apprentissage automatique qui approxime le modèle de simulation. L'objectif premier de la stratégie de couplage proposée est l'utilisation efficiente des ressources en calcul limitées par l'allocation intelligente de chaque prédiction à effectuer à un des deux modèles. Cette allocation est, en particulier, inspirée de l'apprentissage actif et basée sur l'évaluation de niveaux de confiance dans les prédictions du modèle d'apprentissage automatique. Des expériences numériques sont d'abord menées sur huit jeux de données de la littérature scientifique. Une application à l'industrie du sciage est ensuite développée.",
    "uri": "https://hal.univ-lorraine.fr/tel-04257342v1",
    "texte_nettoye": "Ces travaux de thèses s'inscrivent dans le projet ANR Lorraine-Intelligence Artificielle qui se veut un projet multi-disciplinaire promouvant la recherche à la fois sur l'intelligence artificielle elle-même et sur ses applications à d'autres domaines de recherche. A ce titre, cette thèse s'intéresse au développement et à l'utilisation de modèles d'apprentissage automatique comme modèles de substitution a des modèles de simulation. L'intérêt pour ce sujet de recherche est, en particulier, porté par l'engouement des milieux académiques et industriel pour le concept d'ombres et jumeaux numériques, vus comme une évolution des modèles de simulation pour une utilisation pérenne au cœur des systèmes et des processus. La contribution principale de ces travaux de thèse est la proposition d'une stratégie de couplage entre un modèle de simulation et un modèle de substitution réalisant une même tâche de prédiction de manière répétée sur un flux de données. Le modèle de simulation est supposé avoir un haut niveau de fidélité mais être trop lent ou coûteux en calcul pour être utilisé seul pour réaliser l'intégralité des prédictions requises. Le modèle de substitution est un modèle d'apprentissage automatique qui approxime le modèle de simulation. L'objectif premier de la stratégie de couplage proposée est l'utilisation efficiente des ressources en calcul limitées par l'allocation intelligente de chaque prédiction à effectuer à un des deux modèles. Cette allocation est, en particulier, inspirée de l'apprentissage actif et basée sur l'évaluation de niveaux de confiance dans les prédictions du modèle d'apprentissage automatique. Des expériences numériques sont d'abord menées sur huit jeux de données de la littérature scientifique. Une application à l'industrie du sciage est ensuite développée."
  },
  {
    "title": "Optimisation de la Consommation d'Énergie Domestique via l'Apprentissage Automatique : Une Simulation pour Réduire les Coûts Énergétiques des Ménages",
    "authors": [
      "Raphael Kamdoum"
    ],
    "year": 2024,
    "abstract": "Dans cet article, nous présentons une approche innovante pour optimiser la consommation d'énergie dans nos maisons en utilisant un modèle de régression linéaire basé sur l'apprentissage automatique. En générant des données synthétiques pour simuler différents scénarios de consommation, notre modèle prédictif anticipe les besoins énergétiques et ajuste automatiquement les états des appareils électroménagers afin de maintenir la consommation dans des limites optimales. La simulation, réalisée sur une période de 24 heures, a montré une réduction moyenne de la consommation d'énergie de 15%, passant de 100 kWh à 85 kWh, tout en maintenant une précision de prédiction de 92%. Ces résultats démontrent l'efficacité du système proposé pour réaliser des économies d'énergie dans menages sans compromettre le confort des occupants. Notre étude souligne le potentiel de l'apprentissage automatique pour améliorer l'efficacité énergétique dans les Foyers transformes en maisons intelligentes. Des pistes d'amélioration incluent l'exploration d'algorithmes d'apprentissage plus complexes et l'intégration de données réelles pour affiner la stratégie d'optimisation. Ce travail ouvre la voie à de futures recherches et applications pratiques dans la gestion énergétique domestique. Abstract Abstract In this article, we present an innovative approach to optimize energy consumption in our homes using a machine learning-based linear regression model. By generating synthetic data to simulate different consumption scenarios, our predictive model anticipates energy needs and automatically adjusts the states of household appliances to maintain consumption within optimal limits. The simulation, conducted over a 24-hour period, showed an average reduction in energy consumption of 15%, decreasing from 100 kWh to 85 kWh, while maintaining a prediction accuracy of 92%. These results demonstrate the efficiency of the proposed system in achieving energy savings without compromising occupants' comfort. Our study highlights the potential of machine learning to improve energy efficiency in smart homes. Areas for improvement include exploring more complex learning algorithms and integrating real data to refine the optimization strategy. This work paves the way for future research and practical applications in home energy management.",
    "uri": "https://inria.hal.science/hal-04598401v1",
    "texte_nettoye": "Dans cet article, nous présentons une approche innovante pour optimiser la consommation d'énergie dans nos maisons en utilisant un modèle de régression linéaire basé sur l'apprentissage automatique. En générant des données synthétiques pour simuler différents scénarios de consommation, notre modèle prédictif anticipe les besoins énergétiques et ajuste automatiquement les états des appareils électroménagers afin de maintenir la consommation dans des limites optimales. La simulation, réalisée sur une période de 24 heures, a montré une réduction moyenne de la consommation d'énergie de 15%, passant de 100 kWh à 85 kWh, tout en maintenant une précision de prédiction de 92%. Ces résultats démontrent l'efficacité du système proposé pour réaliser des économies d'énergie dans menages sans compromettre le confort des occupants. Notre étude souligne le potentiel de l'apprentissage automatique pour améliorer l'efficacité énergétique dans les Foyers transformes en maisons intelligentes. Des pistes d'amélioration incluent l'exploration d'algorithmes d'apprentissage plus complexes et l'intégration de données réelles pour affiner la stratégie d'optimisation. Ce travail ouvre la voie à de futures recherches et applications pratiques dans la gestion énergétique domestique."
  },
  {
    "title": "Apprentissage par renforcement pour la conception de Systèmes Multi-Agents Réactifs",
    "authors": [
      "Alain Dutech",
      "Olivier Buffet",
      "François Charpillet"
    ],
    "year": 2003,
    "abstract": "Nous proposons une nouvelle méthodologie d'apprentissage par renforcement (AR) pour la conception de systèmes multi-agents réactifs. Bien que le cadre réaliste d'agents situés avec des perceptions locales sorte du cadre théorique de convergence des algorithmes classiques d'apprentissage par renforcement, notre méthode permet à chaque agent d'apprendre individuellement et localement son comportement. L'aspect progressif de notre algorithme qui met les agents en présence de sous-tâches de plus en plus complexes permet de dépasser les limitations classiques de l'AR dans ce contexte. Notre méthodologie, qui se veut générale, est validée en simulation sur un problème où les agents doivent se coordonner pour atteindre un but global.",
    "uri": "https://inria.hal.science/inria-00099564v1",
    "texte_nettoye": "Nous proposons une nouvelle méthodologie d'apprentissage par renforcement (AR) pour la conception de systèmes multi-agents réactifs. Bien que le cadre réaliste d'agents situés avec des perceptions locales sorte du cadre théorique de convergence des algorithmes classiques d'apprentissage par renforcement, notre méthode permet à chaque agent d'apprendre individuellement et localement son comportement. L'aspect progressif de notre algorithme qui met les agents en présence de sous-tâches de plus en plus complexes permet de dépasser les limitations classiques de l'AR dans ce contexte. Notre méthodologie, qui se veut générale, est validée en simulation sur un problème où les agents doivent se coordonner pour atteindre un but global."
  },
  {
    "title": "Contextualisation, Visualisation et Evaluation en Apprentissage Non Supervisé",
    "authors": [
      "Laurent Candillier"
    ],
    "year": 2006,
    "abstract": "Cette thèse se place dans le cadre de l'apprentissage non supervisé, qui consiste à former différents groupes à partir d'un ensemble de données, de telle manière que les données considérées comme les plus similaires soient associées au même groupe et qu'au contraire les données considérées comme différentes se retrouvent dans des groupes distincts, permettant ainsi d'extraire de la connaissance à partir de ces données. Nous proposons d'abord deux nouvelles méthodes qui prennent en compte le contexte dans lequel les groupes sont créés, c'est-à-dire le fait que les caractéristiques des différents groupes peuvent être définies sur différents sous-ensembles des attributs décrivant les données. Dans la mise en oeuvre de ces méthodes, nous avons également considéré les problématiques de la minimisation du nombre de connaissances a priori requises de la part de l'utilisateur et de la présentation des résultats sous forme compréhensible et visuelle. Nous présentons ensuite plusieurs extensions possibles de ces méthodes, dans le cadre de l'apprentissage supervisé puis face à des données semi-structurées représentées sous forme arborescente. Différentes expérimentations sur données artificielles puis sur données réelles sont présentées qui mettent en avant l'intérêt de ces méthodes. Le problème de l'évaluation des résultats produits par une méthode d'apprentissage non supervisé, et de la comparaison de telles méthodes, restant aujourd'hui un problème ouvert, nous proposons enfin une nouvelle méthode d'évaluation plus objective et quantitative que celles utilisées traditionnellement, et dont la pertinence est montrée expérimentalement.",
    "uri": "https://theses.hal.science/tel-00617420v1",
    "texte_nettoye": "Cette thèse se place dans le cadre de l'apprentissage non supervisé, qui consiste à former différents groupes à partir d'un ensemble de données, de telle manière que les données considérées comme les plus similaires soient associées au même groupe et qu'au contraire les données considérées comme différentes se retrouvent dans des groupes distincts, permettant ainsi d'extraire de la connaissance à partir de ces données. Nous proposons d'abord deux nouvelles méthodes qui prennent en compte le contexte dans lequel les groupes sont créés, c'est-à-dire le fait que les caractéristiques des différents groupes peuvent être définies sur différents sous-ensembles des attributs décrivant les données. Dans la mise en oeuvre de ces méthodes, nous avons également considéré les problématiques de la minimisation du nombre de connaissances a priori requises de la part de l'utilisateur et de la présentation des résultats sous forme compréhensible et visuelle. Nous présentons ensuite plusieurs extensions possibles de ces méthodes, dans le cadre de l'apprentissage supervisé puis face à des données semi-structurées représentées sous forme arborescente. Différentes expérimentations sur données artificielles puis sur données réelles sont présentées qui mettent en avant l'intérêt de ces méthodes. Le problème de l'évaluation des résultats produits par une méthode d'apprentissage non supervisé, et de la comparaison de telles méthodes, restant aujourd'hui un problème ouvert, nous proposons enfin une nouvelle méthode d'évaluation plus objective et quantitative que celles utilisées traditionnellement, et dont la pertinence est montrée expérimentalement."
  },
  {
    "title": "Principes de base en apprentissage supervisé",
    "authors": [
      "Massih-Reza Amini"
    ],
    "year": 2020,
    "abstract": "Ce document constitue le premier chapitre de l'ouvrage [1], présentant la théorie de l'apprentissage machine selon le cadre de [19] et qui a servi de base dans la description des algorithmes d'apprentissage décrits dans les chapitres suivants. Plus particulièrement, nous présentons ici la notion de consistance qui garantit l'apprenabilité d'une fonction de prédiction. Les définitions et les hypothèses de base de cette théorie, ainsi que le principe de la minimisation du risque empirique, sont décrits dans la section 1. L'étude de la consistance de ce principe, présentée dans la section 2, nous mène au second principe de la minimisation du risque structurel, qui stipule que l'apprentissage est un compromis entre une erreur empirique faible et une capacité de la classe de fonctions forte.",
    "uri": "https://hal.science/hal-03049016v1",
    "texte_nettoye": "Ce document constitue le premier chapitre de l'ouvrage [1], présentant la théorie de l'apprentissage machine selon le cadre de [19] et qui a servi de base dans la description des algorithmes d'apprentissage décrits dans les chapitres suivants. Plus particulièrement, nous présentons ici la notion de consistance qui garantit l'apprenabilité d'une fonction de prédiction. Les définitions et les hypothèses de base de cette théorie, ainsi que le principe de la minimisation du risque empirique, sont décrits dans la section 1. L'étude de la consistance de ce principe, présentée dans la section 2, nous mène au second principe de la minimisation du risque structurel, qui stipule que l'apprentissage est un compromis entre une erreur empirique faible et une capacité de la classe de fonctions forte."
  },
  {
    "title": "Apprentissage par renforcement contraint guidé par un graphe de connaissances pour personnaliser les parcours d'apprentissage",
    "authors": [
      "Rania Ait Chabane",
      "Armelle Brun",
      "Azim Roussanaly"
    ],
    "year": 2025,
    "abstract": "Ce travail présente une architecture d'apprentissage adaptatif combinant graphes de connaissances enrichis et contraintes pédagogiques dans un cadre d'apprentissage par renforcement. Le graphe est construit à partir de ressources expertes (ex. manuel scolaire) et enrichi automatiquement par un modèle de langage pour compléter les relations et inférer des contraintes. Un module de knowledge tracing estime la progression de l'apprenant vers un objectif pédagogique donné. Un agent de renforcement, entraîné en environnement simulé, recommande des activités en maximisant la progression attendue tout en respectant les contraintes. Cette approche vise à renforcer la pertinence, la diversité et l'explicabilité des parcours proposés. Une évaluation sur des jeux de données réels est prévue en travaux futurs.",
    "uri": "https://inria.hal.science/hal-05329744v1",
    "texte_nettoye": "Ce travail présente une architecture d'apprentissage adaptatif combinant graphes de connaissances enrichis et contraintes pédagogiques dans un cadre d'apprentissage par renforcement. Le graphe est construit à partir de ressources expertes (ex. manuel scolaire) et enrichi automatiquement par un modèle de langage pour compléter les relations et inférer des contraintes. Un module de knowledge tracing estime la progression de l'apprenant vers un objectif pédagogique donné. Un agent de renforcement, entraîné en environnement simulé, recommande des activités en maximisant la progression attendue tout en respectant les contraintes. Cette approche vise à renforcer la pertinence, la diversité et l'explicabilité des parcours proposés. Une évaluation sur des jeux de données réels est prévue en travaux futurs."
  },
  {
    "title": "Définitions et premières expériences en apprentissage par analogie dans les séquences",
    "authors": [
      "Laurent Miclet",
      "Sabri Bayoudh",
      "Arnaud Delhay"
    ],
    "year": 2005,
    "abstract": "Cet article donne une définition de l’analogie entre séquences fondée sur la distance d’édition et donne deux algorithmes (l’un rapide et approché, l’autre plus complexe, mais optimal) pour calculer la dissemblance analogique entre quatre séquences. Cette notion et ces algorithmes sont ensuite utilisés dans une expérience d’apprentissage sur une base de données artificielle, pour montrer la résistance au bruit de ce type d’apprentissage.",
    "uri": "https://inria.hal.science/hal-04650510v1",
    "texte_nettoye": "Cet article donne une définition de l’analogie entre séquences fondée sur la distance d’édition et donne deux algorithmes (l’un rapide et approché, l’autre plus complexe, mais optimal) pour calculer la dissemblance analogique entre quatre séquences. Cette notion et ces algorithmes sont ensuite utilisés dans une expérience d’apprentissage sur une base de données artificielle, pour montrer la résistance au bruit de ce type d’apprentissage."
  },
  {
    "title": "Analyses comparatives de productions d'apprenants du français et defrancophones, à l'aide d'outils d'extraction automatique du langage",
    "authors": [
      "Isabelle Audras",
      "Jean-Gabriel Ganascia"
    ],
    "year": 2004,
    "abstract": "",
    "uri": "https://hal.science/hal-01520490v1",
    "texte_nettoye": ""
  },
  {
    "title": "Analyses comparatives de productions d'apprenants du français et de francophones, à l'aide d'outils d'extraction automatique du langage",
    "authors": [
      "Isabelle Audras",
      "Jean-Gabriel Ganascia"
    ],
    "year": 2004,
    "abstract": "",
    "uri": "https://hal.science/hal-01416561v1",
    "texte_nettoye": ""
  },
  {
    "title": "Modéliser l'acquisition de la syntaxe du langage naturel via l'hypothèse de la primauté du sens",
    "authors": [
      "Isabelle Tellier"
    ],
    "year": 2005,
    "abstract": "L'objet de ce travail est la modélisation informatique de la capacité d'apprentissage de la syntaxe de leur langue naturelle par les enfants. Une synthèse des connaissances psycho-linguistiques sur la question est donc tout d'abord proposée. Le point de vue adopté pour la modélisation accorde une place privilégiée à la sémantique, qui est supposée acquise avant la syntaxe. Le Principe de compositionnalité, éventuellement adapté, est mis à contribution pour formaliser les liens entre syntaxe et sémantique, et le modèle d'apprentissage ''à la limite'' par exemples positifs de Gold est choisi pour régir les conditions de l'apprentissage. Nous présentons dans ce contexte divers résultats d'apprenabilité de classes de grammaires catégorielles à partir de divers types de données qui véhiculent des informations sémantiques. Nous montrons que, dans tous les cas, la sémantique contribue à spécifier les structures sous-jacentes aux énoncés, et à réduire ainsi l'espace de recherche des algorithmes d'apprentissage.",
    "uri": "https://theses.hal.science/tel-00616522v1",
    "texte_nettoye": "L'objet de ce travail est la modélisation informatique de la capacité d'apprentissage de la syntaxe de leur langue naturelle par les enfants. Une synthèse des connaissances psycho-linguistiques sur la question est donc tout d'abord proposée. Le point de vue adopté pour la modélisation accorde une place privilégiée à la sémantique, qui est supposée acquise avant la syntaxe. Le Principe de compositionnalité, éventuellement adapté, est mis à contribution pour formaliser les liens entre syntaxe et sémantique, et le modèle d'apprentissage ''à la limite'' par exemples positifs de Gold est choisi pour régir les conditions de l'apprentissage. Nous présentons dans ce contexte divers résultats d'apprenabilité de classes de grammaires catégorielles à partir de divers types de données qui véhiculent des informations sémantiques. Nous montrons que, dans tous les cas, la sémantique contribue à spécifier les structures sous-jacentes aux énoncés, et à réduire ainsi l'espace de recherche des algorithmes d'apprentissage."
  },
  {
    "title": "Crowdtuning : towards practical and reproducible auto-tuning via crowdsourcing and predictive analytics",
    "authors": [
      "Abdul Wahid Memon"
    ],
    "year": 2016,
    "abstract": "Le réglage des heuristiques d'optimisation de compilateur pour de multiples cibles ou implémentations d’une même architecture est devenu complexe. De plus, ce problème est généralement traité de façon ad-hoc et consomme beaucoup de temps sans être nécessairement reproductible. Enfin, des erreurs de choix de paramétrage d’heuristiques sont fréquentes en raison du grand nombre de possibilités d’optimisation et des interactions complexes entre tous les composants matériels et logiciels. La prise en compte de multiples exigences, comme la performance, la consommation d'énergie, la taille de code, la fiabilité et le coût, peut aussi nécessiter la gestion de plusieurs solutions candidates. La compilation itérative avec profil d’exécution (profiling feedback), le réglage automatique (auto tuning) et l'apprentissage automatique ont montré un grand potentiel pour résoudre ces problèmes. Par exemple, nous les avons utilisés avec succès pour concevoir le premier compilateur qui utilise l'apprentissage pour l'optimisation automatique de code. Il s'agit du compilateur Milepost GCC, qui apprend automatiquement les meilleures optimisations pour plusieurs programmes, données et architectures en se basant sur les caractéristiques statiques et dynamiques du programme. Malheureusement, son utilisation en pratique, a été très limitée par le temps d'apprentissage très long et le manque de benchmarks et de données représentatives. De plus, les modèles d'apprentissage «boîte noire» ne pouvaient pas représenter de façon pertinente les corrélations entre les caractéristiques des programme ou architectures et les meilleures optimisations. Dans cette thèse, nous présentons une nouvelle méthodologie et un nouvel écosystème d’outils(framework) sous la nomination Collective Mind (cM). L’objectif est de permettre à la communauté de partager les différents benchmarks, données d’entrée, compilateurs, outils et autres objets tout en formalisant et facilitant la contribution participative aux boucles d’apprentissage. Une contrainte est la reproductibilité des expérimentations pour l’ensemble des utilisateurs et plateformes. Notre cadre de travail open-source et notre dépôt (repository) public permettent de rendre le réglage automatique et l'apprentissage d’optimisations praticable. De plus, cM permet à la communauté de valider les résultats, les comportements inattendus et les modèles conduisant à de mauvaises prédictions. cM permet aussi de fournir des informations utiles pour l'amélioration et la personnalisation des modules de réglage automatique et d'apprentissage ainsi que pour l'amélioration des modèles de prévision et l'identification des éléments manquants. Notre analyse et évaluation du cadre de travail proposé montre qu'il peut effectivement exposer, isoler et identifier de façon collaborative les principales caractéristiques qui contribuent à la précision de la prédiction du modèle. En même temps, la formalisation du réglage automatique et de l'apprentissage nous permettent d'appliquer en permanence des techniques standards de réduction de complexité. Ceci permet de se contenter d'un ensemble minimal d'optimisations pertinentes ainsi que de benchmarks et de données d’entrée réellement représentatifs. Nous avons publié la plupart des résultats expérimentaux, des benchmarks et des données d’entrée à l'adresse http://c-mind.org tout en validant nos techniques dans le projet EU FP6 Milepost et durant un stage de thèse HiPEAC avec STMicroelectronics.",
    "uri": "https://theses.hal.science/tel-01395556v1",
    "texte_nettoye": "Le réglage des heuristiques d'optimisation de compilateur pour de multiples cibles ou implémentations d’une même architecture est devenu complexe. De plus, ce problème est généralement traité de façon ad-hoc et consomme beaucoup de temps sans être nécessairement reproductible. Enfin, des erreurs de choix de paramétrage d’heuristiques sont fréquentes en raison du grand nombre de possibilités d’optimisation et des interactions complexes entre tous les composants matériels et logiciels. La prise en compte de multiples exigences, comme la performance, la consommation d'énergie, la taille de code, la fiabilité et le coût, peut aussi nécessiter la gestion de plusieurs solutions candidates. La compilation itérative avec profil d’exécution (profiling feedback), le réglage automatique (auto tuning) et l'apprentissage automatique ont montré un grand potentiel pour résoudre ces problèmes. Par exemple, nous les avons utilisés avec succès pour concevoir le premier compilateur qui utilise l'apprentissage pour l'optimisation automatique de code. Il s'agit du compilateur Milepost GCC, qui apprend automatiquement les meilleures optimisations pour plusieurs programmes, données et architectures en se basant sur les caractéristiques statiques et dynamiques du programme. Malheureusement, son utilisation en pratique, a été très limitée par le temps d'apprentissage très long et le manque de benchmarks et de données représentatives. De plus, les modèles d'apprentissage «boîte noire» ne pouvaient pas représenter de façon pertinente les corrélations entre les caractéristiques des programme ou architectures et les meilleures optimisations. Dans cette thèse, nous présentons une nouvelle méthodologie et un nouvel écosystème d’outils(framework) sous la nomination Collective Mind (cM). L’objectif est de permettre à la communauté de partager les différents benchmarks, données d’entrée, compilateurs, outils et autres objets tout en formalisant et facilitant la contribution participative aux boucles d’apprentissage. Une contrainte est la reproductibilité des expérimentations pour l’ensemble des utilisateurs et plateformes. Notre cadre de travail open-source et notre dépôt (repository) public permettent de rendre le réglage automatique et l'apprentissage d’optimisations praticable. De plus, cM permet à la communauté de valider les résultats, les comportements inattendus et les modèles conduisant à de mauvaises prédictions. cM permet aussi de fournir des informations utiles pour l'amélioration et la personnalisation des modules de réglage automatique et d'apprentissage ainsi que pour l'amélioration des modèles de prévision et l'identification des éléments manquants. Notre analyse et évaluation du cadre de travail proposé montre qu'il peut effectivement exposer, isoler et identifier de façon collaborative les principales caractéristiques qui contribuent à la précision de la prédiction du modèle. En même temps, la formalisation du réglage automatique et de l'apprentissage nous permettent d'appliquer en permanence des techniques standards de réduction de complexité. Ceci permet de se contenter d'un ensemble minimal d'optimisations pertinentes ainsi que de benchmarks et de données d’entrée réellement représentatifs. Nous avons publié la plupart des résultats expérimentaux, des benchmarks et des données d’entrée à l'adresse http://c-mind.org tout en validant nos techniques dans le projet EU FP6 Milepost et durant un stage de thèse HiPEAC avec STMicroelectronics."
  },
  {
    "title": "Induction interactive d'extracteurs n-aires pour les documents semi-structurés",
    "authors": [
      "Patrick Marty"
    ],
    "year": 2007,
    "abstract": "La thèse défendue dans ce mémoire est qu'il est possible de concevoir des algorithmes d'apprentissage de programmes d'extraction n-aire pour les documents semi-structurés, qui est une classe non triviale de transformation d'arbres, de manière supervisée et avec peu d'intervention de l'utilisateur. Les documents semi-structurés ont une structure arborescente. Hors peu de systèmes d'induction supervisée d'extracteurs en tirent partie. La plupart d'entre eux considèrent les documents comme une séquence mélangeant balises et contenu [51, 42, 40, 78, 65]. Plus récemment sont apparus des algorithmes d'induction exploitant pleinement la structure d'arbre des documents semi-structurés [43, 48, 81, 12, 39, 56, 36]. Cette thèse s'inscrit dans ce courant et soutient l'idée que l'exploitation de la structure des documents semi-structurés permet d'induire des extracteurs expressifs et performants. L'induction est réalisée à l'aide d'algorithmes d'apprentissage automatique de classification supervisée. Ce choix est motivé à la fois par le succès des approches d'extractions fondée sur la classification, mais surtout par la volonté d'utiliser des algorithmes d'apprentissage existants et connus. Bien que le codage de exemples d'apprentissage en attribut-valeur prenne en compte la nature arborescente des documents semi-structurés, il est générique et intègre peu de connaissance de base. Cependant toute nouvelle connaissance est facilement intégrable. Notre représentation des données est adaptative. Dans notre approche, l'extraction n-aire est réalisée de manière incrémentale au cours d'une boucle croissante sur la taille des n-uplets. Ce procédé d'extraction ne fait aucune hypothèse sur la disposition des données dans les documents. Aucun post-traitement n'est effectué : notre algorithme réalise en même temps l'extraction des composantes et leur combinaison en n-uplets. Précisons qu'un extracteur obtenu par PaF, notre système, est utilisable tel quel, comme une boite noire, avec en entrée des documents HTML ou XML, et en sortie l'ensemble des n-uplets extraits. De plus le système PaF est implémenté dans un cadre interactif qui permet l'induction à partir d'un faible nombre d'interactions. L'utilisateur fournit quelques annotations qui servent d'amorce à l'apprentissage d'un extracteur hypothèse. Ici commence une boucle d'interaction dans laquelle l'utilisateur corrige les erreurs de l'hypothèse courante et relance l'apprentissage jusqu'à l'obtention d'une hypothèse correcte. PaF permet d'apprendre des extracteurs n-aires performants à partir de peu d'exemples. Les résultats expérimentaux montrent que PaF atteint les performances des meilleurs systèmes n-aires. De plus son procédé d'extraction reste applicable et efficace même lorsque l'organisation des données dans les documents semi-structurés est complexe. L'évaluation expérimentale montre également que le cadre interactif de PaF permet de réduire l'effort d'annotation de l'utilisateur, tout en préservant la qualité des extracteurs induits.",
    "uri": "https://theses.hal.science/tel-00613195v1",
    "texte_nettoye": "La thèse défendue dans ce mémoire est qu'il est possible de concevoir des algorithmes d'apprentissage de programmes d'extraction n-aire pour les documents semi-structurés, qui est une classe non triviale de transformation d'arbres, de manière supervisée et avec peu d'intervention de l'utilisateur. Les documents semi-structurés ont une structure arborescente. Hors peu de systèmes d'induction supervisée d'extracteurs en tirent partie. La plupart d'entre eux considèrent les documents comme une séquence mélangeant balises et contenu [51, 42, 40, 78, 65]. Plus récemment sont apparus des algorithmes d'induction exploitant pleinement la structure d'arbre des documents semi-structurés [43, 48, 81, 12, 39, 56, 36]. Cette thèse s'inscrit dans ce courant et soutient l'idée que l'exploitation de la structure des documents semi-structurés permet d'induire des extracteurs expressifs et performants. L'induction est réalisée à l'aide d'algorithmes d'apprentissage automatique de classification supervisée. Ce choix est motivé à la fois par le succès des approches d'extractions fondée sur la classification, mais surtout par la volonté d'utiliser des algorithmes d'apprentissage existants et connus. Bien que le codage de exemples d'apprentissage en attribut-valeur prenne en compte la nature arborescente des documents semi-structurés, il est générique et intègre peu de connaissance de base. Cependant toute nouvelle connaissance est facilement intégrable. Notre représentation des données est adaptative. Dans notre approche, l'extraction n-aire est réalisée de manière incrémentale au cours d'une boucle croissante sur la taille des n-uplets. Ce procédé d'extraction ne fait aucune hypothèse sur la disposition des données dans les documents. Aucun post-traitement n'est effectué : notre algorithme réalise en même temps l'extraction des composantes et leur combinaison en n-uplets. Précisons qu'un extracteur obtenu par PaF, notre système, est utilisable tel quel, comme une boite noire, avec en entrée des documents HTML ou XML, et en sortie l'ensemble des n-uplets extraits. De plus le système PaF est implémenté dans un cadre interactif qui permet l'induction à partir d'un faible nombre d'interactions. L'utilisateur fournit quelques annotations qui servent d'amorce à l'apprentissage d'un extracteur hypothèse. Ici commence une boucle d'interaction dans laquelle l'utilisateur corrige les erreurs de l'hypothèse courante et relance l'apprentissage jusqu'à l'obtention d'une hypothèse correcte. PaF permet d'apprendre des extracteurs n-aires performants à partir de peu d'exemples. Les résultats expérimentaux montrent que PaF atteint les performances des meilleurs systèmes n-aires. De plus son procédé d'extraction reste applicable et efficace même lorsque l'organisation des données dans les documents semi-structurés est complexe. L'évaluation expérimentale montre également que le cadre interactif de PaF permet de réduire l'effort d'annotation de l'utilisateur, tout en préservant la qualité des extracteurs induits."
  },
  {
    "title": "Apprentissage de modèles en télémédecine",
    "authors": [
      "Laurent Jeanpierre",
      "François Charpillet"
    ],
    "year": 2002,
    "abstract": "L'application de modèles de diagnostic à des problèmes médicaux pose le problème de l'adaptation des paramètres de ces modèles dans des conditions difficiles. Les données médicales disponibles ne sont en effet pas toujours suffisantes, et la condition d'un patient est amenée à évoluer au cours du temps. L'apprentissage par descente de gradient visant à imiter le diagnostic d'un médecin permet de s'abstraire de ces problèmes en guidant le système vers une solution viable en un nombre restreint d'itérations.",
    "uri": "https://inria.hal.science/inria-00100813v1",
    "texte_nettoye": "L'application de modèles de diagnostic à des problèmes médicaux pose le problème de l'adaptation des paramètres de ces modèles dans des conditions difficiles. Les données médicales disponibles ne sont en effet pas toujours suffisantes, et la condition d'un patient est amenée à évoluer au cours du temps. L'apprentissage par descente de gradient visant à imiter le diagnostic d'un médecin permet de s'abstraire de ces problèmes en guidant le système vers une solution viable en un nombre restreint d'itérations."
  },
  {
    "title": "Métriques d'équité en Apprentissage Automatique et droit de l'Union Europénne en matière de non-discrimination",
    "authors": [
      "Magali Legast",
      "Yasaman Yousefi",
      "Lisa Koutsoviti",
      "Axel Legay",
      "Christopher Schommer",
      "Koen Vanhoof"
    ],
    "year": 2023,
    "abstract": "Les modèles d'apprentissage automatique (AA) peuvent présenter des biais discriminatoires envers certains groupes sociaux. Nous étudions à quel point les techniques et définitions d'équité utilisées en AA peuvent garantir le respect du droit de l'UE en matière de non discrimination. À travers des modèles de classification entraînés avec différentes contraintes d'équité, nous évaluons l'efficacité des méthodes de correction de biais et discutons les résultats sous l'angle de l'AA et de l'informatique juridique.",
    "uri": "https://hal.science/hal-04164322v1",
    "texte_nettoye": "Les modèles d'apprentissage automatique (AA) peuvent présenter des biais discriminatoires envers certains groupes sociaux. Nous étudions à quel point les techniques et définitions d'équité utilisées en AA peuvent garantir le respect du droit de l'UE en matière de non discrimination. À travers des modèles de classification entraînés avec différentes contraintes d'équité, nous évaluons l'efficacité des méthodes de correction de biais et discutons les résultats sous l'angle de l'AA et de l'informatique juridique."
  },
  {
    "title": "Les méthodes formelles sont-elles applicables à l'apprentissage automatique et à l'intelligence artificielle",
    "authors": [
      "Moez Krichen"
    ],
    "year": 2022,
    "abstract": "Les approches formelles peuvent fournir des garanties d'exactitude strictes pour le développement de systèmes matériels et logiciels. Dans ce travail, nous examinons les méthodes formelles de pointe pour la vérification et la validation des systèmes d'apprentissage automatique en particulier. Nous fournissons d'abord un bref résumé des approches formelles existantes en général. Ensuite, nous rendons compte des méthodes formelles développées pour valider les phases de préparation et d'entraînement des données. Ensuite, nous passons en revue les méthodes formelles utilisées pour la vérification des systèmes d'apprentissage automatique. A ce niveau, nous considérons à la fois des techniques partielles et exhaustives. De plus, nous passons en revue les travaux de recherche dédiés à la vérification des machines à vecteurs de support et des ensembles d'arbres de décision. Enfin, nous proposons plusieurs orientations futures potentielles pour la vérification formelle des systèmes d'apprentissage automatique.",
    "uri": "https://hal.science/hal-03751705v1",
    "texte_nettoye": "Les approches formelles peuvent fournir des garanties d'exactitude strictes pour le développement de systèmes matériels et logiciels. Dans ce travail, nous examinons les méthodes formelles de pointe pour la vérification et la validation des systèmes d'apprentissage automatique en particulier. Nous fournissons d'abord un bref résumé des approches formelles existantes en général. Ensuite, nous rendons compte des méthodes formelles développées pour valider les phases de préparation et d'entraînement des données. Ensuite, nous passons en revue les méthodes formelles utilisées pour la vérification des systèmes d'apprentissage automatique. A ce niveau, nous considérons à la fois des techniques partielles et exhaustives. De plus, nous passons en revue les travaux de recherche dédiés à la vérification des machines à vecteurs de support et des ensembles d'arbres de décision. Enfin, nous proposons plusieurs orientations futures potentielles pour la vérification formelle des systèmes d'apprentissage automatique."
  },
  {
    "title": "Communication et apprentissage par renforcement pour une équipe d'agents",
    "authors": [
      "Daniel Szer",
      "François Charpillet"
    ],
    "year": 2004,
    "abstract": "Nous présentons un nouvel algorithme d'apprentissage par renforcement pour des systèmes multi-agents coopératifs. Le problème de contrôle est formalisé comme un processus de décision markovien que nous cherchons à résoudre de manière décentralisée. Pour cela, nous proposons une variante du Q-learning avec communication, à savoir un mécanisme de notification réciproque. Nous allons introduire le problème de coopération multi-agents et poser un critère d'optimalité pour la solution souhaitée. Nous allons ensuite présenter l'algorithme de notification réciproque, prouver sa convergence et étudier des variantes de l'algorithme qui permettent des stratégies de communication plus flexibles. Nous conclurons avec les performances de l'algorithme sur un exemple d'apprentissage précis.",
    "uri": "https://inria.hal.science/inria-00100256v1",
    "texte_nettoye": "Nous présentons un nouvel algorithme d'apprentissage par renforcement pour des systèmes multi-agents coopératifs. Le problème de contrôle est formalisé comme un processus de décision markovien que nous cherchons à résoudre de manière décentralisée. Pour cela, nous proposons une variante du Q-learning avec communication, à savoir un mécanisme de notification réciproque. Nous allons introduire le problème de coopération multi-agents et poser un critère d'optimalité pour la solution souhaitée. Nous allons ensuite présenter l'algorithme de notification réciproque, prouver sa convergence et étudier des variantes de l'algorithme qui permettent des stratégies de communication plus flexibles. Nous conclurons avec les performances de l'algorithme sur un exemple d'apprentissage précis."
  },
  {
    "title": "Échantillonnage progressif guidé pour stabiliser la courbe d'apprentissage",
    "authors": [
      "François Portet",
      "René Quiniou"
    ],
    "year": 2008,
    "abstract": "L'un des enjeux de l'apprentissage artificiel est de pouvoir fonctionner avec des volumes de données toujours plus grands. Bien qu'il soit généralement admis que plus un ensemble d'apprentissage est large et plus les résultats sont performants, il existe des limites à la masse d'informations qu'un algorithme d'apprentissage peut manipuler. Pour résoudre ce problème, nous proposons d'améliorer la méthode d'échantillonnage progressif en guidant la construction d'un ensemble d'apprentissage réduit à partir d'un large ensemble de données. L'apprentissage à partir de l'ensemble réduit doit conduire à des performances similaires à l'apprentissage effectué avec l'ensemble complet. Le guidage de l'échantillonnage s'appuie sur une connaissance a priori qui accélère la convergence de l'algorithme. Cette approche présente trois avantages : 1) l'ensemble d'apprentissage réduit est composé des cas les plus représentatifs de l'ensemble complet; 2) la courbe d'apprentissage est stabilisée; 3) la détection de convergence est accélérée. L'application de cette méthode à des données classiques et à des données provenant d'unités de soins intensifs révèle qu'il est possible de réduire de façon significative un ensemble d'apprentissage sans diminuer la performance de l'apprentissage.",
    "uri": "https://inria.hal.science/inria-00266536v1",
    "texte_nettoye": "L'un des enjeux de l'apprentissage artificiel est de pouvoir fonctionner avec des volumes de données toujours plus grands. Bien qu'il soit généralement admis que plus un ensemble d'apprentissage est large et plus les résultats sont performants, il existe des limites à la masse d'informations qu'un algorithme d'apprentissage peut manipuler. Pour résoudre ce problème, nous proposons d'améliorer la méthode d'échantillonnage progressif en guidant la construction d'un ensemble d'apprentissage réduit à partir d'un large ensemble de données. L'apprentissage à partir de l'ensemble réduit doit conduire à des performances similaires à l'apprentissage effectué avec l'ensemble complet. Le guidage de l'échantillonnage s'appuie sur une connaissance a priori qui accélère la convergence de l'algorithme. Cette approche présente trois avantages : 1) l'ensemble d'apprentissage réduit est composé des cas les plus représentatifs de l'ensemble complet; 2) la courbe d'apprentissage est stabilisée; 3) la détection de convergence est accélérée. L'application de cette méthode à des données classiques et à des données provenant d'unités de soins intensifs révèle qu'il est possible de réduire de façon significative un ensemble d'apprentissage sans diminuer la performance de l'apprentissage."
  },
  {
    "title": "Apprentissage par renforcement dans un système multi-agents",
    "authors": [
      "Olivier Buffet"
    ],
    "year": 2000,
    "abstract": "Longtemps l'Intelligence Artificielle s'est attachée à faire effectuer par un unique agent des tâches plus ou moins complexes. Les méthodes exactes, cherchant par des méthodes systématiques à résoudre un problème, sont hélas souvent inexploitables quand des contraintes de taille mémoire ou de temps-réel entrent en jeu. On se propose alors de trouver plutôt des solutions optimales ou sous-optimales, par des méthodes approchées de planification ou d'apprentissage qui permettent des convergences beaucoup plus rapides (les processus décisionnels de Markov dont il sera sujet en font partie). Mais les capacités d'un unique agent peuvent rester insuffisantes, au moins au niveau de ses moyens matériels. l'idée est que certains problèmes sont résolus non par des individus séparément, mais par des groupes d'individus. Même de simples insectes, du fait de leur organisation, peuvent ainsi effectuer des prouesses. Ce stage de DEA vise à faire une étude des travaux existants dans le domaine de l'apprentissage au sein de systèmes mono- et multi-agents. Le cadre multi-agents pose des problèmes nouveaux aux méthodes d'apprentissage par renforcement connues, les agents devant coordonner leurs actions. Le stage a été en outre l'occasion de mettre en oeuvre des algorithmes classiques sur quelques exemples simples.",
    "uri": "https://inria.hal.science/inria-00099176v1",
    "texte_nettoye": "Longtemps l'Intelligence Artificielle s'est attachée à faire effectuer par un unique agent des tâches plus ou moins complexes. Les méthodes exactes, cherchant par des méthodes systématiques à résoudre un problème, sont hélas souvent inexploitables quand des contraintes de taille mémoire ou de temps-réel entrent en jeu. On se propose alors de trouver plutôt des solutions optimales ou sous-optimales, par des méthodes approchées de planification ou d'apprentissage qui permettent des convergences beaucoup plus rapides (les processus décisionnels de Markov dont il sera sujet en font partie). Mais les capacités d'un unique agent peuvent rester insuffisantes, au moins au niveau de ses moyens matériels. l'idée est que certains problèmes sont résolus non par des individus séparément, mais par des groupes d'individus. Même de simples insectes, du fait de leur organisation, peuvent ainsi effectuer des prouesses. Ce stage de DEA vise à faire une étude des travaux existants dans le domaine de l'apprentissage au sein de systèmes mono- et multi-agents. Le cadre multi-agents pose des problèmes nouveaux aux méthodes d'apprentissage par renforcement connues, les agents devant coordonner leurs actions. Le stage a été en outre l'occasion de mettre en oeuvre des algorithmes classiques sur quelques exemples simples."
  },
  {
    "title": "Rapport d’avancement AMCER n°14 : Intelligence Artificielle et Apprentissage Automatique appliqués à l'exploitation, la maintenance et la sécurité ferroviaire : Apports et limites",
    "authors": [
      "Habib Hadj-Mabrouk"
    ],
    "year": 2025,
    "abstract": "Cette étude offre un panorama non exhaustif des méthodes, algorithmes et applications de l'IA, notamment l'apprentissage automatique (ML) dans le transport ferroviaire. S'inspirant des deux directives européennes relatives au développement (Directive 2012/34/UE) et à l'interopérabilité (Directive (UE) 2016/797) du système ferroviaire, cette revue propose de classer les applications de l'IA selon 1) les « éléments structurels » du système ferroviaire (infrastructure, matériel roulant, énergie, contrôle-commande et signalisation), 2) les « éléments fonctionnels » (exploitation, maintenance et applications télématiques) et 3) les éléments à la fois « structurels et fonctionnels », afin d'identifier les approches de l'IA visant à améliorer la sécurité ferroviaire, notamment l'analyse des accidents et incidents ferroviaires à partir des rapports d'enquête. Plusieurs techniques d'IA « classiques » sont mises en oeuvre, notamment le ML (supervisé, semi-supervisé, non supervisé), l'apprentissage profond comme les réseaux de neurones artificiels (RNA), le traitement du langage naturel (NLP), le raisonnement à base de cas (CBR), etc. Cependant, le manque d'interopérabilité entre les outils d'IA « classiques » et l'inadéquation de ces approches à capitaliser, partager et réutiliser ces connaissances ont poussé les recherches vers le développement de nouvelles approches basées sur des ontologies et des graphes de connaissances. L'étude de l'ensemble des applications de l'IA montre que les étapes d'acquisition, d'analyse, de structuration, de formalisation, de modélisation, de traitement et d'interprétation des données produites par un système d’apprentissage posent un problème crucial dans le secteur du transport ferroviaire. De plus, avec des modèles complexes appelés « boîtes noires », il est difficile de comprendre comment et pourquoi les mécanismes de raisonnement internes du système d'IA influencent la solution et les prédictions. La nouvelle approche de l'IA explicable (XAI) ou de l'apprentissage automatique explicable (XML) peut apporter une réponse à ce problème, notamment lorsqu'il s'agit d'un enjeu crucial comme celui de la sécurité ferroviaire.",
    "uri": "https://hal.science/hal-05241309v1",
    "texte_nettoye": "Cette étude offre un panorama non exhaustif des méthodes, algorithmes et applications de l'IA, notamment l'apprentissage automatique (ML) dans le transport ferroviaire. S'inspirant des deux directives européennes relatives au développement (Directive 2012/34/UE) et à l'interopérabilité (Directive (UE) 2016/797) du système ferroviaire, cette revue propose de classer les applications de l'IA selon 1) les « éléments structurels » du système ferroviaire (infrastructure, matériel roulant, énergie, contrôle-commande et signalisation), 2) les « éléments fonctionnels » (exploitation, maintenance et applications télématiques) et 3) les éléments à la fois « structurels et fonctionnels », afin d'identifier les approches de l'IA visant à améliorer la sécurité ferroviaire, notamment l'analyse des accidents et incidents ferroviaires à partir des rapports d'enquête. Plusieurs techniques d'IA « classiques » sont mises en oeuvre, notamment le ML (supervisé, semi-supervisé, non supervisé), l'apprentissage profond comme les réseaux de neurones artificiels (RNA), le traitement du langage naturel (NLP), le raisonnement à base de cas (CBR), etc. Cependant, le manque d'interopérabilité entre les outils d'IA « classiques » et l'inadéquation de ces approches à capitaliser, partager et réutiliser ces connaissances ont poussé les recherches vers le développement de nouvelles approches basées sur des ontologies et des graphes de connaissances. L'étude de l'ensemble des applications de l'IA montre que les étapes d'acquisition, d'analyse, de structuration, de formalisation, de modélisation, de traitement et d'interprétation des données produites par un système d’apprentissage posent un problème crucial dans le secteur du transport ferroviaire. De plus, avec des modèles complexes appelés « boîtes noires », il est difficile de comprendre comment et pourquoi les mécanismes de raisonnement internes du système d'IA influencent la solution et les prédictions. La nouvelle approche de l'IA explicable (XAI) ou de l'apprentissage automatique explicable (XML) peut apporter une réponse à ce problème, notamment lorsqu'il s'agit d'un enjeu crucial comme celui de la sécurité ferroviaire."
  },
  {
    "title": "Extraction et gestion d'informations pour la construction d'une base vidéo d'apprentissage",
    "authors": [
      "Alain Simac-Lejeune"
    ],
    "year": 2012,
    "abstract": "Indexer une vidéo consiste à rattacher un ou plusieurs concepts à des segments de cette vidéo, un concept étant défini comme une représentation intellectuelle d'une idée abstraite. L'indexation automatique se base sur l'extraction automatique de caractéristiques fournies par un système de traitement d'images. Cependant, il est nécessaire de définir les index ou concepts. Pour cela il faut définir le lien qui existe entre ces caractéristiques et ces concepts. Ce qui sépare les caractéristiques extraites sur lesquelles se base l'indexation automatique et les concepts est appelé fossé sémantique qui est le manque de concordance entre les informations que les machines peuvent extraire depuis les documents numériques et les interprétations que les humaines en font. La définition d'un concept peut être faite automatiquement si l'on dispose d'une base d'apprentissage liée au concept. Dans ce cas, il est possible \"d'apprendre\" le concept de manière statistique. Mais la construction de cette base d'apprentissage nécessite de faire intervenir un utilisateur ou un expert applicatif. En fait, il s'agit de s'appuyer sur ses connaissances pour extraire des segments vidéo représentatifs du concept que l'on souhaite définir. On peut lui demander d'indexer manuellement la base d'apprentissage, mais cette opération est longue et fastidieuse. Dans cet article, nous proposons une méthode qui permet d'extraire l'expertise pour que l'implication de l'expert soit la plus simple et la plus limitée possible.",
    "uri": "https://hal.science/hal-00674616v1",
    "texte_nettoye": "Indexer une vidéo consiste à rattacher un ou plusieurs concepts à des segments de cette vidéo, un concept étant défini comme une représentation intellectuelle d'une idée abstraite. L'indexation automatique se base sur l'extraction automatique de caractéristiques fournies par un système de traitement d'images. Cependant, il est nécessaire de définir les index ou concepts. Pour cela il faut définir le lien qui existe entre ces caractéristiques et ces concepts. Ce qui sépare les caractéristiques extraites sur lesquelles se base l'indexation automatique et les concepts est appelé fossé sémantique qui est le manque de concordance entre les informations que les machines peuvent extraire depuis les documents numériques et les interprétations que les humaines en font. La définition d'un concept peut être faite automatiquement si l'on dispose d'une base d'apprentissage liée au concept. Dans ce cas, il est possible \"d'apprendre\" le concept de manière statistique. Mais la construction de cette base d'apprentissage nécessite de faire intervenir un utilisateur ou un expert applicatif. En fait, il s'agit de s'appuyer sur ses connaissances pour extraire des segments vidéo représentatifs du concept que l'on souhaite définir. On peut lui demander d'indexer manuellement la base d'apprentissage, mais cette opération est longue et fastidieuse. Dans cet article, nous proposons une méthode qui permet d'extraire l'expertise pour que l'implication de l'expert soit la plus simple et la plus limitée possible."
  },
  {
    "title": "Contribution au développement de l’apprentissage profond dans les systèmes distribués",
    "authors": [
      "Corentin Hardy"
    ],
    "year": 2019,
    "abstract": "L'apprentissage profond permet de développer un nombre de services de plus en plus important. Il nécessite cependant de grandes bases de données d'apprentissage et beaucoup de puissance de calcul. Afin de réduire les coûts de cet apprentissage profond, nous proposons la mise en œuvre d'un apprentissage collaboratif. Les futures utilisateurs des services permis par l'apprentissage profond peuvent ainsi participer à celui-ci en mettant à disposition leurs machines ainsi que leurs données sans déplacer ces dernières sur le cloud. Nous proposons différentes méthodes afin d'apprendre des réseaux de neurones profonds dans ce contexte de système distribué.",
    "uri": "https://theses.hal.science/tel-02284916v1",
    "texte_nettoye": "L'apprentissage profond permet de développer un nombre de services de plus en plus important. Il nécessite cependant de grandes bases de données d'apprentissage et beaucoup de puissance de calcul. Afin de réduire les coûts de cet apprentissage profond, nous proposons la mise en œuvre d'un apprentissage collaboratif. Les futures utilisateurs des services permis par l'apprentissage profond peuvent ainsi participer à celui-ci en mettant à disposition leurs machines ainsi que leurs données sans déplacer ces dernières sur le cloud. Nous proposons différentes méthodes afin d'apprendre des réseaux de neurones profonds dans ce contexte de système distribué."
  },
  {
    "title": "Apprentissage actif de modèle de MDP",
    "authors": [
      "Mauricio Araya-López",
      "Olivier Buffet",
      "Vincent Thomas",
      "François Charpillet"
    ],
    "year": 2011,
    "abstract": "Dans cet article, nous nous intéressons à un problème d'apprentissage actif consistant à déduire le modèle de transition d'un Processus de Décision Markovien (MDP) en agissant et en observant les transitions résultantes. Ceci est particulièrement utile lorsque la fonction de récompense n'est pas initialement accessible. Notre proposition consiste à formuler ce problème d'apprentissage actif en un problème de maximisation d'utilité dans le cadre de l'apprentissage par renforcement bayésien avec des récompenses dépendant de l'état de croyance. Après avoir présenté trois critères de performance possibles, nous en dérivons des récompenses dépendant de l'état de croyance que l'on pourra utiliser dans le processus de prise de décision. Comme le calcul de la fonction de valeur bayésienne optimale n'est pas envisageable pour de larges horizons, nous utilisons un algorithme simple pour résoudre de manière approchée ce problème d'optimisation. Malgré le fait que la solution est sous- optimale, nous montrons expérimentalement que notre proposition est néanmoins efficace dans un certain nombre de domaines.",
    "uri": "https://inria.hal.science/hal-00642913v1",
    "texte_nettoye": "Dans cet article, nous nous intéressons à un problème d'apprentissage actif consistant à déduire le modèle de transition d'un Processus de Décision Markovien (MDP) en agissant et en observant les transitions résultantes. Ceci est particulièrement utile lorsque la fonction de récompense n'est pas initialement accessible. Notre proposition consiste à formuler ce problème d'apprentissage actif en un problème de maximisation d'utilité dans le cadre de l'apprentissage par renforcement bayésien avec des récompenses dépendant de l'état de croyance. Après avoir présenté trois critères de performance possibles, nous en dérivons des récompenses dépendant de l'état de croyance que l'on pourra utiliser dans le processus de prise de décision. Comme le calcul de la fonction de valeur bayésienne optimale n'est pas envisageable pour de larges horizons, nous utilisons un algorithme simple pour résoudre de manière approchée ce problème d'optimisation. Malgré le fait que la solution est sous- optimale, nous montrons expérimentalement que notre proposition est néanmoins efficace dans un certain nombre de domaines."
  },
  {
    "title": "Détection automatique d'erreurs d'annotations pour améliorer les performances des algorithmes d'apprentissage automatique",
    "authors": [
      "Carole Lemort"
    ],
    "year": 2011,
    "abstract": "Actuellement, les techniques de transcriptions automatiques de parole et d'annotation de ces transcriptions restent très dépendantes de l'environnement dans lequel ont lieu les enregistrements. Cela pose un problème pour le résumé automatique, la traduction, ainsi que la reconnaissance d'entités nommées. Aussi, le but de ce document est d'améliorer la détection des annotations erronées afin qu'il soit possible par la suite les corriger et d'obtenir des annotations de meilleure qualité. Dans un premier temps, nous présentons un état de l'art, puis le protocole expérimental. Dans un second temps, nous commentons les résultats.",
    "uri": "https://dumas.ccsd.cnrs.fr/dumas-00636454v1",
    "texte_nettoye": "Actuellement, les techniques de transcriptions automatiques de parole et d'annotation de ces transcriptions restent très dépendantes de l'environnement dans lequel ont lieu les enregistrements. Cela pose un problème pour le résumé automatique, la traduction, ainsi que la reconnaissance d'entités nommées. Aussi, le but de ce document est d'améliorer la détection des annotations erronées afin qu'il soit possible par la suite les corriger et d'obtenir des annotations de meilleure qualité. Dans un premier temps, nous présentons un état de l'art, puis le protocole expérimental. Dans un second temps, nous commentons les résultats."
  },
  {
    "title": "Évaluation des architectures d'apprentissage automatique pour la détection et le diagnostic des anomalies dans les procédés chimiques",
    "authors": [
      "Rayane Ammar Khodja",
      "Alexandre Voisin",
      "Victor Costa",
      "Fanny Casteran",
      "Benoit Celse",
      "Benoît Iung"
    ],
    "year": 2025,
    "abstract": "La détection et le diagnostic des anomalies (FDD : Fault Detection and Diagnosis) est un sujet crucial tant pour le pilotage que la maintenance afin d’anticiper les défaillances coûteuses et optimiser les opérations notamment dans l’industrie chimique comme par exemple dans le cas d’unités pilotes, comme celles d'IFP Energies Nouvelles (IFPEN), qui fonctionnent sur des temps courts avec des paramètres opérationnels variant fréquemment. Bien que diverses approches d'apprentissage automatique aient été proposées pour le FDD dans la littérature [1] [2], il subsiste un manque dans l'évaluation plus globale dans ce type d’application. Cette étude consiste en une analyse comparative de plusieurs approches basées sur l’apprentissage automatique pour la FDD. Pour cela nous utilisons le Tennessee Eastman Process (TEP), un jeu de données de simulation de procédé chimique, qui est un benchmark dans ce domaine et qui comporte plusieurs type dysfonctionnements ainsi que plusieurs « productions » rendant compte de la variabilité du processus dans des conditions opérationnelles identiques. Dans cette étude, plusieurs méthodes ont été implémentées et comparées, notamment l'ACP multi-échelle (Multi Scale PCA), l'AutoEncoder, l'Apprentissage d'Ensemble (Ensemble Learning), et les modèles LSTM (Long Short Term Memory) pour la détection des défauts, ainsi que Random Forest, XGBoost et BLSTM (Bidirectional LSTM) pour leur diagnostic.",
    "uri": "https://hal.science/hal-05471853v1",
    "texte_nettoye": "La détection et le diagnostic des anomalies (FDD : Fault Detection and Diagnosis) est un sujet crucial tant pour le pilotage que la maintenance afin d’anticiper les défaillances coûteuses et optimiser les opérations notamment dans l’industrie chimique comme par exemple dans le cas d’unités pilotes, comme celles d'IFP Energies Nouvelles (IFPEN), qui fonctionnent sur des temps courts avec des paramètres opérationnels variant fréquemment. Bien que diverses approches d'apprentissage automatique aient été proposées pour le FDD dans la littérature [1] [2], il subsiste un manque dans l'évaluation plus globale dans ce type d’application. Cette étude consiste en une analyse comparative de plusieurs approches basées sur l’apprentissage automatique pour la FDD. Pour cela nous utilisons le Tennessee Eastman Process (TEP), un jeu de données de simulation de procédé chimique, qui est un benchmark dans ce domaine et qui comporte plusieurs type dysfonctionnements ainsi que plusieurs « productions » rendant compte de la variabilité du processus dans des conditions opérationnelles identiques. Dans cette étude, plusieurs méthodes ont été implémentées et comparées, notamment l'ACP multi-échelle (Multi Scale PCA), l'AutoEncoder, l'Apprentissage d'Ensemble (Ensemble Learning), et les modèles LSTM (Long Short Term Memory) pour la détection des défauts, ainsi que Random Forest, XGBoost et BLSTM (Bidirectional LSTM) pour leur diagnostic."
  },
  {
    "title": "Apprentissage et planification",
    "authors": [
      "F. Garcia"
    ],
    "year": 1997,
    "abstract": "Les applications de plus en plus nombreuses de la planification en Intelligence Artificielle conduit à un développement des besoins en apprentissage automatique. Nous présentons ici un premier état de l'art des différentes utilisations de l'apprentissage en planification, selon les trois grand thèmes actuels de recherche : apprentissage d'heuristique de contrôle, apprentissage des domaines et apprentissage direct des stratégies.",
    "uri": "https://hal.inrae.fr/hal-02768682v1",
    "texte_nettoye": "Les applications de plus en plus nombreuses de la planification en Intelligence Artificielle conduit à un développement des besoins en apprentissage automatique. Nous présentons ici un premier état de l'art des différentes utilisations de l'apprentissage en planification, selon les trois grand thèmes actuels de recherche : apprentissage d'heuristique de contrôle, apprentissage des domaines et apprentissage direct des stratégies."
  },
  {
    "title": "Heuristique pour l'apprentissage automatique décentralisé d'interactions dans des systèmes multi-agents réactifs",
    "authors": [
      "Vincent Thomas",
      "Vincent Chevrier",
      "Christine Bourjot"
    ],
    "year": 2006,
    "abstract": "Cet article propose une heuristique pour la construction automatique d'interactions dans un système multi-agent réactif. Il décrit le formalisme interac-DEC-POMDP qui permet de représenter dans un cadre homogène interactions et actions puis développe un algorithme fondé sur des échanges de récompenses et des techniques d'apprentissage par renforcement pour construire automatiquement et de manière entièrement décentralisée des organisations dans une sous-classe des interac-DEC-POMDPs. Ces techniques permettent en outre de produire à moindre coûts des comportements collectifs adaptatifs basés sur la notion d'interaction directe.",
    "uri": "https://inria.hal.science/inria-00104872v1",
    "texte_nettoye": "Cet article propose une heuristique pour la construction automatique d'interactions dans un système multi-agent réactif. Il décrit le formalisme interac-DEC-POMDP qui permet de représenter dans un cadre homogène interactions et actions puis développe un algorithme fondé sur des échanges de récompenses et des techniques d'apprentissage par renforcement pour construire automatiquement et de manière entièrement décentralisée des organisations dans une sous-classe des interac-DEC-POMDPs. Ces techniques permettent en outre de produire à moindre coûts des comportements collectifs adaptatifs basés sur la notion d'interaction directe."
  },
  {
    "title": "Quelques lois d'adaptation pour neurones artificiels et réels",
    "authors": [
      "Frédéric Alexandre"
    ],
    "year": 2000,
    "abstract": "Le but de ce texte est de proposer un point de vue de l'adaptation, sous l'angle du connexionnisme. Cet angle sera pris volontairement large, dans la mesure où il me semble important de parler, bien sûr, du caractère adaptatif des réseaux de neurones artificiels, mais au moins aussi important de replacer ces propriétés dans le cadre plus général de l'apprentissage animal et humain. Je commencerai par une présentation générale du connexionnisme, puis des principales lois d'apprentissage permettant l'adaptation des modèles de réseaux de neurones artificiels. Ceci pourra être replacé d'une part, dans le cadre des statistiques et d'autre part, dans le cadre de l'apprentissage animal et humain. Sur ce dernier point, je ferai une présentation rapide des théories actuellement admises et de leur possible rencontre avec des modèles de réseaux de neurones artificiels.",
    "uri": "https://inria.hal.science/inria-00099049v1",
    "texte_nettoye": "Le but de ce texte est de proposer un point de vue de l'adaptation, sous l'angle du connexionnisme. Cet angle sera pris volontairement large, dans la mesure où il me semble important de parler, bien sûr, du caractère adaptatif des réseaux de neurones artificiels, mais au moins aussi important de replacer ces propriétés dans le cadre plus général de l'apprentissage animal et humain. Je commencerai par une présentation générale du connexionnisme, puis des principales lois d'apprentissage permettant l'adaptation des modèles de réseaux de neurones artificiels. Ceci pourra être replacé d'une part, dans le cadre des statistiques et d'autre part, dans le cadre de l'apprentissage animal et humain. Sur ce dernier point, je ferai une présentation rapide des théories actuellement admises et de leur possible rencontre avec des modèles de réseaux de neurones artificiels."
  },
  {
    "title": "Apprentissage de bonnes similarités pour la classification linéaire parcimonieuse",
    "authors": [
      "Aurélien Bellet",
      "Amaury Habrard",
      "Marc Sebban"
    ],
    "year": 2012,
    "abstract": "Le rôle crucial joué par les métriques au sein des processus d'apprentissage automatique a donné lieu ces dernières années à un intérêt croissant pour l'optimisation de fonctions de distances ou de similarités. La plupart des approches de l'état de l'art visent à apprendre une distance de Mahalanobis, devant satisfaire la contrainte de semi-définie positivité (SDP), exploitée in fine dans un algorithme local de type plus-proches-voisins. Cependant, aucun résultat théorique n'établit le lien entre les métriques apprises et leur comportement en classification. Dans cet article, nous exploitons le cadre formel des bonnes similarités pour proposer un algorithme d'apprentissage de similarité linéaire, optimisée dans un espace kernélisé. Nous montrons que la similarité apprise, ne requérant pas d'être SDP, possède des propriétés théoriques de stabilité permettant d'établir une borne en généralisation. Les expérimentations menées sur plusieurs jeux de données confirment son efficacité par rapport à l'état de l'art.",
    "uri": "https://hal.science/hal-00690242v1",
    "texte_nettoye": "Le rôle crucial joué par les métriques au sein des processus d'apprentissage automatique a donné lieu ces dernières années à un intérêt croissant pour l'optimisation de fonctions de distances ou de similarités. La plupart des approches de l'état de l'art visent à apprendre une distance de Mahalanobis, devant satisfaire la contrainte de semi-définie positivité (SDP), exploitée in fine dans un algorithme local de type plus-proches-voisins. Cependant, aucun résultat théorique n'établit le lien entre les métriques apprises et leur comportement en classification. Dans cet article, nous exploitons le cadre formel des bonnes similarités pour proposer un algorithme d'apprentissage de similarité linéaire, optimisée dans un espace kernélisé. Nous montrons que la similarité apprise, ne requérant pas d'être SDP, possède des propriétés théoriques de stabilité permettant d'établir une borne en généralisation. Les expérimentations menées sur plusieurs jeux de données confirment son efficacité par rapport à l'état de l'art."
  },
  {
    "title": "Interaction texte/figure : effet de leur disposition spatiale relative sur l'apprentissage",
    "authors": [
      "Mireille Betrancourt"
    ],
    "year": 1992,
    "abstract": "A la suite de travaux de psychologie cognitive et tout particulierement des recents resultats de Sweller, cette recherche vise a preciser l'effet sur l'apprentissage de la modalite de presentation de textes et figures en relation mutuelle, au point de vue de leur disposition spatiale relative dans la page. Plus precisement, l'objectif de cette etude est de tester l'influence de l'integration du commentaire textuel au sein meme de la figure. Ainsi, nous avons mene une experience pour comparer l'efficacite de trois modalites de presentation du materiel d'apprentissage sur ecran d'ordinateur : presentation \"conventionnelle\" (texte et figure separes), presentation \"integree\" (texte directement place sur l'element graphique concerne), presentation \"escamot\" (textes integres en \"pop-up window\" n'apparaissant que sous demarche active du sujet). Les resultats montrent un avantage en faveur des presentations \"integree\" et surtout \"escamot\" en temps d'apprentissage et en performances, ce qui permet de garder l'hypothese d'une part d'une facilitation de l'apprentissage d'un materiel ou textes et figures sont integres, d'autre part d'une facilitation accrue lorsque les textes n'apparaissent que sous demarche active du sujet.",
    "uri": "https://inria.hal.science/inria-00077021v1",
    "texte_nettoye": "A la suite de travaux de psychologie cognitive et tout particulierement des recents resultats de Sweller, cette recherche vise a preciser l'effet sur l'apprentissage de la modalite de presentation de textes et figures en relation mutuelle, au point de vue de leur disposition spatiale relative dans la page. Plus precisement, l'objectif de cette etude est de tester l'influence de l'integration du commentaire textuel au sein meme de la figure. Ainsi, nous avons mene une experience pour comparer l'efficacite de trois modalites de presentation du materiel d'apprentissage sur ecran d'ordinateur : presentation \"conventionnelle\" (texte et figure separes), presentation \"integree\" (texte directement place sur l'element graphique concerne), presentation \"escamot\" (textes integres en \"pop-up window\" n'apparaissant que sous demarche active du sujet). Les resultats montrent un avantage en faveur des presentations \"integree\" et surtout \"escamot\" en temps d'apprentissage et en performances, ce qui permet de garder l'hypothese d'une part d'une facilitation de l'apprentissage d'un materiel ou textes et figures sont integres, d'autre part d'une facilitation accrue lorsque les textes n'apparaissent que sous demarche active du sujet."
  },
  {
    "title": "Optimisation des propriétés d’un supercontinuum par apprentissage automatique pour la microscopie multiphotonique",
    "authors": [
      "Van Thuy Hoang",
      "Yassin Boussafa",
      "Lynn Sader",
      "Sébastien Février",
      "Vincent Couderc",
      "Benjamin Wetzel"
    ],
    "year": 2022,
    "abstract": "Nous étudions des approches innovantes d’apprentissage automatique afin d’optimiser les propriétés spectro-temporelles d’un supercontinuum fibré. Nous démontrons numériquement que l’ajustement d’un motif d’impulsions femtosecondes permet de maximiser le profil temporel à des longueurs d’onde pertinentes en imagerie multiphotonique.",
    "uri": "https://hal.science/hal-03811797v1",
    "texte_nettoye": "Nous étudions des approches innovantes d’apprentissage automatique afin d’optimiser les propriétés spectro-temporelles d’un supercontinuum fibré. Nous démontrons numériquement que l’ajustement d’un motif d’impulsions femtosecondes permet de maximiser le profil temporel à des longueurs d’onde pertinentes en imagerie multiphotonique."
  },
  {
    "title": "Définitions et caractérisations de modèles à base d'analogies pour l'apprentissage automatique des langues naturelles",
    "authors": [
      "Nicolas Stroppa"
    ],
    "year": 2005,
    "abstract": "Le panorama du Traitement Automatique des Langues est dominé par deux familles d'approches~: dans la première, la connaissance linguistique s'exprime sous forme de règles (grammaticales pour le traitement syntaxique, d'inférence pour le traitement sémantique, etc.), et de représentations sur lesquelles ces règles opèrent. La deuxième repose sur l'hypothèse d'un modèle probabiliste sous-jacent aux données, modèle dont les paramètres s'infèrent à partir de corpus de données linguistiques annotées. Ces deux familles de méthodes, bien qu'efficaces pour nombre d'applications, présentent de sérieuses limitations. Pour la première, il s'agit de la difficulté et du coût de construction des bases de connaissances de haute qualité~: les experts sont rares et la connaissance accumulée sur un domaine $X$ ne se transporte pas toujours simplement sur un autre domaine $Y$. Les méthodes probabilistes, quant à elles, ne traitent pas naturellement les objets fortement structurés, ne prévoient pas d'inclusion de connaissances linguistiques explicites, et surtout, reposent lourdement sur le choix a priori d'un certain modèle, puisqu'utilisant principalement des techniques de statistiques paramétriques. Dans le cadre d'un apprentissage automatique de données linguistiques, des modèles inférentiels alternatifs ont alors été proposés qui remettent en cause le principe d'abstraction opéré par les règles ou les modèles probabilistes. Selon cette conception, la connaissance linguistique reste implicitement représentée dans le corpus accumulé. Dans le domaine de l'Apprentissage Automatique, les méthodes suivant les même principes sont regroupées sous l'appellation d'apprentissage \\og{}paresseux\\fg{}. Ces méthodes reposent généralement sur le biais d'apprentissage suivant~: si un objet $Y$ est \\og{}proche\\fg{} d'un objet $X$, alors son analyse $f(Y)$ est un bon candidat pour $f(X)$. Alors que l'hypothèse invoquée se justifie pour les applications usuellement traitées en Apprentissage Automatique, la nature structurée et l'organisation paradigmatique des données linguistiques suggèrent une approche légèrement différente. Pour rendre compte de cette particularité, nous étudions un modèle reposant sur la notion de \\og{}proportion analogique\\fg{}. Dans ce modèle, l'analyse $f(T)$ d'un nouvel objet $T$ s'opère par identification d'une proportion analogique avec des objets $X$, $Y$ et $Z$ déjà connus. L'hypothèse analogique postule ainsi que si \\lana{X}{Y}{Z}{T}, alors \\lana{$f(X)$}{$f(Y)$}{$f(Z)$}{$f(T)$}. Pour inférer $f(T)$ à partir des $f(X)$, $f(Y)$, $f(Z)$ déjà connus, on résout l'\\og{}équation analogique\\fg{} d'inconnue $I$~: \\lana{$f(X)$}{$f(Y)$}{$f(Z)$}{$I$}. Nous présentons, dans la première partie de ce travail, une étude de ce modèle de proportion analogique au regard d'un cadre plus général que nous qualifierons d'\\og{}apprentissage par analogie\\fg{}. Ce cadre s'instancie dans un certain nombre de contextes~: dans le domaine des sciences cognitives, il s'agit de raisonnement par analogie, faculté essentielle au c\\oe{}ur de nombreux processus cognitifs~; dans le cadre de la linguistique traditionnelle, il fournit un support à un certain nombre de mécanismes tels que la création analogique, l'opposition ou la commutation~; dans le contexte de l'apprentissage automatique, il correspond à l'ensemble des méthodes d'apprentissage paresseux. Cette mise en perspective offre un éclairage sur la nature du modèle et les mécanismes sous-jacents. La deuxième partie de notre travail propose un cadre algébrique unifié, définissant la notion de proportion analogique. Partant d'un modèle de proportion analogique entre chaînes de symboles, éléments d'un monoïde libre, nous présentons une extension au cas plus général des semigroupes. Cette généralisation conduit directement à une définition valide pour tous les ensembles dérivant de la structure de semigroupe, permettant ainsi la modélisation des proportions analogiques entre représentations courantes d'entités linguistiques telles que chaînes de symboles, arbres, structures de traits et langages finis. Des algorithmes adaptés au traitement des proportions analogiques entre de tels objets structurés sont présentés. Nous proposons également quelques directions pour enrichir le modèle, et permettre ainsi son utilisation dans des cas plus complexes. Le modèle inférentiel étudié, motivé par des besoins en Traitement Automatique des Langues, est ensuite explicitement interprété comme une méthode d'Apprentissage Automatique. Cette formalisation a permis de mettre en évidence plusieurs de ses éléments caractéristiques. Une particularité notable du modèle réside dans sa capacité à traiter des objets structurés, aussi bien en entrée qu'en sortie, alors que la tâche classique de classification suppose en général un espace de sortie constitué d'un ensemble fini de classes. Nous montrons ensuite comment exprimer le biais d'apprentissage de la méthode à l'aide de l'introduction de la notion d'extension analogique. Enfin, nous concluons par la présentation de résultats expérimentaux issus de l'application de notre modèle à plusieurs tâches de Traitement Automatique des Langues~: transcription orthographique/phonétique, analyse flexionnelle et analyse dérivationnelle.",
    "uri": "https://pastel.hal.science/tel-00145147v1",
    "texte_nettoye": "Le panorama du Traitement Automatique des Langues est dominé par deux familles d'approches~: dans la première, la connaissance linguistique s'exprime sous forme de règles (grammaticales pour le traitement syntaxique, d'inférence pour le traitement sémantique, etc.), et de représentations sur lesquelles ces règles opèrent. La deuxième repose sur l'hypothèse d'un modèle probabiliste sous-jacent aux données, modèle dont les paramètres s'infèrent à partir de corpus de données linguistiques annotées. Ces deux familles de méthodes, bien qu'efficaces pour nombre d'applications, présentent de sérieuses limitations. Pour la première, il s'agit de la difficulté et du coût de construction des bases de connaissances de haute qualité~: les experts sont rares et la connaissance accumulée sur un domaine ne se transporte pas toujours simplement sur un autre domaine . Les méthodes probabilistes, quant à elles, ne traitent pas naturellement les objets fortement structurés, ne prévoient pas d'inclusion de connaissances linguistiques explicites, et surtout, reposent lourdement sur le choix a priori d'un certain modèle, puisqu'utilisant principalement des techniques de statistiques paramétriques. Dans le cadre d'un apprentissage automatique de données linguistiques, des modèles inférentiels alternatifs ont alors été proposés qui remettent en cause le principe d'abstraction opéré par les règles ou les modèles probabilistes. Selon cette conception, la connaissance linguistique reste implicitement représentée dans le corpus accumulé. Dans le domaine de l'Apprentissage Automatique, les méthodes suivant les même principes sont regroupées sous l'appellation d'apprentissage \\og{}paresseux\\fg{}. Ces méthodes reposent généralement sur le biais d'apprentissage suivant~: si un objet est \\og{}proche\\fg{} d'un objet , alors son analyse est un bon candidat pour . Alors que l'hypothèse invoquée se justifie pour les applications usuellement traitées en Apprentissage Automatique, la nature structurée et l'organisation paradigmatique des données linguistiques suggèrent une approche légèrement différente. Pour rendre compte de cette particularité, nous étudions un modèle reposant sur la notion de \\og{}proportion analogique\\fg{}. Dans ce modèle, l'analyse d'un nouvel objet s'opère par identification d'une proportion analogique avec des objets , et déjà connus. L'hypothèse analogique postule ainsi que si \\lana{X}{Y}{Z}{T}, alors \\lana{ }{ }{ }{ }. Pour inférer à partir des , , déjà connus, on résout l'\\og{}équation analogique\\fg{} d'inconnue ~: \\lana{ }{ }{ }{ }. Nous présentons, dans la première partie de ce travail, une étude de ce modèle de proportion analogique au regard d'un cadre plus général que nous qualifierons d'\\og{}apprentissage par analogie\\fg{}. Ce cadre s'instancie dans un certain nombre de contextes~: dans le domaine des sciences cognitives, il s'agit de raisonnement par analogie, faculté essentielle au c\\oe{}ur de nombreux processus cognitifs~; dans le cadre de la linguistique traditionnelle, il fournit un support à un certain nombre de mécanismes tels que la création analogique, l'opposition ou la commutation~; dans le contexte de l'apprentissage automatique, il correspond à l'ensemble des méthodes d'apprentissage paresseux. Cette mise en perspective offre un éclairage sur la nature du modèle et les mécanismes sous-jacents. La deuxième partie de notre travail propose un cadre algébrique unifié, définissant la notion de proportion analogique. Partant d'un modèle de proportion analogique entre chaînes de symboles, éléments d'un monoïde libre, nous présentons une extension au cas plus général des semigroupes. Cette généralisation conduit directement à une définition valide pour tous les ensembles dérivant de la structure de semigroupe, permettant ainsi la modélisation des proportions analogiques entre représentations courantes d'entités linguistiques telles que chaînes de symboles, arbres, structures de traits et langages finis. Des algorithmes adaptés au traitement des proportions analogiques entre de tels objets structurés sont présentés. Nous proposons également quelques directions pour enrichir le modèle, et permettre ainsi son utilisation dans des cas plus complexes. Le modèle inférentiel étudié, motivé par des besoins en Traitement Automatique des Langues, est ensuite explicitement interprété comme une méthode d'Apprentissage Automatique. Cette formalisation a permis de mettre en évidence plusieurs de ses éléments caractéristiques. Une particularité notable du modèle réside dans sa capacité à traiter des objets structurés, aussi bien en entrée qu'en sortie, alors que la tâche classique de classification suppose en général un espace de sortie constitué d'un ensemble fini de classes. Nous montrons ensuite comment exprimer le biais d'apprentissage de la méthode à l'aide de l'introduction de la notion d'extension analogique. Enfin, nous concluons par la présentation de résultats expérimentaux issus de l'application de notre modèle à plusieurs tâches de Traitement Automatique des Langues~: transcription orthographique/phonétique, analyse flexionnelle et analyse dérivationnelle."
  },
  {
    "title": "DEFT 2020 : détection de similarité entre phrases et extraction d'information",
    "authors": [
      "Mike Tapi Nzali"
    ],
    "year": 2020,
    "abstract": "Ce papier décrit la participation de Reezocar à la campagne d’évaluation DEFT 2020. Cette seizième édition du challenge a porté sur le calcul de similarité entre phrases et l’extraction d’information fine autour d’une douzaine de catégories dans des textes rédigés en Français. Le challenge propose trois tâches : (i) la première concerne l’identification du degré de similarité entre paires de phrases ; (ii) la deuxième concerne l’identification des phrases parallèles possibles pour une phrase source et (iii) la troisième concerne l’extraction d’information. Nous avons utilisé des méthodes d’apprentissage automatique pour effectuer ces tâches et avons obtenu des résultats satisfaisants sur l’ensemble des tâches.",
    "uri": "https://hal.science/hal-02784745v3",
    "texte_nettoye": "Ce papier décrit la participation de Reezocar à la campagne d’évaluation DEFT 2020. Cette seizième édition du challenge a porté sur le calcul de similarité entre phrases et l’extraction d’information fine autour d’une douzaine de catégories dans des textes rédigés en Français. Le challenge propose trois tâches : (i) la première concerne l’identification du degré de similarité entre paires de phrases ; (ii) la deuxième concerne l’identification des phrases parallèles possibles pour une phrase source et (iii) la troisième concerne l’extraction d’information. Nous avons utilisé des méthodes d’apprentissage automatique pour effectuer ces tâches et avons obtenu des résultats satisfaisants sur l’ensemble des tâches."
  },
  {
    "title": "La confidentialité différentielle : quelle quantification de la privacy dans le monde de l’apprentissage automatique ?",
    "authors": [
      "Edwige Cyffers"
    ],
    "year": 2021,
    "abstract": "L’effondrement des prix de stockage de l’information, la couverture croissante des usages informatiques et des collectes de données qui y sont associées ainsi que l’accroissement des capacités de traitement de l’information sont autant de bouleversements techniques dans le domaine de l’information. Que l’on parle de Big Data, ou que l’on considère simplement les conséquences de la numérisation lors de la crise sanitaire ces deux dernières années, la collecte généralisée de données sensibles est un nouvel enjeu de notre société. À titre d’exemple, un téléphone récolte le généralement la position instantanée, les relations, les heures de sommeil, les questions et autres données de santé de son utilisateur. La nécessité de sécuriser et d’éviter les fuites de données, qu’elles soient malicieuses ou non, est donc un enjeu clé de la transition numérique. Mais comment peut-on garantir la privacy ? Ce concept a de nombreuses facettes : offuscation, droit à l’oubli, anonymat, confidentialité, minimisation des données. Dans le cadre de l’apprentissage automatique (Machine learning), une métrique s’est imposée au sein de la recherche et des applications des GAFAM pour quantifier le niveau de privacy d’un procédé donné. La confidentialité différentielle (differential privacy) est en effet une définition mathématique qui réduit à un nombre réel le niveau de persistance d’une donnée dans les sorties d’un algorithme. Ce mémoire décrit l’émergence et les facteurs qui ont contribué au succès de cette quantification, ainsi que les conséquences implicites de cette définition sur les attentes de l’apprentissage automatique et le rapport entre l’individu et ses données. Nous abordons donc l’évolution de la notion de privacy face aux nouvelles réalités techniques, nous mettons en contexte la définition de confidentialité différentielle comme une technique de quantification et nous analysons ses variantes comme limites de la définition originelle.",
    "uri": "https://dumas.ccsd.cnrs.fr/dumas-03538878v1",
    "texte_nettoye": "L’effondrement des prix de stockage de l’information, la couverture croissante des usages informatiques et des collectes de données qui y sont associées ainsi que l’accroissement des capacités de traitement de l’information sont autant de bouleversements techniques dans le domaine de l’information. Que l’on parle de Big Data, ou que l’on considère simplement les conséquences de la numérisation lors de la crise sanitaire ces deux dernières années, la collecte généralisée de données sensibles est un nouvel enjeu de notre société. À titre d’exemple, un téléphone récolte le généralement la position instantanée, les relations, les heures de sommeil, les questions et autres données de santé de son utilisateur. La nécessité de sécuriser et d’éviter les fuites de données, qu’elles soient malicieuses ou non, est donc un enjeu clé de la transition numérique. Mais comment peut-on garantir la privacy ? Ce concept a de nombreuses facettes : offuscation, droit à l’oubli, anonymat, confidentialité, minimisation des données. Dans le cadre de l’apprentissage automatique (Machine learning), une métrique s’est imposée au sein de la recherche et des applications des GAFAM pour quantifier le niveau de privacy d’un procédé donné. La confidentialité différentielle (differential privacy) est en effet une définition mathématique qui réduit à un nombre réel le niveau de persistance d’une donnée dans les sorties d’un algorithme. Ce mémoire décrit l’émergence et les facteurs qui ont contribué au succès de cette quantification, ainsi que les conséquences implicites de cette définition sur les attentes de l’apprentissage automatique et le rapport entre l’individu et ses données. Nous abordons donc l’évolution de la notion de privacy face aux nouvelles réalités techniques, nous mettons en contexte la définition de confidentialité différentielle comme une technique de quantification et nous analysons ses variantes comme limites de la définition originelle."
  },
  {
    "title": "Utilisation de l’apprentissage de variété pour le diagnostic médical : application aux neuropathies optiques",
    "authors": [
      "Charlotte Fabert"
    ],
    "year": 2019,
    "abstract": "Les images de résonance magnétique pondérées en diffusion révèlent le mouvement brownien des molécules d’eau, qui peut être utilisé pour la reconstruction des faisceaux nerveux lors d'un post-traitement de tractographie. Des algorithmes d'apprentissage automatique ont récemment été proposés pour améliorer le processus de reconstruction de fibres. Cependant, le processus tractographique en tant que tel est chronophage en pratique clinique et dépend fortement de la modélisation du signal de diffusion. De plus, l'utilisation de méthodes d'apprentissage automatique supervisées pose un problème de gold standard. Proposer une méthode d’apprentissage automatique non supervisée avec la capacité de reconstruire des structures anatomiques directement à partir de la distribution d’orientation des fibres nerveuses (FOD) permet d’explorer des modèles de maladies. La représentation statistique de la géométrie du FOD nécessite des théories appropriées pour imposer des propriétés physiquement dépendant d'un espace non euclidien. Nous présentons ici un nouveau moyen d'analyse des FOD, tel qu’il est extrait par approximation Riemannienne. Nous avons ensuite appliqué cet algorithme sur les voies optiques antérieures humaines dans des conditions normales et pathologiques, afin de démontrer la faisabilité d'un diagnostic de neuropathie optique ischémique ou démyélinisante.",
    "uri": "https://dumas.ccsd.cnrs.fr/dumas-03240571v1",
    "texte_nettoye": "Les images de résonance magnétique pondérées en diffusion révèlent le mouvement brownien des molécules d’eau, qui peut être utilisé pour la reconstruction des faisceaux nerveux lors d'un post-traitement de tractographie. Des algorithmes d'apprentissage automatique ont récemment été proposés pour améliorer le processus de reconstruction de fibres. Cependant, le processus tractographique en tant que tel est chronophage en pratique clinique et dépend fortement de la modélisation du signal de diffusion. De plus, l'utilisation de méthodes d'apprentissage automatique supervisées pose un problème de gold standard. Proposer une méthode d’apprentissage automatique non supervisée avec la capacité de reconstruire des structures anatomiques directement à partir de la distribution d’orientation des fibres nerveuses (FOD) permet d’explorer des modèles de maladies. La représentation statistique de la géométrie du FOD nécessite des théories appropriées pour imposer des propriétés physiquement dépendant d'un espace non euclidien. Nous présentons ici un nouveau moyen d'analyse des FOD, tel qu’il est extrait par approximation Riemannienne. Nous avons ensuite appliqué cet algorithme sur les voies optiques antérieures humaines dans des conditions normales et pathologiques, afin de démontrer la faisabilité d'un diagnostic de neuropathie optique ischémique ou démyélinisante."
  },
  {
    "title": "De l'utilisation de la proportion analogique en apprentissage artificiel",
    "authors": [
      "Laurent Miclet",
      "Sabri Bayoudh",
      "Arnaud Delhay",
      "Harold Mouchère"
    ],
    "year": 2007,
    "abstract": "Cet article s'intéresse à la proportion analogique, une forme simple du raisonnement par analogie, et décrit son utilisation en apprentissage artificiel. Nous nous attachons plus particulièrement à définir une nouvelle notion, la dissimilarité analogique et à l'appliquer à des séquences. Après avoir défini la proportion analogique, la dissimilarité analogique et la résolution approchée d'équations analogiques, nous décrivons deux algorithmes qui rendent opérationnels ces notions pour des objets numériques ou symboliques et pour des séquences de ces objets. Nous montrons ensuite leur efficacité au travers de deux cas pratiques : le premier est l'apprentissage d'une règle de classification pour des objets décrits par des attributs nominaux; le second montre comment la génération de nouveaux exemples (par résolution approchée d'équations analogiques) peut aider un système de reconnaissance de caractères manuscrits à s'adapter très rapidement à un nouveau scripteur. Une discussion plus générale sur l'apport du raisonnement par analogie pour l'apprentissage artificiel termine cet article.",
    "uri": "https://inria.hal.science/hal-04650493v1",
    "texte_nettoye": "Cet article s'intéresse à la proportion analogique, une forme simple du raisonnement par analogie, et décrit son utilisation en apprentissage artificiel. Nous nous attachons plus particulièrement à définir une nouvelle notion, la dissimilarité analogique et à l'appliquer à des séquences. Après avoir défini la proportion analogique, la dissimilarité analogique et la résolution approchée d'équations analogiques, nous décrivons deux algorithmes qui rendent opérationnels ces notions pour des objets numériques ou symboliques et pour des séquences de ces objets. Nous montrons ensuite leur efficacité au travers de deux cas pratiques : le premier est l'apprentissage d'une règle de classification pour des objets décrits par des attributs nominaux; le second montre comment la génération de nouveaux exemples (par résolution approchée d'équations analogiques) peut aider un système de reconnaissance de caractères manuscrits à s'adapter très rapidement à un nouveau scripteur. Une discussion plus générale sur l'apport du raisonnement par analogie pour l'apprentissage artificiel termine cet article."
  },
  {
    "title": "Apprentissage par Renforcement pour la Conception de Systèmes Multi-Agents",
    "authors": [
      "Olivier Buffet"
    ],
    "year": 2002,
    "abstract": "Ce rapport présente les résultats des deux premières années d'une thèse s'intéressant à l'utilisation de l'apprentissage par renforcement (formalisme des Processus de Décision Markoviens) pour la conception de groupes d'agents coopérants (Systèmes Multi-Agents). Ce document a été rédigé à l'occasion de la réunion d'un comité de thèse.",
    "uri": "https://inria.hal.science/inria-00100787v1",
    "texte_nettoye": "Ce rapport présente les résultats des deux premières années d'une thèse s'intéressant à l'utilisation de l'apprentissage par renforcement (formalisme des Processus de Décision Markoviens) pour la conception de groupes d'agents coopérants (Systèmes Multi-Agents). Ce document a été rédigé à l'occasion de la réunion d'un comité de thèse."
  },
  {
    "title": "Apprentissage par Renforcement : Au delà des Processus Décisionnels de Markov (Vers la cognition incarnée)",
    "authors": [
      "Alain Dutech"
    ],
    "year": 2010,
    "abstract": "Ce document présente mon ``projet de recherche'' sur le thème de l'embodiment (``cognition incarnée'') au croisement des sciences cognitives, de l'intelligence artificielle et de la robotique. Plus précisément, je montre comment je compte explorer la façon dont un agent, artificiel ou biologique, élabore des représentations utiles et pertinentes de son environnement. Dans un premier temps, je positionne mes travaux en explicitant notamment les concepts de l'embodiment et de l'apprentissage par renforcement. Je m'attarde notamment sur la problématique de l'apprentissage par renforcement pour des tâches non-Markoviennes qui est une problématique commune aux différents travaux de recherche que j'ai menés au cours des treize dernières années dans des contextes mono et multi-agents, mais aussi robotique. L'analyse de ces travaux et de l'état de l'art du domaine me conforte dans l'idée que la principale difficulté pour l'agent est bien celle de trouver des représentations adaptées, utiles et pertinentes. J'argumente que l'on se retrouve face à une problématique fondamentale de la cognition, intimement liée aux problèmes de ``l'ancrage des symboles'', du ``frame problem'' et du fait ``d'être en situation'' et qu'on ne pourra y apporter des réponses que dans le cadre de l'embodiment. C'est à partir de ce constat que, dans une dernière partie, j'aborde les axes et les approches que je vais suivre pour poursuivre mes travaux en développant des techniques d'apprentissage robotique qui soient incrémentales, holistiques et motivationnelles.",
    "uri": "https://theses.hal.science/tel-00549108v1",
    "texte_nettoye": "Ce document présente mon ``projet de recherche'' sur le thème de l'embodiment (``cognition incarnée'') au croisement des sciences cognitives, de l'intelligence artificielle et de la robotique. Plus précisément, je montre comment je compte explorer la façon dont un agent, artificiel ou biologique, élabore des représentations utiles et pertinentes de son environnement. Dans un premier temps, je positionne mes travaux en explicitant notamment les concepts de l'embodiment et de l'apprentissage par renforcement. Je m'attarde notamment sur la problématique de l'apprentissage par renforcement pour des tâches non-Markoviennes qui est une problématique commune aux différents travaux de recherche que j'ai menés au cours des treize dernières années dans des contextes mono et multi-agents, mais aussi robotique. L'analyse de ces travaux et de l'état de l'art du domaine me conforte dans l'idée que la principale difficulté pour l'agent est bien celle de trouver des représentations adaptées, utiles et pertinentes. J'argumente que l'on se retrouve face à une problématique fondamentale de la cognition, intimement liée aux problèmes de ``l'ancrage des symboles'', du ``frame problem'' et du fait ``d'être en situation'' et qu'on ne pourra y apporter des réponses que dans le cadre de l'embodiment. C'est à partir de ce constat que, dans une dernière partie, j'aborde les axes et les approches que je vais suivre pour poursuivre mes travaux en développant des techniques d'apprentissage robotique qui soient incrémentales, holistiques et motivationnelles."
  },
  {
    "title": "Vers une approche Bio-inspirée pour l'orchestration pédagogique des activités mobiles d'apprentissage",
    "authors": [
      "Nassim Dennouni",
      "Yvan Peter",
      "Luigi Lancieri",
      "Zohra Slama"
    ],
    "year": 2015,
    "abstract": "Cet article présente une nouvelle approche pour recommander des parcours d'apprentissage adaptés au contexte des apprenants mobiles. Pour cela, nous proposons un système de filtrage collaboratif des points d'intérêt qui permet de guider l'apprenant dans le cadre d'une sortie pédagogique. Ce système de recommandation permet ainsi l’orchestration automatique des activités mobiles d’apprentissage en fonction de la localisation géographique des apprenants et de leurs historiques.",
    "uri": "https://hal.science/hal-01405974v1",
    "texte_nettoye": "Cet article présente une nouvelle approche pour recommander des parcours d'apprentissage adaptés au contexte des apprenants mobiles. Pour cela, nous proposons un système de filtrage collaboratif des points d'intérêt qui permet de guider l'apprenant dans le cadre d'une sortie pédagogique. Ce système de recommandation permet ainsi l’orchestration automatique des activités mobiles d’apprentissage en fonction de la localisation géographique des apprenants et de leurs historiques."
  },
  {
    "title": "Apprentissage Actif à l'ère des Grands Modèles de Langue (LLMs)",
    "authors": [
      "Shami Thirion Sen",
      "Rime Abrougui",
      "Guillaume Lechien",
      "Damien Nouvel"
    ],
    "year": 2025,
    "abstract": "<div><p>En TAL, la performance des modèles dépend fortement de la qualité et de la quantité des données annotées. Lorsque ces ressources sont limitées, l'apprentissage actif (Active Learning) offre une solution efficace en sélectionnant les échantillons les plus pertinents à annoter. Traditionnellement, cette tâche est réalisée par des annotateurs humains, mais nous explorons ici le potentiel du grand modèle de langue Mixtral-8x7B pour générer automatiquement ces annotations. En outre, nous analysons l'influence de l'augmentation des données dans un processus d'apprentissage actif pour la reconnaissance d'entités nommées afin d'améliorer la performance des catégories sous-représentées, ainsi que l'impact du prompt et des hyper-paramètres sur la qualité des annotations générées. Les évaluations conduites sur le corpus WiNER montrent que, malgré l'absence d'annotations manuelles, cette approche permet d'obtenir des performances comparables à notre baseline, tout en réduisant de 80 % la quantité des données annotées.</p></div>",
    "uri": "https://hal.science/hal-05332233v1",
    "texte_nettoye": "<div><p>En TAL, la performance des modèles dépend fortement de la qualité et de la quantité des données annotées. Lorsque ces ressources sont limitées, l'apprentissage actif (Active Learning) offre une solution efficace en sélectionnant les échantillons les plus pertinents à annoter. Traditionnellement, cette tâche est réalisée par des annotateurs humains, mais nous explorons ici le potentiel du grand modèle de langue Mixtral-8x7B pour générer automatiquement ces annotations. En outre, nous analysons l'influence de l'augmentation des données dans un processus d'apprentissage actif pour la reconnaissance d'entités nommées afin d'améliorer la performance des catégories sous-représentées, ainsi que l'impact du prompt et des hyper-paramètres sur la qualité des annotations générées. Les évaluations conduites sur le corpus WiNER montrent que, malgré l'absence d'annotations manuelles, cette approche permet d'obtenir des performances comparables à notre baseline, tout en réduisant de 80 % la quantité des données annotées.</p></div>"
  },
  {
    "title": "Personnalisation automatique des parcours d’apprentissage dans les Systèmes Tuteurs Intelligents",
    "authors": [
      "Didier Roy"
    ],
    "year": 2015,
    "abstract": "La recherche d’efficacité des systèmes tutoriels intelligents (STI) est un enjeu majeur. Nous présentons ici une méthode d’optimisation des parcours d’apprentissage pour chaque apprenant. Nous cherchons à proposer à chaque instant à l’apprenant l’activité qui lui fait faire le plus de progrès dans son apprentissage. Nous introduisons deux algorithmes : RiARiT, qui nécessite des informations préalables sur les activités, et ZPDES, qui n’en a pas besoin.",
    "uri": "https://inria.hal.science/hal-01144515v2",
    "texte_nettoye": "La recherche d’efficacité des systèmes tutoriels intelligents (STI) est un enjeu majeur. Nous présentons ici une méthode d’optimisation des parcours d’apprentissage pour chaque apprenant. Nous cherchons à proposer à chaque instant à l’apprenant l’activité qui lui fait faire le plus de progrès dans son apprentissage. Nous introduisons deux algorithmes : RiARiT, qui nécessite des informations préalables sur les activités, et ZPDES, qui n’en a pas besoin."
  },
  {
    "title": "Les apports de la transcription automatique de la parole : du cours magistral à la production collaborative de contenus pédagogiques",
    "authors": [
      "Raphaëlle Crétin"
    ],
    "year": 2021,
    "abstract": "L’article s’inscrit dans le cadre du projet ANR Pastel (Performing Automated Speech Transcription for Enhancing Learning) ayant pour objectif d’explorer le potentiel de la transcription automatique de la parole dans des contextes d’apprentissage mixtes. L’un des objectifs du projet est d’instrumenter des situations pédagogiques permettant d’envisager la création de Spoc (small private online course) destinés à des publics diversifiés. Nous présentons dans un premier temps les éléments méthodologiques ayant guidé l’analyse des besoins et les principaux résultats. Nous dressons dans un deuxième temps un état des lieux des pratiques informationnelles en et hors contexte pour un échantillon composé d’étudiants ou d’enseignants, ainsi que des changements de perspective pédagogique induits par les potentialités d’usage d’une technologie nouvelle comme la transcription automatique. Dans un troisième temps, le protocole d’analyse des usages quantitatifs et qualitatifs concernant la situation pédagogique instrumentée en décembre dernier et une expérimentation menée en mars est expliqué. Enfin, nous discutons des limites de notre approche du point de vue de l’instrumentation de situations pédagogiques et des perspectives du projet.",
    "uri": "https://hal.science/hal-01824285v1",
    "texte_nettoye": "L’article s’inscrit dans le cadre du projet ANR Pastel (Performing Automated Speech Transcription for Enhancing Learning) ayant pour objectif d’explorer le potentiel de la transcription automatique de la parole dans des contextes d’apprentissage mixtes. L’un des objectifs du projet est d’instrumenter des situations pédagogiques permettant d’envisager la création de Spoc (small private online course) destinés à des publics diversifiés. Nous présentons dans un premier temps les éléments méthodologiques ayant guidé l’analyse des besoins et les principaux résultats. Nous dressons dans un deuxième temps un état des lieux des pratiques informationnelles en et hors contexte pour un échantillon composé d’étudiants ou d’enseignants, ainsi que des changements de perspective pédagogique induits par les potentialités d’usage d’une technologie nouvelle comme la transcription automatique. Dans un troisième temps, le protocole d’analyse des usages quantitatifs et qualitatifs concernant la situation pédagogique instrumentée en décembre dernier et une expérimentation menée en mars est expliqué. Enfin, nous discutons des limites de notre approche du point de vue de l’instrumentation de situations pédagogiques et des perspectives du projet."
  },
  {
    "title": "Évaluation des performances de l’intelligence artificielle et de l’apprentissage automatique pour la prévision des crues : étude de cas du bassin versant de l’Ill",
    "authors": [
      "Nicolas Reiminger",
      "Xavier Jurado",
      "Loïc Saunier",
      "Loïc Maurer",
      "Eva Reiminger",
      "Lucie Weber",
      "Ly Nguyen",
      "Cédric Wemmert"
    ],
    "year": 2024,
    "abstract": "Les inondations, une aléa récurrente pouvant se montrer dévastatrice, peuvent causer de lourds dommages humains et économiques, et sont susceptibles de s'intensifier avec le changement climatique. Leurs impacts soulignent la nécessité d'anticiper les crues en développant constamment des outils efficaces et à la pointe pour la gestion du risque d'inondation. En France, la prévision des crues repose sur des outils de modélisation spécifiques basés sur la physique, la donnée ou une combinaison des deux. L'intelligence artificielle (IA), en plein essor, offre de nouvelles possibilités et perspectives pour les ingénieurs et chercheurs du domaine, mais n'est cependant pas encore utilisée à l'échelle nationale pour la prévision des crues. Bien que les algorithmes complexes d'apprentissage automatique (ML, Machine Learning) soient prometteurs pour la prévision des crues, leur mise en œuvre et leur utilisation courante restent difficiles pour les non-spécialistes. Cet article explore l'utilisation de six algorithmes d'apprentissage automatique, incluant des modèles simples d'ensemble, pour prévoir les hauteurs d'eau à 24 heures au niveau de deux stations de prévision des crues en France situées dans le Grand Est. Les résultats montrent que les niveaux d'eau, et donc les crues, peuvent être anticipés 24 heures à l'avance en utilisant uniquement des données de hauteur d'eau et par l'intermédiaire de modèles d'apprentissage automatique simples appartenant aux modèles ensemblistes. (Random Forest et Gradient Boosting). Ces derniers se révèlent performants, tout en restant accessible aux non-spécialistes, tandis que les modèles plus complexes ne permettent pas d'obtenir des résultats supérieurs en l'absence de données météorologiques supplémentaires telles que les pluies. Ce travail ouvre la voie vers l'utilisation de l'apprentissage automatique et de l'intelligence artificielle pour la prévision des crues en France. Abstract Les inondations, un risque récurrent au potentiel dévastateur, peuvent causer des dommages humains et économiques importants et sont susceptibles de s'intensifier avec le changement climatique. Leurs impacts soulignent la nécessité d'anticiper les inondations en développant continuellement des outils efficaces et de pointe pour la gestion du risque d'inondation. En France, la prévision des inondations s'appuie sur des outils de modélisation spécifiques basés sur la physique, les données ou une combinaison des deux. L'intelligence artificielle (IA), en pleine expansion, offre de nouvelles possibilités et perspectives aux ingénieurs et chercheurs du domaine, mais n'est pas encore utilisée à l'échelle nationale pour la prévision des inondations. Si les algorithmes complexes d'apprentissage automatique sont prometteurs pour la prévision des inondations, leur mise en œuvre et leur utilisation généralisée restent difficiles pour les non-spécialistes. Cet article explore l'utilisation de six algorithmes d'apprentissage automatique, dont des modèles d'ensemble simples, pour prévoir les niveaux d'eau 24 heures à l'avance dans deux stations de prévision des inondations dans la région du Grand Est en France. Les résultats démontrent que les niveaux d'eau, et par conséquent les inondations, peuvent être anticipés en utilisant uniquement des données de niveau d'eau grâce à des modèles d'apprentissage automatique simples appartenant à des modèles d'ensemble (Random Forest et Gradient Boosting). Ces modèles sont performants tout en restant accessibles aux non-spécialistes, alors que les modèles plus complexes ne donnent pas de résultats supérieurs en l’absence de données météorologiques complémentaires telles que les précipitations. Ces travaux ouvrent la voie à l’utilisation de l’apprentissage automatique et de l’intelligence artificielle dans la prévision des inondations en France.",
    "uri": "https://hal.science/hal-04799457v1",
    "texte_nettoye": "Les inondations, une aléa récurrente pouvant se montrer dévastatrice, peuvent causer de lourds dommages humains et économiques, et sont susceptibles de s'intensifier avec le changement climatique. Leurs impacts soulignent la nécessité d'anticiper les crues en développant constamment des outils efficaces et à la pointe pour la gestion du risque d'inondation. En France, la prévision des crues repose sur des outils de modélisation spécifiques basés sur la physique, la donnée ou une combinaison des deux. L'intelligence artificielle (IA), en plein essor, offre de nouvelles possibilités et perspectives pour les ingénieurs et chercheurs du domaine, mais n'est cependant pas encore utilisée à l'échelle nationale pour la prévision des crues. Bien que les algorithmes complexes d'apprentissage automatique (ML, Machine Learning) soient prometteurs pour la prévision des crues, leur mise en œuvre et leur utilisation courante restent difficiles pour les non-spécialistes. Cet article explore l'utilisation de six algorithmes d'apprentissage automatique, incluant des modèles simples d'ensemble, pour prévoir les hauteurs d'eau à 24 heures au niveau de deux stations de prévision des crues en France situées dans le Grand Est. Les résultats montrent que les niveaux d'eau, et donc les crues, peuvent être anticipés 24 heures à l'avance en utilisant uniquement des données de hauteur d'eau et par l'intermédiaire de modèles d'apprentissage automatique simples appartenant aux modèles ensemblistes. (Random Forest et Gradient Boosting). Ces derniers se révèlent performants, tout en restant accessible aux non-spécialistes, tandis que les modèles plus complexes ne permettent pas d'obtenir des résultats supérieurs en l'absence de données météorologiques supplémentaires telles que les pluies. Ce travail ouvre la voie vers l'utilisation de l'apprentissage automatique et de l'intelligence artificielle pour la prévision des crues en France."
  },
  {
    "title": "Analyse automatique de la structure thématique du discours pour la navigation documentaire",
    "authors": [
      "Frédérik Bilhaut"
    ],
    "year": 2004,
    "abstract": "Le système que nous proposons s'articule sur trois points. En premier lieu, nous définissons une notion de thème composite permettant d'indexer finement le contenu informationnel d'un document par sa structure thématique. D'autre part, nous proposons un modèle de représentation des connaissances permettant de guider l'analyse discursive, sous la forme d'axes structurants du domaine. Enfin, nous proposons un système d'apprentissage supervisé qui, combiné à une interface utilisateur, permet la constitution et l'édition rapides de ces ressources. À partir de l'indexation obtenue, les applications visées sont la recherche d'information mais aussi la navigation intradocumentaire et la synthèse automatique.",
    "uri": "https://archivesic.ccsd.cnrs.fr/sic_00001223v1",
    "texte_nettoye": "Le système que nous proposons s'articule sur trois points. En premier lieu, nous définissons une notion de thème composite permettant d'indexer finement le contenu informationnel d'un document par sa structure thématique. D'autre part, nous proposons un modèle de représentation des connaissances permettant de guider l'analyse discursive, sous la forme d'axes structurants du domaine. Enfin, nous proposons un système d'apprentissage supervisé qui, combiné à une interface utilisateur, permet la constitution et l'édition rapides de ces ressources. À partir de l'indexation obtenue, les applications visées sont la recherche d'information mais aussi la navigation intradocumentaire et la synthèse automatique."
  },
  {
    "title": "Apprentissage auto-supervisé pour la détection d'actions illégales lors de la surveillance du trafic maritime",
    "authors": [
      "P Bernabé",
      "A Gotlieb",
      "B Legeard",
      "F Olaf Sem-Jacobsen",
      "H Spieker"
    ],
    "year": 2021,
    "abstract": "La surveillance du trafic maritime est confrontée à des difficultés très importantes dans la détection des activités illégales en mer. Dans cet article, nous présentons les premiers résultats d'une méthode d'apprentissage autosupervisé qui vise à déceler les déconnexions volontaires du système d'identification des navires. En traitant les données provenant de quatre satellites de surveillance norvégiens, notre modèle d'apprentissage vise l'identification de navires soupçonnés d'activités illégales telles que la pêche dans des zones protégées ou bien le franchissement de zones d'exclusion économique en temps réel. Dans cet article, nous présentons une approche fondée sur des techniques d'apprentissage auto-supervisé, et expérimentée à partir de données réelles.",
    "uri": "https://hal.science/hal-03874246v1",
    "texte_nettoye": "La surveillance du trafic maritime est confrontée à des difficultés très importantes dans la détection des activités illégales en mer. Dans cet article, nous présentons les premiers résultats d'une méthode d'apprentissage autosupervisé qui vise à déceler les déconnexions volontaires du système d'identification des navires. En traitant les données provenant de quatre satellites de surveillance norvégiens, notre modèle d'apprentissage vise l'identification de navires soupçonnés d'activités illégales telles que la pêche dans des zones protégées ou bien le franchissement de zones d'exclusion économique en temps réel. Dans cet article, nous présentons une approche fondée sur des techniques d'apprentissage auto-supervisé, et expérimentée à partir de données réelles."
  },
  {
    "title": "Optimisation et apprentissage de modèles biologiques : application à lirrigation [sic l'irrigation] de pomme de terre",
    "authors": [
      "Amaury Dubois"
    ],
    "year": 2020,
    "abstract": "Le sujet de la thèse porte sur une des thématiques du LISIC : la modélisation et la simulation de systèmes complexes, ainsi que sur l'optimisation et l'apprentissage automatique pour l'agronomie. Les objectifs de la thèse sont de répondre aux questions de pilotage de l'irrigation de la culture de pomme de terre par le développement d'outils d'aide à la décision à destination des exploitants agricoles. Le choix de cette culture est motivé par sa part importante dans la région des Hauts-de-France. Le manuscrit s'articule en 3 parties. La première partie traite de l'optimisation continue mutlimodale dans un contexte de boîte noire. Il en suit une présentation d'une méthodologie d'étalonnage automatique de paramètres de modèle biologique grâce à une reformulation en un problème d'optimisation continue mono-objectif multimodale de type boîte noire. La pertinence de l'utilisation de l'analyse inverse comme méthodologie de paramétrage automatique de modèles de grandes dimensions est ensuite démontrée. La deuxième partie présente 2 nouveaux algorithmes UCB Random with Decreasing Step-size et UCT Random with Decreasing Step-size. Ce sont des algorithmes d'optimisation continue multimodale boîte noire dont le choix de la position initiale des individus est assisté par un algorithmes d'apprentissage par renforcement. Les résultats montrent que ces algorithmes possèdent de meilleures performances que les algorithmes état de l'art Quasi Random with Decreasing Step-size. Enfin, la dernière partie est focalisée sur les principes et les méthodes d'apprentissage automatique (machine learning). Une reformulation du problème de la prédiction à une semaine de la teneur en eau dans le sol en un problème d'apprentissage supervisé a permis le développement d'un nouvel outil d'aide à la décision pour répondre à la problématique du pilotage des cultures.",
    "uri": "https://theses.hal.science/tel-03144024v1",
    "texte_nettoye": "Le sujet de la thèse porte sur une des thématiques du LISIC : la modélisation et la simulation de systèmes complexes, ainsi que sur l'optimisation et l'apprentissage automatique pour l'agronomie. Les objectifs de la thèse sont de répondre aux questions de pilotage de l'irrigation de la culture de pomme de terre par le développement d'outils d'aide à la décision à destination des exploitants agricoles. Le choix de cette culture est motivé par sa part importante dans la région des Hauts-de-France. Le manuscrit s'articule en 3 parties. La première partie traite de l'optimisation continue mutlimodale dans un contexte de boîte noire. Il en suit une présentation d'une méthodologie d'étalonnage automatique de paramètres de modèle biologique grâce à une reformulation en un problème d'optimisation continue mono-objectif multimodale de type boîte noire. La pertinence de l'utilisation de l'analyse inverse comme méthodologie de paramétrage automatique de modèles de grandes dimensions est ensuite démontrée. La deuxième partie présente 2 nouveaux algorithmes UCB Random with Decreasing Step-size et UCT Random with Decreasing Step-size. Ce sont des algorithmes d'optimisation continue multimodale boîte noire dont le choix de la position initiale des individus est assisté par un algorithmes d'apprentissage par renforcement. Les résultats montrent que ces algorithmes possèdent de meilleures performances que les algorithmes état de l'art Quasi Random with Decreasing Step-size. Enfin, la dernière partie est focalisée sur les principes et les méthodes d'apprentissage automatique (machine learning). Une reformulation du problème de la prédiction à une semaine de la teneur en eau dans le sol en un problème d'apprentissage supervisé a permis le développement d'un nouvel outil d'aide à la décision pour répondre à la problématique du pilotage des cultures."
  },
  {
    "title": "Acquisition de connaissances et apprentissage automatique pour l'élaboration d'une base de connaissances",
    "authors": [
      "Habib Hadj-Mabrouk",
      "Bernard Houriez"
    ],
    "year": 1992,
    "abstract": "Cet article présente une étude de faisabilité d'un système à base de connaissances pour l'aide à la certification des Systèmes de Transport Terrestre Automatisé (STA). L'étude nous a conduit à introduire conjointement des techniques d'acquisition de connaissances et d'apprentissage symbolique-numérique. L'approche développée met à contribution deux mécanismes d'apprentissage : CHARADE /GANA 87/ et CLASCA /HADJ 91/, pour dégager un savoir-faire en certification, à partir d'exemples de scénarios historiques d'analyse de sécurité détenus par les experts de certification et les constructeurs sur les \"STA\" déjà certifiés. Cette recherche est soutenue par une bourse de doctorat cofinancée par la Région Nord-Pas de Calais et l'INRETS. Elle résulte d'une collaboration entre l'INRETS-CRESTA et le LAIH de l'Université de Valenciennes dans le cadre du GRRT (Groupement Régional pour la Recherche dans les Transports).",
    "uri": "https://hal.science/hal-03027659v1",
    "texte_nettoye": "Cet article présente une étude de faisabilité d'un système à base de connaissances pour l'aide à la certification des Systèmes de Transport Terrestre Automatisé (STA). L'étude nous a conduit à introduire conjointement des techniques d'acquisition de connaissances et d'apprentissage symbolique-numérique. L'approche développée met à contribution deux mécanismes d'apprentissage : CHARADE /GANA 87/ et CLASCA /HADJ 91/, pour dégager un savoir-faire en certification, à partir d'exemples de scénarios historiques d'analyse de sécurité détenus par les experts de certification et les constructeurs sur les \"STA\" déjà certifiés. Cette recherche est soutenue par une bourse de doctorat cofinancée par la Région Nord-Pas de Calais et l'INRETS. Elle résulte d'une collaboration entre l'INRETS-CRESTA et le LAIH de l'Université de Valenciennes dans le cadre du GRRT (Groupement Régional pour la Recherche dans les Transports)."
  },
  {
    "title": "Apprentissage symbolique de grammaires et traitement automatique des langues",
    "authors": [
      "Erwan Moreau"
    ],
    "year": 2007,
    "abstract": "Le modèle de Gold formalise le processus d'apprentissage d'un langage. Nous présentons dans cet article les avantages et inconvénients de ce cadre théorique contraignant, dans la perspective d'applications en TAL. Nous décrivons brièvement les récentes avancées dans ce domaine, qui soulèvent selon nous certaines questions importantes.",
    "uri": "https://hal.science/hal-00487071v1",
    "texte_nettoye": "Le modèle de Gold formalise le processus d'apprentissage d'un langage. Nous présentons dans cet article les avantages et inconvénients de ce cadre théorique contraignant, dans la perspective d'applications en TAL. Nous décrivons brièvement les récentes avancées dans ce domaine, qui soulèvent selon nous certaines questions importantes."
  },
  {
    "title": "Apprentissage de représentation et auto-organisation modulaire pour un agent autonome",
    "authors": [
      "Bruno Scherrer"
    ],
    "year": 2003,
    "abstract": "Cette thèse étudie l'utilisation d'algorithmes connexionnistes pour résoudre des problèmes d'apprentissage par renforcement. Les algorithmes connexionnistes sont inspirés de la manière dont le cerveau traite l'information : ils impliquent un grand nombre d'unités simples fortement interconnectées, manipulant des informations numériques de manière distribuée et massivement parallèle. L'apprentissage par renforcement est une théorie computationnelle qui permet de décrire l'interaction entre un agent et un environnement : elle permet de formaliser précisément le problème consistant à atteindre un certain nombre de buts via l'interaction. Nous avons considéré trois problèmes de complexité croissante et montré qu'ils admettaient des solutions algorithmiques connexionnistes : 1) L'apprentissage par renforcement dans un petit espace d'états : nous nous appuyons sur un algorithme de la littérature pour construire un réseau connexionniste ; les paramètres du problème sont stockés par les poids des unités et des connexions et le calcul du plan est le résultat d'une activité distribuée dans le réseau. 2) L'apprentissage d'une représentation pour approximer un problème d'apprentissage par renforcement ayant un grand espace d'états : nous automatisons le procédé consistant à construire une partition de l'espace d'états pour approximer un problème de grande taille. 3) L'auto-organisation en modules spécialisés pour approximer plusieurs problèmes d'apprentissage par renforcement ayant un grand espace d'états : nous proposons d'exploiter le principe \"diviser pour régner\" et montrons comment plusieurs tâches peuvent être réparties efficacement sur un petit nombre de modules fonctionnels spécialisés.",
    "uri": "https://theses.hal.science/tel-00003377v1",
    "texte_nettoye": "Cette thèse étudie l'utilisation d'algorithmes connexionnistes pour résoudre des problèmes d'apprentissage par renforcement. Les algorithmes connexionnistes sont inspirés de la manière dont le cerveau traite l'information : ils impliquent un grand nombre d'unités simples fortement interconnectées, manipulant des informations numériques de manière distribuée et massivement parallèle. L'apprentissage par renforcement est une théorie computationnelle qui permet de décrire l'interaction entre un agent et un environnement : elle permet de formaliser précisément le problème consistant à atteindre un certain nombre de buts via l'interaction. Nous avons considéré trois problèmes de complexité croissante et montré qu'ils admettaient des solutions algorithmiques connexionnistes : 1) L'apprentissage par renforcement dans un petit espace d'états : nous nous appuyons sur un algorithme de la littérature pour construire un réseau connexionniste ; les paramètres du problème sont stockés par les poids des unités et des connexions et le calcul du plan est le résultat d'une activité distribuée dans le réseau. 2) L'apprentissage d'une représentation pour approximer un problème d'apprentissage par renforcement ayant un grand espace d'états : nous automatisons le procédé consistant à construire une partition de l'espace d'états pour approximer un problème de grande taille. 3) L'auto-organisation en modules spécialisés pour approximer plusieurs problèmes d'apprentissage par renforcement ayant un grand espace d'états : nous proposons d'exploiter le principe \"diviser pour régner\" et montrons comment plusieurs tâches peuvent être réparties efficacement sur un petit nombre de modules fonctionnels spécialisés."
  },
  {
    "title": "Tri-apprentissage génératif : génération de données pour de la reconnaissance d'entitées nommées semi-supervisé",
    "authors": [
      "Hugo Boulanger",
      "Thomas Lavergne",
      "Sophie Rosset"
    ],
    "year": 2023,
    "abstract": "Le développement de solutions de traitement automatique de la langue pour de nouvelles tâches nécessite des données, dont l'obtention est coûteuses. L'accès aux données peut être limité en raison de la nature sensible des données. La plupart des travaux récents ont exploité de grands modèles pré-entraînés pour initialiser des versions spécialisées de ceux-ci. La spécialisation d'un tel modèle nécessite toujours une quantité élevée de données étiquetées spécifiques à la tâche cible. Nous utilisons l'apprentissage semi-supervisé pour entraîner des modèles dans un contexte où le nombre d'exemples étiquetés est limité et le nombre de données non étiquetées est nul. Nous étudions plusieurs méthodes pour générer le corpus non étiqueté nécessaire à l'utilisation de l'apprentissage semi-supervisé. Nous introduisons les méthodes de génération entre les épisodes d'entraînement et utilisons les modèles entraînés pour filtrer les exemples générés. Nous testons cette génération avec le tri-apprentissage et l'auto-apprentissage sur des corpus Anglais et Français.",
    "uri": "https://hal.science/hal-04130130v1",
    "texte_nettoye": "Le développement de solutions de traitement automatique de la langue pour de nouvelles tâches nécessite des données, dont l'obtention est coûteuses. L'accès aux données peut être limité en raison de la nature sensible des données. La plupart des travaux récents ont exploité de grands modèles pré-entraînés pour initialiser des versions spécialisées de ceux-ci. La spécialisation d'un tel modèle nécessite toujours une quantité élevée de données étiquetées spécifiques à la tâche cible. Nous utilisons l'apprentissage semi-supervisé pour entraîner des modèles dans un contexte où le nombre d'exemples étiquetés est limité et le nombre de données non étiquetées est nul. Nous étudions plusieurs méthodes pour générer le corpus non étiqueté nécessaire à l'utilisation de l'apprentissage semi-supervisé. Nous introduisons les méthodes de génération entre les épisodes d'entraînement et utilisons les modèles entraînés pour filtrer les exemples générés. Nous testons cette génération avec le tri-apprentissage et l'auto-apprentissage sur des corpus Anglais et Français."
  },
  {
    "title": "Détection de l'état de surface de la route pour un véhicule autonome par un système NIR LED et des approches d'apprentissage automatique",
    "authors": [
      "Hongyi Zhang"
    ],
    "year": 2022,
    "abstract": "Le domaine des véhicules autonomes a suscité un grand intérêt ces dernières années. Afin de garantir au passager une expérience sûre et confortable sur les véhicules autonomes, des systèmes d'obstacles avancés doivent être mis en œuvre. Bien que les solutions actuelles de détection d'obstacles aient montré de bonnes performances, elles doivent être encore améliorées pour une sécurité accrue des véhicules autonomes sur route, de jour comme de nuit. En particulier, les véhicules autonomes dans la vie réelle peuvent rencontrer de la glace, de la neige ou des flaques d'eau, qui peuvent être la cause de collisions graves et d'accidents de la circulation. Les systèmes de détection doivent donc permettre de détecter les changements d'état de la route pour anticiper la réaction du véhicule et/ou désactiver les fonctions automatisées. L'objectif de cette thèse est de proposer un système pour les véhicules autonomes afin de détecter les conditions de chaussée induites par la météo. Après une étude approfondie de l'état de l'art, un système proche infrarouge (NIR) basé sur des LED et un système d'apprentissage automatique sont proposés pour la détection diurne et nocturne. Le système NIR a été conçu puis validé expérimentalement et, les spécifications techniques du système ont été définies. Le système d'apprentissage automatique est de plus proposé comme solution complémentaire au système NIR. Différents modèles d'apprentissage ont été testés et comparés en termes de performance. Enfin, les résultats sont discutés et une combinaison des deux systèmes est proposée afin de garantir une performance accrue pour la reconnaissance des conditions de route.",
    "uri": "https://hal.science/tel-04515266v1",
    "texte_nettoye": "Le domaine des véhicules autonomes a suscité un grand intérêt ces dernières années. Afin de garantir au passager une expérience sûre et confortable sur les véhicules autonomes, des systèmes d'obstacles avancés doivent être mis en œuvre. Bien que les solutions actuelles de détection d'obstacles aient montré de bonnes performances, elles doivent être encore améliorées pour une sécurité accrue des véhicules autonomes sur route, de jour comme de nuit. En particulier, les véhicules autonomes dans la vie réelle peuvent rencontrer de la glace, de la neige ou des flaques d'eau, qui peuvent être la cause de collisions graves et d'accidents de la circulation. Les systèmes de détection doivent donc permettre de détecter les changements d'état de la route pour anticiper la réaction du véhicule et/ou désactiver les fonctions automatisées. L'objectif de cette thèse est de proposer un système pour les véhicules autonomes afin de détecter les conditions de chaussée induites par la météo. Après une étude approfondie de l'état de l'art, un système proche infrarouge (NIR) basé sur des LED et un système d'apprentissage automatique sont proposés pour la détection diurne et nocturne. Le système NIR a été conçu puis validé expérimentalement et, les spécifications techniques du système ont été définies. Le système d'apprentissage automatique est de plus proposé comme solution complémentaire au système NIR. Différents modèles d'apprentissage ont été testés et comparés en termes de performance. Enfin, les résultats sont discutés et une combinaison des deux systèmes est proposée afin de garantir une performance accrue pour la reconnaissance des conditions de route."
  },
  {
    "title": "L'apprentissage automatique à la rescousse de la relation client : vers un modèle de compréhension polyvalente de messages clients",
    "authors": [
      "Octavia Efraim"
    ],
    "year": 2019,
    "abstract": "",
    "uri": "https://hal.science/hal-02498277v1",
    "texte_nettoye": ""
  },
  {
    "title": "Apprentissage automatique et acquisition des connaissances : deux approches complémentaires pour les systèmes à base de connaissances. Application au système \"ACASYA\" d'aide à la certification des systèmes de transport automatisés",
    "authors": [
      "Habib Hadj-Mabrouk"
    ],
    "year": 1992,
    "abstract": "Ce mémoire de thèse de Doctorat présente une contribution à l'amélioration des méthodes usuelles d'analyse de sécurité employées dans le cadre de la certification des systèmes de transport automatisés (STA). La mission des experts de certification consiste à apprécier le caractère sécuritaire d'un nouveau STA en évaluant la complétude des scénarios d'accidents envisagés dans l'étude de sécurité du constructeur. La méthodologie d'aide à la certification développée repose sur l'utilisation conjointe et complémentaire de l'acquisition des connaissances et de l'apprentissage automatique. La méthode d'acquisition de connaissances choisie s'est révélée efficace pour extraire et formaliser les connaissances historiques d'analyse de sécurité mais insuffisante pour acquérir en détail la démarche experte de certification qui est fortement intuitive et évolutive. Pour pallier cette lacune, notre étude s'est orientée vers l'utilisation des techniques d'apprentissage automatique. La difficulté de définir et choisir un système d'apprentissage adapté aux exigences d'une application industrielle nous a conduits à proposer une caractérisation du processus d'apprentissage. ACASYA est l'environnement logiciel développé pour supporter la méthodologie d'aide à la certification. Il est composé de deux modules principaux: CLASCA et EVALSCA, respectivement dédiés à la classification et à l'évaluation des scénarios d'accidents. CLASCA que nous avons entièrement conçu est un système d'apprentissage symbolique-numérique, inductif, incrémental, non monotone et interactif. EVALSCA, développé autour du système d'apprentissage de règles CHARADE, a pour objectif de suggérer aux experts de certification d'éventuelles pannes non considérées par le constructeur et susceptibles de mettre en défaut la sécurité d'un nouveau STA. A ce jour, ACASYA a prouvé l'intérêt de la méthodologie pour formaliser, exploiter et pérenniser le savoir faire de l'expert de certification, en vue de tendre vers l'exhaustivité de l'analyse de sécurité. Par opposition aux systèmes d'aide au diagnostic, ACASYA peut être perçu comme un outil d'aide à la prévention des pannes, situé en aval des méthodes prévisionnelles classiques d'analyse de sécurité.",
    "uri": "https://hal.science/tel-03020419v1",
    "texte_nettoye": "Ce mémoire de thèse de Doctorat présente une contribution à l'amélioration des méthodes usuelles d'analyse de sécurité employées dans le cadre de la certification des systèmes de transport automatisés (STA). La mission des experts de certification consiste à apprécier le caractère sécuritaire d'un nouveau STA en évaluant la complétude des scénarios d'accidents envisagés dans l'étude de sécurité du constructeur. La méthodologie d'aide à la certification développée repose sur l'utilisation conjointe et complémentaire de l'acquisition des connaissances et de l'apprentissage automatique. La méthode d'acquisition de connaissances choisie s'est révélée efficace pour extraire et formaliser les connaissances historiques d'analyse de sécurité mais insuffisante pour acquérir en détail la démarche experte de certification qui est fortement intuitive et évolutive. Pour pallier cette lacune, notre étude s'est orientée vers l'utilisation des techniques d'apprentissage automatique. La difficulté de définir et choisir un système d'apprentissage adapté aux exigences d'une application industrielle nous a conduits à proposer une caractérisation du processus d'apprentissage. ACASYA est l'environnement logiciel développé pour supporter la méthodologie d'aide à la certification. Il est composé de deux modules principaux: CLASCA et EVALSCA, respectivement dédiés à la classification et à l'évaluation des scénarios d'accidents. CLASCA que nous avons entièrement conçu est un système d'apprentissage symbolique-numérique, inductif, incrémental, non monotone et interactif. EVALSCA, développé autour du système d'apprentissage de règles CHARADE, a pour objectif de suggérer aux experts de certification d'éventuelles pannes non considérées par le constructeur et susceptibles de mettre en défaut la sécurité d'un nouveau STA. A ce jour, ACASYA a prouvé l'intérêt de la méthodologie pour formaliser, exploiter et pérenniser le savoir faire de l'expert de certification, en vue de tendre vers l'exhaustivité de l'analyse de sécurité. Par opposition aux systèmes d'aide au diagnostic, ACASYA peut être perçu comme un outil d'aide à la prévention des pannes, situé en aval des méthodes prévisionnelles classiques d'analyse de sécurité."
  },
  {
    "title": "Apprentissage de règles de réactions biochimiques à partir de propriétés en logique temporelle",
    "authors": [
      "Laurence Calzone",
      "Nathalie Chabrier-Rivier",
      "Francois Fages",
      "Sylvain Soliman"
    ],
    "year": 2005,
    "abstract": "Avec le développement de langages formels pour modéliser les systèmes d' interactions biomoléculaires, la possibilité d'effectuer des calculs symboliques au delà des simulations numér iques ouvre la voie à la conception de nouveaux outils de raisonnement automatique destinés au biologiste modélisateur. La machine abstraite biochimique BIOCHAM est un environnement logiciel qui offre un langage simple de règles pour modéliser les interactions biomoléculaires et un langage original fondé sur la logique temporelle pour formaliser les propriétés biologiques du système. En s'appuyant sur ces deux langages formels, il devient possible d'utiliser des techniques d'apprentissage automatique pour inférer de nouvelles règles de réaction moléculaire à partir de propriétés temporelles observées. Dans ce contexte, le but est de corriger ou compléter les modèles BIOCHAM semi-automatiquement. Dans cet article, nous décrivons le système d'apprentissage automatique de BIOCHAM, qui permet, d'une part, de trouver de nouvelles règles d'interaction à partir d' un modèle partiel et de contraintes exprimées en logique temporelle, et d'autre part, d'estimer les valeurs de paramètres cinétiques à partir de propriétés formalisées en logique temporelle avec contraintes numériques sur les concentrations ou leurs dérivées.",
    "uri": "https://inria.hal.science/inria-00000813v1",
    "texte_nettoye": "Avec le développement de langages formels pour modéliser les systèmes d' interactions biomoléculaires, la possibilité d'effectuer des calculs symboliques au delà des simulations numér iques ouvre la voie à la conception de nouveaux outils de raisonnement automatique destinés au biologiste modélisateur. La machine abstraite biochimique BIOCHAM est un environnement logiciel qui offre un langage simple de règles pour modéliser les interactions biomoléculaires et un langage original fondé sur la logique temporelle pour formaliser les propriétés biologiques du système. En s'appuyant sur ces deux langages formels, il devient possible d'utiliser des techniques d'apprentissage automatique pour inférer de nouvelles règles de réaction moléculaire à partir de propriétés temporelles observées. Dans ce contexte, le but est de corriger ou compléter les modèles BIOCHAM semi-automatiquement. Dans cet article, nous décrivons le système d'apprentissage automatique de BIOCHAM, qui permet, d'une part, de trouver de nouvelles règles d'interaction à partir d' un modèle partiel et de contraintes exprimées en logique temporelle, et d'autre part, d'estimer les valeurs de paramètres cinétiques à partir de propriétés formalisées en logique temporelle avec contraintes numériques sur les concentrations ou leurs dérivées."
  },
  {
    "title": "Enseignement de l’apprentissage automatique à destination d’ingénieur·e·s généralistes",
    "authors": [
      "Frédéric Sur"
    ],
    "year": 2023,
    "abstract": "L’apprentissage automatique est la discipline scientifique au cœur des récents et fulgurants succès de l’intelligence artificielle dans de nombreux domaines. Dans cet article, nous proposons un retour d’expérience sur l’enseignement de l’apprentissage automatique à destination d’ingénieur·e·s généralistes, dans le cadre d’un cours de tronc commun scientifique de la formation « Ingénieur Civil des Mines » de l’École des Mines de Nancy. L’objectif est de proposer un cours pertinent pour les élèves-ingénieur·e·s quel que soit le département scientifique choisi, grâce auquel ils et elles sauront mettre en application les principaux modèles d’apprentissage sur des données réelles variées et s’appuyer sur leur compréhension des fondements de la discipline pour identifier les limites pratiques, tout en ayant conscience des questions éthiques potentiellement soulevées.",
    "uri": "https://hal.science/hal-04213203v1",
    "texte_nettoye": "L’apprentissage automatique est la discipline scientifique au cœur des récents et fulgurants succès de l’intelligence artificielle dans de nombreux domaines. Dans cet article, nous proposons un retour d’expérience sur l’enseignement de l’apprentissage automatique à destination d’ingénieur·e·s généralistes, dans le cadre d’un cours de tronc commun scientifique de la formation « Ingénieur Civil des Mines » de l’École des Mines de Nancy. L’objectif est de proposer un cours pertinent pour les élèves-ingénieur·e·s quel que soit le département scientifique choisi, grâce auquel ils et elles sauront mettre en application les principaux modèles d’apprentissage sur des données réelles variées et s’appuyer sur leur compréhension des fondements de la discipline pour identifier les limites pratiques, tout en ayant conscience des questions éthiques potentiellement soulevées."
  },
  {
    "title": "Apprentissage multi-vue de co-similarités pour la classification",
    "authors": [
      "Gilles Bisson",
      "Clément Grimal"
    ],
    "year": 2012,
    "abstract": "En classification, les données se présentent souvent sous la forme d'une matrice de données unique [objets/caractéristiques]. Cependant, dans de nombreuses applications, plusieurs types d'objets liés par des relations peuvent exister, ce qui conduit à avoir plusieurs matrices représentant chacune une vue particulière sur les données. C'est le cas dans l'étude des réseaux sociaux ou les différents nœuds d'un graphe d'interaction font intervenir des utilisateurs, des documents, des termes, etc. Dans ce papier, nous introduisons une architecture permettant d'étendre les capacités de l'algorithme de calcul de co-similarité \\x-sim afin de le rendre apte à travailler sur des collections de matrices décrivant les relations entre plusieurs paires d'objets différents (multi-view clustering). Nous montrons que cette architecture offre un cadre formel intéressant et permet de délivrer souvent des résultats supérieurs ou égaux aux approches classiques mono-relation tout en permettant, grâce à une parallélisation possible des calculs, de réduire la complexité en temps et en espace des problèmes traités.",
    "uri": "https://hal.science/hal-00741470v1",
    "texte_nettoye": "En classification, les données se présentent souvent sous la forme d'une matrice de données unique [objets/caractéristiques]. Cependant, dans de nombreuses applications, plusieurs types d'objets liés par des relations peuvent exister, ce qui conduit à avoir plusieurs matrices représentant chacune une vue particulière sur les données. C'est le cas dans l'étude des réseaux sociaux ou les différents nœuds d'un graphe d'interaction font intervenir des utilisateurs, des documents, des termes, etc. Dans ce papier, nous introduisons une architecture permettant d'étendre les capacités de l'algorithme de calcul de co-similarité \\x-sim afin de le rendre apte à travailler sur des collections de matrices décrivant les relations entre plusieurs paires d'objets différents (multi-view clustering). Nous montrons que cette architecture offre un cadre formel intéressant et permet de délivrer souvent des résultats supérieurs ou égaux aux approches classiques mono-relation tout en permettant, grâce à une parallélisation possible des calculs, de réduire la complexité en temps et en espace des problèmes traités."
  },
  {
    "title": "Conférence d'Apprentissage (CAp 2008)",
    "authors": [
      "Florence d'Alché-Buc"
    ],
    "year": 2008,
    "abstract": "Ce volume rassemble les actes de la 10ème Conférence d'Apprentissage (CAp 2008) qui s'est tenue en France sur l'Ile de Porquerolles du 29 au 31 Mai 2008. L'Apprentissage Automatique, à la croisée de plusieurs disciplines, offre un cadre théorique et méthodologique pour l'inférence à partir de données. Depuis les années 80 où cette spécialité a été identifiée comme un domaine de recherche à part entière, différentes approches ont successivement ou parallèlement permis d'enrichir le bagage du chercheur en Apprentissage Automatique : les réseaux de neurones formels, les outils de la physique statistique, la programmation logique inductive, la théorie PAC de Valiant, la théorie statistique de l'apprentissage de Vapnik, les modèles graphiques, l'inférence grammaticale, les méthodes à noyaux, les méthodes d'ensemble, les méthodes spectrales...",
    "uri": "https://hal.science/hal-00343081v1",
    "texte_nettoye": "Ce volume rassemble les actes de la 10ème Conférence d'Apprentissage (CAp 2008) qui s'est tenue en France sur l'Ile de Porquerolles du 29 au 31 Mai 2008. L'Apprentissage Automatique, à la croisée de plusieurs disciplines, offre un cadre théorique et méthodologique pour l'inférence à partir de données. Depuis les années 80 où cette spécialité a été identifiée comme un domaine de recherche à part entière, différentes approches ont successivement ou parallèlement permis d'enrichir le bagage du chercheur en Apprentissage Automatique : les réseaux de neurones formels, les outils de la physique statistique, la programmation logique inductive, la théorie PAC de Valiant, la théorie statistique de l'apprentissage de Vapnik, les modèles graphiques, l'inférence grammaticale, les méthodes à noyaux, les méthodes d'ensemble, les méthodes spectrales..."
  },
  {
    "title": "Outils d'exploration de corpus et désambiguïsation lexicale automatique",
    "authors": [
      "Laurent Audibert"
    ],
    "year": 2003,
    "abstract": "Ce travail de thèse adresse le problème de la désambiguïsation lexicale automatique à l'aide de méthodes d'apprentissage supervisé. Dans une première partie, nous proposons un ensemble de puissants outils de manipulation de corpus linguistiques étiquetés. Pour réaliser ces outils, nous avons développé une bibliothèque C++ qui implémente un langage élaboré et expressif d'interrogation de corpus, basé sur des méta-expressions régulières. Dans une seconde partie, nous comparons divers algorithmes d'apprentissage supervisé, que nous utilisons ensuite pour mener à bien une étude systématique et approfondie de différents critères de désambiguïsation, basés sur la cooccurrence de mots et plus généralement de n-grammes. Nos résultats vont parfois à l'encontre de certaines pratiques dans le domaine. Par exemple, nous montrons que la suppression des mots grammaticaux dégrade les performances et que les bigrammes permettent d'obtenir de meilleurs résultats que les unigrammes.",
    "uri": "https://theses.hal.science/tel-00004475v1",
    "texte_nettoye": "Ce travail de thèse adresse le problème de la désambiguïsation lexicale automatique à l'aide de méthodes d'apprentissage supervisé. Dans une première partie, nous proposons un ensemble de puissants outils de manipulation de corpus linguistiques étiquetés. Pour réaliser ces outils, nous avons développé une bibliothèque C++ qui implémente un langage élaboré et expressif d'interrogation de corpus, basé sur des méta-expressions régulières. Dans une seconde partie, nous comparons divers algorithmes d'apprentissage supervisé, que nous utilisons ensuite pour mener à bien une étude systématique et approfondie de différents critères de désambiguïsation, basés sur la cooccurrence de mots et plus généralement de n-grammes. Nos résultats vont parfois à l'encontre de certaines pratiques dans le domaine. Par exemple, nous montrons que la suppression des mots grammaticaux dégrade les performances et que les bigrammes permettent d'obtenir de meilleurs résultats que les unigrammes."
  },
  {
    "title": "Apprentissage d'espaces prétopologiques pour l'extraction de connaissances structurées",
    "authors": [
      "Gaëtan Caillaut"
    ],
    "year": 2019,
    "abstract": "La prétopologie est une théorie mathématique visant à relaxer les axiomes régissant la théorie, bien connue, de la topologie. L'affaiblissement de cette axiomatique passe principalement par la redéfinition de l'opérateur d'adhérence qui, en topologie, est idempotent. La non-idempotence de l'opérateur d'adhérence prétopologique offre un cadre de travail plus pertinent pour la modélisation de phénomènes variés, par exemple des processus itératifs évoluant au cours du temps. La prétopologie est le fruit de la généralisation de plusieurs concepts, parmi lesquels la topologie mais aussi la théorie des graphes. Cette thèse comprend quatre parties majeures. La première partie consiste en une introduction du cadre théorique de la prétopologie puis à une mise en lumière de plusieurs applications de la prétopologie dans des domaines tels que l'apprentissage automatique, l'analyse d'images ou encore l'étude des systèmes complexes. La seconde partie permettra de poser et de définir la modélisation logique et multi-critères d'un espace prétopologique sur laquelle est basée cette thèse. Cette modélisation permet de définir des algorithmes d'apprentissage automatique de règles logiques afin de construire des espaces prétopologiques. Cette partie se focalisera sur l'apprentissage d'espaces prétopologiques non-restreints. L'étude des espaces prétopologiques non-restreints peut s'avérer incommode, notamment lorsque la population étudiée exhibe certaines propriétés structurelles pouvant être décrites dans un espace plus restreint et plus simple à appréhender. C'est pourquoi la troisième partie est dédiée à l'apprentissage d'espaces prétopologiques de type V. Ces espaces sont définis par une famille de préfiltres, ce qui impose une structure particulière. La méthode d'apprentissage, LPSMI, présentée dans cette partie, qui constitue la contribution majeure de cette thèse, tient compte de cette structure si particulière en exploitant le concept d'apprentissage multi-instances. Enfin la dernière partie décrit plusieurs cas d'applications du cadre théorique proposé dans cette thèse. Ainsi, des applications à l'extraction de taxonomies lexicales, à la détection de communautés ainsi qu'à l'ordonnancement d'évènements temporels sont présentées et permettent de montrer l'intérêt, la souplesse et la pertinence de la prétopologie et de l'apprentissage d'espaces prétopologiques dans des domaines variés.",
    "uri": "https://hal.science/tel-03625475v1",
    "texte_nettoye": "La prétopologie est une théorie mathématique visant à relaxer les axiomes régissant la théorie, bien connue, de la topologie. L'affaiblissement de cette axiomatique passe principalement par la redéfinition de l'opérateur d'adhérence qui, en topologie, est idempotent. La non-idempotence de l'opérateur d'adhérence prétopologique offre un cadre de travail plus pertinent pour la modélisation de phénomènes variés, par exemple des processus itératifs évoluant au cours du temps. La prétopologie est le fruit de la généralisation de plusieurs concepts, parmi lesquels la topologie mais aussi la théorie des graphes. Cette thèse comprend quatre parties majeures. La première partie consiste en une introduction du cadre théorique de la prétopologie puis à une mise en lumière de plusieurs applications de la prétopologie dans des domaines tels que l'apprentissage automatique, l'analyse d'images ou encore l'étude des systèmes complexes. La seconde partie permettra de poser et de définir la modélisation logique et multi-critères d'un espace prétopologique sur laquelle est basée cette thèse. Cette modélisation permet de définir des algorithmes d'apprentissage automatique de règles logiques afin de construire des espaces prétopologiques. Cette partie se focalisera sur l'apprentissage d'espaces prétopologiques non-restreints. L'étude des espaces prétopologiques non-restreints peut s'avérer incommode, notamment lorsque la population étudiée exhibe certaines propriétés structurelles pouvant être décrites dans un espace plus restreint et plus simple à appréhender. C'est pourquoi la troisième partie est dédiée à l'apprentissage d'espaces prétopologiques de type V. Ces espaces sont définis par une famille de préfiltres, ce qui impose une structure particulière. La méthode d'apprentissage, LPSMI, présentée dans cette partie, qui constitue la contribution majeure de cette thèse, tient compte de cette structure si particulière en exploitant le concept d'apprentissage multi-instances. Enfin la dernière partie décrit plusieurs cas d'applications du cadre théorique proposé dans cette thèse. Ainsi, des applications à l'extraction de taxonomies lexicales, à la détection de communautés ainsi qu'à l'ordonnancement d'évènements temporels sont présentées et permettent de montrer l'intérêt, la souplesse et la pertinence de la prétopologie et de l'apprentissage d'espaces prétopologiques dans des domaines variés."
  },
  {
    "title": "Apprentissage sur corpus de relations lexicales sémantiques - La linguistique et l'apprentissage au service d'applications du traitement automatique des langues",
    "authors": [
      "Pascale Sébillot"
    ],
    "year": 2002,
    "abstract": "Le document présente une synthèse des recherches que nous avons menées sur le thème de l'acquisition de ressources lexicales à partir de corpus textuels. Plus particulièrement, ces travaux portent sur le développement de méthodes d'apprentissage automatique de relations lexicales sémantiques, ayant pour objectif d'enrichir la description de mots dans une double optique de désambiguïsation et de traitement de variantes sémantiques intra- et intercatégorielles, et susceptibles d'être utilisées au sein de différentes applications (recherche d'information, filtrage...). Nos études se caractérisent particulièrement par le fort couplage que nous recherchons entre les méthodes d'apprentissage développées et des théories linguistiques. Ces théories nous servent de cadres pour déterminer les relations lexicales pertinentes, valider ce qui est acquis, voire mettre au point la méthode d'apprentissage nécessaire à cette acquisition; de plus, les éléments appris doivent être linguistiquement motivés et significatifs. Ainsi, nous décrivons comment, en nous positionnant dans le cadre de la sémantique interprétative de F. Rastier, nous cherchons à apprendre, par des méthodes statistiques (en particulier de classification ascendante hiérarchique), des liens paradigmatiques intracatégoriels - antonymie, synonymie..., mais aussi d'autres liens plus fins de type sémique - à partir de corpus non spécialisés. D'autre part, nous expliquons comment, en contrôlant leur pertinence grâce au formalisme du Lexique génératif de J. Pustejovsky, nous acquérons par de l'apprentissage symbolique de type programmation logique inductive des liens transcatégoriels nomino-verbaux. Parmi les perspectives évoquées en conclusion, nous abordons en particulier les questions soulevées lorsque l'on s'intéresse à l'insertion des relations acquises dans un système de recherche d'information pour reformuler des requêtes, ainsi que celles concernant l'évaluation des apports de ces ressources lexicales. Nous discutons également de la pertinence de l'utilisation de méthodes d'apprentissage explicative pour acquérir des informations en corpus.",
    "uri": "https://theses.hal.science/tel-00533657v1",
    "texte_nettoye": "Le document présente une synthèse des recherches que nous avons menées sur le thème de l'acquisition de ressources lexicales à partir de corpus textuels. Plus particulièrement, ces travaux portent sur le développement de méthodes d'apprentissage automatique de relations lexicales sémantiques, ayant pour objectif d'enrichir la description de mots dans une double optique de désambiguïsation et de traitement de variantes sémantiques intra- et intercatégorielles, et susceptibles d'être utilisées au sein de différentes applications (recherche d'information, filtrage...). Nos études se caractérisent particulièrement par le fort couplage que nous recherchons entre les méthodes d'apprentissage développées et des théories linguistiques. Ces théories nous servent de cadres pour déterminer les relations lexicales pertinentes, valider ce qui est acquis, voire mettre au point la méthode d'apprentissage nécessaire à cette acquisition; de plus, les éléments appris doivent être linguistiquement motivés et significatifs. Ainsi, nous décrivons comment, en nous positionnant dans le cadre de la sémantique interprétative de F. Rastier, nous cherchons à apprendre, par des méthodes statistiques (en particulier de classification ascendante hiérarchique), des liens paradigmatiques intracatégoriels - antonymie, synonymie..., mais aussi d'autres liens plus fins de type sémique - à partir de corpus non spécialisés. D'autre part, nous expliquons comment, en contrôlant leur pertinence grâce au formalisme du Lexique génératif de J. Pustejovsky, nous acquérons par de l'apprentissage symbolique de type programmation logique inductive des liens transcatégoriels nomino-verbaux. Parmi les perspectives évoquées en conclusion, nous abordons en particulier les questions soulevées lorsque l'on s'intéresse à l'insertion des relations acquises dans un système de recherche d'information pour reformuler des requêtes, ainsi que celles concernant l'évaluation des apports de ces ressources lexicales. Nous discutons également de la pertinence de l'utilisation de méthodes d'apprentissage explicative pour acquérir des informations en corpus."
  },
  {
    "title": "Apprentissage automatique profond et données géologiques : vers des assistants numériques intelligents pour les géosciences ?",
    "authors": [
      "Antoine Bouziat",
      "François Cokelaer",
      "Renaud Divies",
      "Sylvain Desroziers",
      "Mathieu Feraille"
    ],
    "year": 2021,
    "abstract": "",
    "uri": "https://hal.science/hal-03588997v1",
    "texte_nettoye": ""
  },
  {
    "title": "Méta-apprentissage pour l'analyse AMR translingue",
    "authors": [
      "Jeongwoo Kang",
      "Maximin Coavoux",
      "Cédric Lopez",
      "Didier Schwab"
    ],
    "year": 2024,
    "abstract": "L’analyse AMR multilingue consiste à prédire des analyses sémantiques AMR dans une langue cible lorsque les données d’entraînement ne sont disponibles que dans une langue source. Cette tâche n’a été étudiée que pour un petit nombre de langues en raison du manque de données multilingues. En s’inspirant de Langedijk et al. (2022), qui appliquent le méta-apprentissage à l’analyse syntaxique en dépendances translingue, nous étudions le méta-apprentissage pour l’analyse AMR translingue. Nous évaluons nos modèles dans des scénarios zero-shot et few-shot en croate, en farsi, en coréen, en chinois et en français. En particulier, nous développons dans le cadre de cet article des données d’évaluation en coréen et en croate, à partir du corpus AMR anglais Le Petit Prince. Nous étudions empiriquement cette approche en la comparant à une méthode classique d’apprentissage conjoint.",
    "uri": "https://inria.hal.science/hal-04623014v1",
    "texte_nettoye": "L’analyse AMR multilingue consiste à prédire des analyses sémantiques AMR dans une langue cible lorsque les données d’entraînement ne sont disponibles que dans une langue source. Cette tâche n’a été étudiée que pour un petit nombre de langues en raison du manque de données multilingues. En s’inspirant de Langedijk et al. (2022), qui appliquent le méta-apprentissage à l’analyse syntaxique en dépendances translingue, nous étudions le méta-apprentissage pour l’analyse AMR translingue. Nous évaluons nos modèles dans des scénarios zero-shot et few-shot en croate, en farsi, en coréen, en chinois et en français. En particulier, nous développons dans le cadre de cet article des données d’évaluation en coréen et en croate, à partir du corpus AMR anglais Le Petit Prince. Nous étudions empiriquement cette approche en la comparant à une méthode classique d’apprentissage conjoint."
  },
  {
    "title": "Méthode de suppression de cas pour une maintenance de base de cas.",
    "authors": [
      "Mohamed-Karim Haouchine",
      "Brigitte Chebel-Morello",
      "Noureddine Zerhouni"
    ],
    "year": 2006,
    "abstract": "Nous vous présentons un début de travail concernant la maintenance de la base de cas. Après un état de l'art des travaux fait dans le domaine, nous sommes amenés à faire deux propositions que nous n'avons pas encore exploitées. Une solution sur la suppression de cas et une autre en utilisant les outils d'apprentissage automatique, sachant que nous pouvons modéliser notre base de cas sous forme d'exemples. La proposition se fait sur deux niveaux, sur des travaux d'apprentissage automatique et sur l'élaboration d'une nouvelle mesure.",
    "uri": "https://hal.science/hal-00339349v1",
    "texte_nettoye": "Nous vous présentons un début de travail concernant la maintenance de la base de cas. Après un état de l'art des travaux fait dans le domaine, nous sommes amenés à faire deux propositions que nous n'avons pas encore exploitées. Une solution sur la suppression de cas et une autre en utilisant les outils d'apprentissage automatique, sachant que nous pouvons modéliser notre base de cas sous forme d'exemples. La proposition se fait sur deux niveaux, sur des travaux d'apprentissage automatique et sur l'élaboration d'une nouvelle mesure."
  },
  {
    "title": "Apprentissage sur corpus de relations lexicales sémantiques - La linguistique et l'apprentissage au service d'applications du traitement automatique des langues",
    "authors": [
      "Pascale Sébillot"
    ],
    "year": 2002,
    "abstract": "Le document présente une synthèse des recherches que nous avons menées sur le thème de l'acquisition de ressources lexicales à partir de corpus textuels. Plus particulièrement, ces travaux portent sur le développement de méthodes d'apprentissage automatique de relations lexicales sémantiques, ayant pour objectif d'enrichir la description de mots dans une double optique de désambiguïsation et de traitement de variantes sémantiques intra- et intercatégorielles, et susceptibles d'être utilisées au sein de différentes applications (recherche d'information, filtrage...). Nos études se caractérisent particulièrement par le fort couplage que nous recherchons entre les méthodes d'apprentissage développées et des théories linguistiques. Ces théories nous servent de cadres pour déterminer les relations lexicales pertinentes, valider ce qui est acquis, voire mettre au point la méthode d'apprentissage nécessaire à cette acquisition; de plus, les éléments appris doivent être linguistiquement motivés et significatifs. Ainsi, nous décrivons comment, en nous positionnant dans le cadre de la sémantique interprétative de F. Rastier, nous cherchons à apprendre, par des méthodes statistiques (en particulier de classification ascendante hiérarchique), des liens paradigmatiques intracatégoriels - antonymie, synonymie..., mais aussi d'autres liens plus fins de type sémique - à partir de corpus non spécialisés. D'autre part, nous expliquons comment, en contrôlant leur pertinence grâce au formalisme du Lexique génératif de J. Pustejovsky, nous acquérons par de l'apprentissage symbolique de type programmation logique inductive des liens transcatégoriels nomino-verbaux. Parmi les perspectives évoquées en conclusion, nous abordons en particulier les questions soulevées lorsque l'on s'intéresse à l'insertion des relations acquises dans un système de recherche d'information pour reformuler des requêtes, ainsi que celles concernant l'évaluation des apports de ces ressources lexicales. Nous discutons également de la pertinence de l'utilisation de méthodes d'apprentissage explicative pour acquérir des informations en corpus.",
    "uri": "https://theses.hal.science/tel-00533657v1",
    "texte_nettoye": "Le document présente une synthèse des recherches que nous avons menées sur le thème de l'acquisition de ressources lexicales à partir de corpus textuels. Plus particulièrement, ces travaux portent sur le développement de méthodes d'apprentissage automatique de relations lexicales sémantiques, ayant pour objectif d'enrichir la description de mots dans une double optique de désambiguïsation et de traitement de variantes sémantiques intra- et intercatégorielles, et susceptibles d'être utilisées au sein de différentes applications (recherche d'information, filtrage...). Nos études se caractérisent particulièrement par le fort couplage que nous recherchons entre les méthodes d'apprentissage développées et des théories linguistiques. Ces théories nous servent de cadres pour déterminer les relations lexicales pertinentes, valider ce qui est acquis, voire mettre au point la méthode d'apprentissage nécessaire à cette acquisition; de plus, les éléments appris doivent être linguistiquement motivés et significatifs. Ainsi, nous décrivons comment, en nous positionnant dans le cadre de la sémantique interprétative de F. Rastier, nous cherchons à apprendre, par des méthodes statistiques (en particulier de classification ascendante hiérarchique), des liens paradigmatiques intracatégoriels - antonymie, synonymie..., mais aussi d'autres liens plus fins de type sémique - à partir de corpus non spécialisés. D'autre part, nous expliquons comment, en contrôlant leur pertinence grâce au formalisme du Lexique génératif de J. Pustejovsky, nous acquérons par de l'apprentissage symbolique de type programmation logique inductive des liens transcatégoriels nomino-verbaux. Parmi les perspectives évoquées en conclusion, nous abordons en particulier les questions soulevées lorsque l'on s'intéresse à l'insertion des relations acquises dans un système de recherche d'information pour reformuler des requêtes, ainsi que celles concernant l'évaluation des apports de ces ressources lexicales. Nous discutons également de la pertinence de l'utilisation de méthodes d'apprentissage explicative pour acquérir des informations en corpus."
  },
  {
    "title": "Nouvelles contributions du boosting en apprentissage automatique",
    "authors": [
      "Henri-Maxime Suchier"
    ],
    "year": 2006,
    "abstract": "L'apprentissage automatique vise la production d'une hypothèse modélisant un concept à partir d'exemples, dans le but notamment de prédire si de nouvelles observations relèvent ou non de ce concept. Parmi les algorithmes d'apprentissage, les méthodes ensemblistes combinent des hypothèses de base (dites ``faibles'') en une hypothèse globale plus performante. Le boosting, et son algorithme AdaBoost, est une méthode ensembliste très étudiée depuis plusieurs années : ses performances expérimentales remarquables reposent sur des fondements théoriques rigoureux. Il construit de manière adaptative et itérative des hypothèses de base en focalisant l'apprentissage, à chaque nouvelle itération, sur les exemples qui ont été difficiles à apprendre lors des itérations précédentes. Cependant, AdaBoost est relativement inadapté aux données du monde réel. Dans cette thèse, nous nous concentrons en particulier sur les données bruitées, et sur les données hétérogènes. Dans le cas des données bruitées, non seulement la méthode peut devenir très lente, mais surtout, AdaBoost apprend par coeur les données, et le pouvoir prédictif des hypothèses globales générées, s'en trouve extrêmement dégradé. Nous nous sommes donc intéressés à une adaptation du boosting pour traiter les données bruitées. Notre solution exploite l'information provenant d'un oracle de confiance permettant d'annihiler les effets dramatiques du bruit. Nous montrons que notre nouvel algorithme conserve les propriétés théoriques du boosting standard. Nous mettons en pratique cette nouvelle méthode, d'une part sur des données numériques, et d'autre part, de manière plus originale, sur des données textuelles. Dans le cas des données hétérogènes, aucune adaptation du boosting n'a été proposée jusqu'à présent. Pourtant, ces données, caractérisées par des attributs multiples mais de natures différentes (comme des images, du son, du texte, etc), sont extrêmement fréquentes sur le web, par exemple. Nous avons donc développé un nouvel algorithme de boosting permettant de les utiliser. Plutôt que de combiner des hypothèses boostées indépendamment, nous construisons un nouveau schéma de boosting permettant de faire collaborer durant l'apprentissage des algorithmes spécialisés sur chaque type d'attribut. Nous prouvons que les décroissances exponentielles des erreurs sont toujours assurées par ce nouveau modèle, aussi bien d'un point de vue théorique qu'expérimental.",
    "uri": "https://theses.hal.science/tel-00379539v1",
    "texte_nettoye": "L'apprentissage automatique vise la production d'une hypothèse modélisant un concept à partir d'exemples, dans le but notamment de prédire si de nouvelles observations relèvent ou non de ce concept. Parmi les algorithmes d'apprentissage, les méthodes ensemblistes combinent des hypothèses de base (dites ``faibles'') en une hypothèse globale plus performante. Le boosting, et son algorithme AdaBoost, est une méthode ensembliste très étudiée depuis plusieurs années : ses performances expérimentales remarquables reposent sur des fondements théoriques rigoureux. Il construit de manière adaptative et itérative des hypothèses de base en focalisant l'apprentissage, à chaque nouvelle itération, sur les exemples qui ont été difficiles à apprendre lors des itérations précédentes. Cependant, AdaBoost est relativement inadapté aux données du monde réel. Dans cette thèse, nous nous concentrons en particulier sur les données bruitées, et sur les données hétérogènes. Dans le cas des données bruitées, non seulement la méthode peut devenir très lente, mais surtout, AdaBoost apprend par coeur les données, et le pouvoir prédictif des hypothèses globales générées, s'en trouve extrêmement dégradé. Nous nous sommes donc intéressés à une adaptation du boosting pour traiter les données bruitées. Notre solution exploite l'information provenant d'un oracle de confiance permettant d'annihiler les effets dramatiques du bruit. Nous montrons que notre nouvel algorithme conserve les propriétés théoriques du boosting standard. Nous mettons en pratique cette nouvelle méthode, d'une part sur des données numériques, et d'autre part, de manière plus originale, sur des données textuelles. Dans le cas des données hétérogènes, aucune adaptation du boosting n'a été proposée jusqu'à présent. Pourtant, ces données, caractérisées par des attributs multiples mais de natures différentes (comme des images, du son, du texte, etc), sont extrêmement fréquentes sur le web, par exemple. Nous avons donc développé un nouvel algorithme de boosting permettant de les utiliser. Plutôt que de combiner des hypothèses boostées indépendamment, nous construisons un nouveau schéma de boosting permettant de faire collaborer durant l'apprentissage des algorithmes spécialisés sur chaque type d'attribut. Nous prouvons que les décroissances exponentielles des erreurs sont toujours assurées par ce nouveau modèle, aussi bien d'un point de vue théorique qu'expérimental."
  },
  {
    "title": "Application d’algorithmes de machine learning pour l’exploitation de données omiques en oncologie",
    "authors": [
      "Jocelyn Gal"
    ],
    "year": 2019,
    "abstract": "Le développement de l’informatique en médecine et en biologie a permis de générer un grand volume de données. La complexité et la quantité d’informations à intégrer lors d’une prise de décision médicale ont largement dépassé les capacités humaines. Ces informations comprennent des variables démographiques, cliniques ou radiologiques mais également des variables biologiques et en particulier omiques (génomique, protéomique, transcriptomique et métabolomique) caractérisées par un grand nombre de variables mesurées relativement au faible nombre de patients. Leur analyse représente un véritable défi dans la mesure où elles sont fréquemment « bruitées » et associées à des situations de multi-colinéarité. De nos jours, la puissance de calcul permet d'identifier des modèles cliniquement pertinents parmi cet ensemble de données en utilisant des algorithmes d’apprentissage automatique. A travers cette thèse, notre objectif est d’appliquer des méthodes d’apprentissage supervisé et non supervisé, à des données biologiques de grande dimension, dans le but de participer à l’optimisation de la classification et de la prise en charge thérapeutique des patients atteints de cancers. La première partie de ce travail consiste à appliquer une méthode d’apprentissage supervisé à des données d’immunogénétique germinale pour prédire l’efficacité thérapeutique et la toxicité d’un traitement par inhibiteur de point de contrôle immunitaire. La deuxième partie compare différentes méthodes d’apprentissage non supervisé permettant d’évaluer l’apport de la métabolomique dans le diagnostic et la prise en charge des cancers du sein en situation adjuvante. Enfin la troisième partie de ce travail a pour but d’exposer l’apport que peuvent présenter les essais thérapeutiques simulés en recherche biomédicale. L’application des méthodes d’apprentissage automatique en oncologie offre de nouvelles perspectives aux cliniciens leur permettant ainsi de poser des diagnostics plus rapidement et plus précisément, ou encore d’optimiser la prise en charge thérapeutique en termes d’efficacité et de toxicité.",
    "uri": "https://theses.hal.science/tel-03917512v1",
    "texte_nettoye": "Le développement de l’informatique en médecine et en biologie a permis de générer un grand volume de données. La complexité et la quantité d’informations à intégrer lors d’une prise de décision médicale ont largement dépassé les capacités humaines. Ces informations comprennent des variables démographiques, cliniques ou radiologiques mais également des variables biologiques et en particulier omiques (génomique, protéomique, transcriptomique et métabolomique) caractérisées par un grand nombre de variables mesurées relativement au faible nombre de patients. Leur analyse représente un véritable défi dans la mesure où elles sont fréquemment « bruitées » et associées à des situations de multi-colinéarité. De nos jours, la puissance de calcul permet d'identifier des modèles cliniquement pertinents parmi cet ensemble de données en utilisant des algorithmes d’apprentissage automatique. A travers cette thèse, notre objectif est d’appliquer des méthodes d’apprentissage supervisé et non supervisé, à des données biologiques de grande dimension, dans le but de participer à l’optimisation de la classification et de la prise en charge thérapeutique des patients atteints de cancers. La première partie de ce travail consiste à appliquer une méthode d’apprentissage supervisé à des données d’immunogénétique germinale pour prédire l’efficacité thérapeutique et la toxicité d’un traitement par inhibiteur de point de contrôle immunitaire. La deuxième partie compare différentes méthodes d’apprentissage non supervisé permettant d’évaluer l’apport de la métabolomique dans le diagnostic et la prise en charge des cancers du sein en situation adjuvante. Enfin la troisième partie de ce travail a pour but d’exposer l’apport que peuvent présenter les essais thérapeutiques simulés en recherche biomédicale. L’application des méthodes d’apprentissage automatique en oncologie offre de nouvelles perspectives aux cliniciens leur permettant ainsi de poser des diagnostics plus rapidement et plus précisément, ou encore d’optimiser la prise en charge thérapeutique en termes d’efficacité et de toxicité."
  },
  {
    "title": "Méta-apprentissage pour l'analyse AMR translingue",
    "authors": [
      "Jeongwoo Kang",
      "Maximin Coavoux",
      "Cédric Lopez",
      "Didier Schwab"
    ],
    "year": 2024,
    "abstract": "L’analyse AMR multilingue consiste à prédire des analyses sémantiques AMR dans une langue cible lorsque les données d’entraînement ne sont disponibles que dans une langue source. Cette tâche n’a été étudiée que pour un petit nombre de langues en raison du manque de données multilingues. En s’inspirant de Langedijk et al. (2022), qui appliquent le méta-apprentissage à l’analyse syntaxique en dépendances translingue, nous étudions le méta-apprentissage pour l’analyse AMR translingue. Nous évaluons nos modèles dans des scénarios zero-shot et few-shot en croate, en farsi, en coréen, en chinois et en français. En particulier, nous développons dans le cadre de cet article des données d’évaluation en coréen et en croate, à partir du corpus AMR anglais Le Petit Prince. Nous étudions empiriquement cette approche en la comparant à une méthode classique d’apprentissage conjoint.",
    "uri": "https://inria.hal.science/hal-04623014v1",
    "texte_nettoye": "L’analyse AMR multilingue consiste à prédire des analyses sémantiques AMR dans une langue cible lorsque les données d’entraînement ne sont disponibles que dans une langue source. Cette tâche n’a été étudiée que pour un petit nombre de langues en raison du manque de données multilingues. En s’inspirant de Langedijk et al. (2022), qui appliquent le méta-apprentissage à l’analyse syntaxique en dépendances translingue, nous étudions le méta-apprentissage pour l’analyse AMR translingue. Nous évaluons nos modèles dans des scénarios zero-shot et few-shot en croate, en farsi, en coréen, en chinois et en français. En particulier, nous développons dans le cadre de cet article des données d’évaluation en coréen et en croate, à partir du corpus AMR anglais Le Petit Prince. Nous étudions empiriquement cette approche en la comparant à une méthode classique d’apprentissage conjoint."
  },
  {
    "title": "Optimisation de contrôleurs par essaim particulaire",
    "authors": [
      "Jérémy Fix",
      "Matthieu Geist"
    ],
    "year": 2012,
    "abstract": "Trouver des contrôleurs optimaux pour des systèmes stochastiques est un problème particulièrement difficile abordé dans les communautés d'apprentissage par renforcement et de contrôle optimal. Le paradigme classique employé pour résoudre ces problèmes est celui des processus décisionnel de Markov. Néanmoins, le problème d'optimisation qui en découle peut être difficile à résoudre. Dans ce papier, nous explorons l'utilisation de l'optimisation par essaim particulaire pour apprendre des contrôleurs optimaux. Nous l'appliquons en particulier à trois problèmes classiques : le pendule inversé, le mountain car et le double pendule.",
    "uri": "https://centralesupelec.hal.science/hal-00701945v1",
    "texte_nettoye": "Trouver des contrôleurs optimaux pour des systèmes stochastiques est un problème particulièrement difficile abordé dans les communautés d'apprentissage par renforcement et de contrôle optimal. Le paradigme classique employé pour résoudre ces problèmes est celui des processus décisionnel de Markov. Néanmoins, le problème d'optimisation qui en découle peut être difficile à résoudre. Dans ce papier, nous explorons l'utilisation de l'optimisation par essaim particulaire pour apprendre des contrôleurs optimaux. Nous l'appliquons en particulier à trois problèmes classiques : le pendule inversé, le mountain car et le double pendule."
  },
  {
    "title": "Livre Libre, moteur de recherche spécialisé",
    "authors": [
      "Isabelle Audras",
      "Jean-Gabriel Ganascia"
    ],
    "year": 2004,
    "abstract": "",
    "uri": "https://hal.science/hal-01520492v1",
    "texte_nettoye": ""
  },
  {
    "title": "Apprentissage automatique d'une distance d'édition dédiée à la reconnaissance de l'écriture manuscrite",
    "authors": [
      "Sabine Carbonnel",
      "Eric Anquetil"
    ],
    "year": 2004,
    "abstract": "Ce travail s'inscrit dans le cadre d'un posttraitement lexical basé à la fois sur une organisation structur ée de dictionnaire (à partir de caractéristiques globales des mots) et sur un algorithme d'appariement de mots spécifique à l'écriture manuscrite en-ligne (distance d'édition). L'objectif est de compenser les erreurs de reconnaissance et de segmentation en s'appuyant sur les informations lexicales extraites d'un dictionnaire. Pour cela nous avons adapté une distance d'édition à l'écriture manuscrite pour le post-traitement lexical en reconnaissance en-ligne de mots. Cet article présente une méthode d'apprentissage automatique de la distance d'édition. L'objectif est de pouvoir automatiquement spécialiser la distance d'édition en fonction des propriétés du reconnaisseur. Les résultats expérimentaux obtenus montrent que l'approche proposée pour adapter automatiquement la distance d'édition obtient d'aussi bons résultats qu'une distance d'édition calibrée empiriquement à la main de fac¸on longue et fastidieuse.",
    "uri": "https://archivesic.ccsd.cnrs.fr/sic_00001198v1",
    "texte_nettoye": "Ce travail s'inscrit dans le cadre d'un posttraitement lexical basé à la fois sur une organisation structur ée de dictionnaire (à partir de caractéristiques globales des mots) et sur un algorithme d'appariement de mots spécifique à l'écriture manuscrite en-ligne (distance d'édition). L'objectif est de compenser les erreurs de reconnaissance et de segmentation en s'appuyant sur les informations lexicales extraites d'un dictionnaire. Pour cela nous avons adapté une distance d'édition à l'écriture manuscrite pour le post-traitement lexical en reconnaissance en-ligne de mots. Cet article présente une méthode d'apprentissage automatique de la distance d'édition. L'objectif est de pouvoir automatiquement spécialiser la distance d'édition en fonction des propriétés du reconnaisseur. Les résultats expérimentaux obtenus montrent que l'approche proposée pour adapter automatiquement la distance d'édition obtient d'aussi bons résultats qu'une distance d'édition calibrée empiriquement à la main de fac¸on longue et fastidieuse."
  },
  {
    "title": "Recherche d'architecture de réseaux de neurones pour la classification extrême et dans un contexte d'apprentissage partiellement étiqueté",
    "authors": [
      "Loïc Pauletto"
    ],
    "year": 2022,
    "abstract": "Les applications d'apprentissage profond se développent rapidement et ne montrent aucun signe de ralentissement. Les topologies des réseaux neuronaux deviennent de plus en plus grandes et complexes pour résoudre les problèmes de la vie réelle.Cette complexité accrue nécessite plus de temps et d'expertise de la part des professionnels, ainsi qu'un investissement financier important pour les entreprises d'IA.La recherche d'architecture neuronale (RAN) est un nouveau paradigme d'apprentissage automatique qui cherche à déterminer la meilleure architecture de réseau neuronal pour un problème donné. Les techniques de RNA, d'autre part, n'ont été étudiées et développées que dans des problèmes d'apprentissage automatique limités et bien définis, qui ne sont pas représentatifs de tous les scénarios d'apprentissage automatique existants.Cette thèse se concentre sur la recherche et le développement des approches RAN pour de nouvelles tâches ainsi que sur un nouveau cadre d'apprentissage qui est plus pertinent pour les applications du monde réel.Nous avons proposé d'utiliser un cadre RAN neuro-évolutif pour résoudre le défi extrême de la classification multi-label en particulier.Nous avons combiné des réseaux de convolution et récurrents pour fournir une recherche spatiale plus appropriée à cette tâche.Sur plusieurs jeux de données, nous évaluons la performance du réseau recherché. Nous avons également examiné le défi de la reconstruction d'une carte RSSI, qui est un processus plus difficile en raison du manque de données d'entrée(c'est-à-dire données partiellement annotées). De cette façon, nous proposons un système de recherche d'architecture dynamique pour les tâches de segmentation sémantique avec un nombre minimal d'échantillons annotés. Nous avons étudié plusieurs algorithmes d'apprentissage semi-supervisé dans ce cadre afin de déterminer celui qui réussit le mieux à utiliser des échantillons non étiquetés.Nous avons examiné un certain nombre de stratégies, y compris des approches de semi-supervision \"traditionnelles\" et \"nouvelles\", ainsi que des approches d'auto-supervision.",
    "uri": "https://theses.hal.science/tel-03921641v1",
    "texte_nettoye": "Les applications d'apprentissage profond se développent rapidement et ne montrent aucun signe de ralentissement. Les topologies des réseaux neuronaux deviennent de plus en plus grandes et complexes pour résoudre les problèmes de la vie réelle.Cette complexité accrue nécessite plus de temps et d'expertise de la part des professionnels, ainsi qu'un investissement financier important pour les entreprises d'IA.La recherche d'architecture neuronale (RAN) est un nouveau paradigme d'apprentissage automatique qui cherche à déterminer la meilleure architecture de réseau neuronal pour un problème donné. Les techniques de RNA, d'autre part, n'ont été étudiées et développées que dans des problèmes d'apprentissage automatique limités et bien définis, qui ne sont pas représentatifs de tous les scénarios d'apprentissage automatique existants.Cette thèse se concentre sur la recherche et le développement des approches RAN pour de nouvelles tâches ainsi que sur un nouveau cadre d'apprentissage qui est plus pertinent pour les applications du monde réel.Nous avons proposé d'utiliser un cadre RAN neuro-évolutif pour résoudre le défi extrême de la classification multi-label en particulier.Nous avons combiné des réseaux de convolution et récurrents pour fournir une recherche spatiale plus appropriée à cette tâche.Sur plusieurs jeux de données, nous évaluons la performance du réseau recherché. Nous avons également examiné le défi de la reconstruction d'une carte RSSI, qui est un processus plus difficile en raison du manque de données d'entrée(c'est-à-dire données partiellement annotées). De cette façon, nous proposons un système de recherche d'architecture dynamique pour les tâches de segmentation sémantique avec un nombre minimal d'échantillons annotés. Nous avons étudié plusieurs algorithmes d'apprentissage semi-supervisé dans ce cadre afin de déterminer celui qui réussit le mieux à utiliser des échantillons non étiquetés.Nous avons examiné un certain nombre de stratégies, y compris des approches de semi-supervision \"traditionnelles\" et \"nouvelles\", ainsi que des approches d'auto-supervision."
  },
  {
    "title": "Veille d'Information sur le Web avec Re-Watch",
    "authors": [
      "Christophe Brouard",
      "Christian Pomot"
    ],
    "year": 2017,
    "abstract": "Les algorithmes d’apprentissage automatique peuvent être utilisés pour créer des outils de recommandation qui permettent de prédire la pertinence d’un document pour une thématique de veille donnée en se basant sur les précédents jugements de pertinence donnés pour cette thématique pour d’autres documents. Ces outils de recommandation permettent de filtrer dans un flux entrant de documents ceux qui sont susceptibles d’être pertinents sans que l’utilisateur ait besoin de déterminer lui-même les mots clefs marquant l’adéquation d’un document pour un sujet de la veille. Bien que cette problématique de recherche ait été abondamment abordée, les outils de veille d’information pour le web intégrant un apprentissage en sont encore à leur balbutiements. Nous présentons ici l’application web Re-Watch permettant la définition d’un thème de veille, la sélection de sources d’information sur le web relatives à ce thème et l’adaptation des scores de pertinence des documents aux retours de l’utilisateur. L’application permet aussi, pour chaque thème, une auto-évaluation de la qualité du filtrage et une interrogation du moteur de recherche Google. Cette application encore en cours de développement est néanmoins actuellement fonctionnelle et accessible sur le web à l’url suivante : http://www.specific search.com.",
    "uri": "https://hal.science/hal-01657537v1",
    "texte_nettoye": "Les algorithmes d’apprentissage automatique peuvent être utilisés pour créer des outils de recommandation qui permettent de prédire la pertinence d’un document pour une thématique de veille donnée en se basant sur les précédents jugements de pertinence donnés pour cette thématique pour d’autres documents. Ces outils de recommandation permettent de filtrer dans un flux entrant de documents ceux qui sont susceptibles d’être pertinents sans que l’utilisateur ait besoin de déterminer lui-même les mots clefs marquant l’adéquation d’un document pour un sujet de la veille. Bien que cette problématique de recherche ait été abondamment abordée, les outils de veille d’information pour le web intégrant un apprentissage en sont encore à leur balbutiements. Nous présentons ici l’application web Re-Watch permettant la définition d’un thème de veille, la sélection de sources d’information sur le web relatives à ce thème et l’adaptation des scores de pertinence des documents aux retours de l’utilisateur. L’application permet aussi, pour chaque thème, une auto-évaluation de la qualité du filtrage et une interrogation du moteur de recherche Google. Cette application encore en cours de développement est néanmoins actuellement fonctionnelle et accessible sur le web à l’url suivante : http://www.specific search.com."
  },
  {
    "title": "Modélisation stochastique d'une population de neurones, méta-apprentissage dans un problème de classification",
    "authors": [
      "Bruno Scherrer",
      "Frédéric Alexandre",
      "François Charpillet",
      "Stéphane Vialle"
    ],
    "year": 2000,
    "abstract": "La modélisation stochastique d'une population de neurones peut s'inspirer de l'étude générale de populations d'entités en interactions. l'étude stochastique de telles populations d'agents a été abordée par une famille de méthodes dérivées de la recherche opérationnelle et de récents travaux concernant l'apprentissage par renforcement dans de tels systèmes peuvent servir d'outil pour la modélisation de neurones en interaction. Nous présentons un cadre générique d'étude stochastique d'agents partageant un environnement et une méthode d'apprentissage par renforcement qui lui est associée. Nous exposons ensuite l'état actuel de nos recherches concernant des possibles adaptations au paradigme neuronal pour un problème de classification.",
    "uri": "https://inria.hal.science/inria-00099114v1",
    "texte_nettoye": "La modélisation stochastique d'une population de neurones peut s'inspirer de l'étude générale de populations d'entités en interactions. l'étude stochastique de telles populations d'agents a été abordée par une famille de méthodes dérivées de la recherche opérationnelle et de récents travaux concernant l'apprentissage par renforcement dans de tels systèmes peuvent servir d'outil pour la modélisation de neurones en interaction. Nous présentons un cadre générique d'étude stochastique d'agents partageant un environnement et une méthode d'apprentissage par renforcement qui lui est associée. Nous exposons ensuite l'état actuel de nos recherches concernant des possibles adaptations au paradigme neuronal pour un problème de classification."
  },
  {
    "title": "Constitution de Corpus d'Apprentissage et étiquetage simple sur des blogues interculturels",
    "authors": [
      "Mario Laurent",
      "Thierry Chanier"
    ],
    "year": 2012,
    "abstract": "Au cours de nos travaux de recherche en EIAH (Environnements informatiques pour l'apprentissage humain), nous avons besoin d'analyser des formations en ligne afin de répondre à plusieurs questions : la formation a-t-elle été efficace, les apprenants ont-ils interagi et comment l'ont-ils fait ? Pour ce faire, nous recueillons les traces des interactions, l'ensemble des ressources et les métadonnées associées à une formation donnée. Ensuite, nous les organisons, de façon systématique au sein de structures appelées corpus d'apprentissage ou LETEC (LEarning and TEaching Corpus). Ces LETEC sont stockés dans la banque de corpus en ligne Mulce (MUltimodal contextualized Learner Corpus Exchange). Nous souhaitons mettre en évidence l'importance de cette structuration et les possibilités d'analyses qui en découlent notamment en utilisant des stratégies de TAL (Traitement Automatique du Langage). Nous prenons ici l'exemple d'un étiquetage automatique simple des messages issus des blogues de la formation Infral qui est le sujet d'un des LETEC de la banque de corpus Mulce.",
    "uri": "https://edutice.hal.science/edutice-00693470v2",
    "texte_nettoye": "Au cours de nos travaux de recherche en EIAH (Environnements informatiques pour l'apprentissage humain), nous avons besoin d'analyser des formations en ligne afin de répondre à plusieurs questions : la formation a-t-elle été efficace, les apprenants ont-ils interagi et comment l'ont-ils fait ? Pour ce faire, nous recueillons les traces des interactions, l'ensemble des ressources et les métadonnées associées à une formation donnée. Ensuite, nous les organisons, de façon systématique au sein de structures appelées corpus d'apprentissage ou LETEC (LEarning and TEaching Corpus). Ces LETEC sont stockés dans la banque de corpus en ligne Mulce (MUltimodal contextualized Learner Corpus Exchange). Nous souhaitons mettre en évidence l'importance de cette structuration et les possibilités d'analyses qui en découlent notamment en utilisant des stratégies de TAL (Traitement Automatique du Langage). Nous prenons ici l'exemple d'un étiquetage automatique simple des messages issus des blogues de la formation Infral qui est le sujet d'un des LETEC de la banque de corpus Mulce."
  },
  {
    "title": "Apprentissage croisé en reconnaissance analytique de l'écriture manuscrite",
    "authors": [
      "Christophe Choisy",
      "Abdel Belaïd"
    ],
    "year": 2000,
    "abstract": "Ce papier présente une méthode d'apprentissage croisé de lettres dans le cadre d'une approche analytique de reconnaissance de mots. Les mots sont représentés par des hmms où chaque état représente une lettre. Chaque lettre est décrite par un champ de Markov causal. La génération des modèles de mots est obtenue par fusion des modèles de lettres. La réestimation des paramètres de ces modèles, objet de cet article, est réalisée au travers de l'apprentissage des modèles de lettres et des transitions des hmms représentant les mots. l'apprentissage des lettres se fait par croisement des paramètres extraits des différents modèles des mots les contenant, de manière à exploiter tous les contextes d'écriture possibles de ces lettres. l'utilisation de l'algorithme de Baum-Welch permet une réestimation optimale de ces paramètres tout en supprimant la nécessité d'une segmentation. Les transitions des hmms représentant les mots sont également réestimées à l'aide des informations des modèles générés. Les premiers tests, effectués sur 7031 mots de montants de chèques fournis par la SRTP (2/3 en apprentissage, 1/3 en reconnaissance), donnent un taux de reconnaissance de 83.4%.",
    "uri": "https://inria.hal.science/inria-00099039v1",
    "texte_nettoye": "Ce papier présente une méthode d'apprentissage croisé de lettres dans le cadre d'une approche analytique de reconnaissance de mots. Les mots sont représentés par des hmms où chaque état représente une lettre. Chaque lettre est décrite par un champ de Markov causal. La génération des modèles de mots est obtenue par fusion des modèles de lettres. La réestimation des paramètres de ces modèles, objet de cet article, est réalisée au travers de l'apprentissage des modèles de lettres et des transitions des hmms représentant les mots. l'apprentissage des lettres se fait par croisement des paramètres extraits des différents modèles des mots les contenant, de manière à exploiter tous les contextes d'écriture possibles de ces lettres. l'utilisation de l'algorithme de Baum-Welch permet une réestimation optimale de ces paramètres tout en supprimant la nécessité d'une segmentation. Les transitions des hmms représentant les mots sont également réestimées à l'aide des informations des modèles générés. Les premiers tests, effectués sur 7031 mots de montants de chèques fournis par la SRTP (2/3 en apprentissage, 1/3 en reconnaissance), donnent un taux de reconnaissance de 83.4%."
  },
  {
    "title": "Apprentissage des Mouvements Humain en Situation Informatisée: Avantages, Limites et Perspectives Apportées par l'Approche Automatique",
    "authors": [
      "Quentin Couland",
      "Ludovic Hamon",
      "Sébastien George"
    ],
    "year": 2017,
    "abstract": "Les applications utilisant les mouvements des utilisateurs afin de renforcer la construction des connaissances et d'améliorer l'immersion, sont de plus en plus utilisées dans des domaines tels que le sport, la chirurgie ou l'éducation. Actuellement, les informations sur les actions, intentions, situations et comportements, sont rarement extraites à partir de données de mouvement 3D. Cela peut être expliqué par l'hétérogénéité, la complexité et la grande dimensionnalité de ces informations d’une part et leurs corrélations avec les besoins d’observation des enseignants d’autre part. Cependant, les méthodes de traitement automatique (machine learning) pourraient être utilisées afin de surmonter ces contraintes. Une meilleure analyse de ces données pourrait être obtenue pour mieux modéliser le comportement de l’apprenant lors de la situation d’apprentissage. Cet article introduit les défis propres à ces problèmes et propose un système d'aide à l'analyse des mouvements humains.",
    "uri": "https://hal.science/hal-01713082v1",
    "texte_nettoye": "Les applications utilisant les mouvements des utilisateurs afin de renforcer la construction des connaissances et d'améliorer l'immersion, sont de plus en plus utilisées dans des domaines tels que le sport, la chirurgie ou l'éducation. Actuellement, les informations sur les actions, intentions, situations et comportements, sont rarement extraites à partir de données de mouvement 3D. Cela peut être expliqué par l'hétérogénéité, la complexité et la grande dimensionnalité de ces informations d’une part et leurs corrélations avec les besoins d’observation des enseignants d’autre part. Cependant, les méthodes de traitement automatique (machine learning) pourraient être utilisées afin de surmonter ces contraintes. Une meilleure analyse de ces données pourrait être obtenue pour mieux modéliser le comportement de l’apprenant lors de la situation d’apprentissage. Cet article introduit les défis propres à ces problèmes et propose un système d'aide à l'analyse des mouvements humains."
  },
  {
    "title": "Actes des 21es Rencontres des Jeunes Chercheurs en Intelligence Artificielle",
    "authors": [
      "Brian Ravenet"
    ],
    "year": 2023,
    "abstract": "",
    "uri": "https://ut3-toulouseinp.hal.science/hal-04565426v1",
    "texte_nettoye": ""
  },
  {
    "title": "Inférence grammaticale régulière : fondements théoriques et principaux algorithmes",
    "authors": [
      "Pierre Dupont",
      "Laurent Miclet"
    ],
    "year": 1998,
    "abstract": "L'objet de cette étude est l'apprentissage automatique d'une grammaire formelle à partir d'exemples. En particulier, nous nous concentrons sur l'inférence de grammaires régulières. Nous proposons une étude détaillée des bases théoriques et de l'espace de recherche de ce problème. Nous y démontrons des propriétés originales définissant un cadre formel et des contraintes pour la consception de nouveaux algorithmes. Ensuite, nous présentons un éventail d'algorithmes d'inférence en comparant la nature des données qu'ils utilisent, leur critère de généralisation et leur complexité calculatoire. Finalement, nous étudions l'inférence régulière sur base d'une présentation séquentielle des données d'apprentissage. Une extension incrémentale d'un algorithme existant est développée. Nous en démontrons la convergence et la complexité théorique.",
    "uri": "https://inria.hal.science/inria-00073241v1",
    "texte_nettoye": "L'objet de cette étude est l'apprentissage automatique d'une grammaire formelle à partir d'exemples. En particulier, nous nous concentrons sur l'inférence de grammaires régulières. Nous proposons une étude détaillée des bases théoriques et de l'espace de recherche de ce problème. Nous y démontrons des propriétés originales définissant un cadre formel et des contraintes pour la consception de nouveaux algorithmes. Ensuite, nous présentons un éventail d'algorithmes d'inférence en comparant la nature des données qu'ils utilisent, leur critère de généralisation et leur complexité calculatoire. Finalement, nous étudions l'inférence régulière sur base d'une présentation séquentielle des données d'apprentissage. Une extension incrémentale d'un algorithme existant est développée. Nous en démontrons la convergence et la complexité théorique."
  },
  {
    "title": "Problème d'affectation dynamique des emplacements de stockage chez Knapp : vers de l'apprentissage automatique ?",
    "authors": [
      "Paul Courtin",
      "Axel Grimault",
      "Mehdi Lhommeau",
      "Jean-Baptiste Fasquel"
    ],
    "year": 2020,
    "abstract": "L’optimisation des performances de préparation des en-trepôts automatisés passe par une affection judicieuse desproduits aux emplacements de stockage. Ce problème d’af-fectation des positions de stockage (SLAP), est générale-ment abordé par les méthodes relevant de la rechercheopérationnelle. Cet article dresse un état de l’art de cetteproblématique en soulignant les liens possibles avec l’ap-prentissage automatique, et des perspectives envisageablesde résolution combinant apprentissage automatique et re-cherche opérationnelle.",
    "uri": "https://univ-angers.hal.science/hal-02527743v6",
    "texte_nettoye": "L’optimisation des performances de préparation des en-trepôts automatisés passe par une affection judicieuse desproduits aux emplacements de stockage. Ce problème d’af-fectation des positions de stockage (SLAP), est générale-ment abordé par les méthodes relevant de la rechercheopérationnelle. Cet article dresse un état de l’art de cetteproblématique en soulignant les liens possibles avec l’ap-prentissage automatique, et des perspectives envisageablesde résolution combinant apprentissage automatique et re-cherche opérationnelle."
  },
  {
    "title": "Apprentissage par renforcement de modeles de contexte pour l'informatique ambiante",
    "authors": [
      "Sofia Zaidenberg"
    ],
    "year": 2009,
    "abstract": "Cette thèse étudie l'acquisition automatique par apprentissage d'un modèle de contexte pour un utilisateur dans un environnement ubiquitaire. Dans un tel environnement, les dispositifs peuvent communiquer et coopérer afin de former un espace informatique cohérent. Certains appareils ont des capacités de perception, utilisées par l'environnement pour détecter la situation - le contexte - de l'utilisateur. D'autres appareils sont capables d'exécuter des actions. La problématique que nous nous sommes posée est de déterminer les associations optimales pour un utilisateur donné entre les situations et les actions. L'apprentissage apparaît comme une bonne approche car il permet de personnaliser l'environnement sans spécification explicite de la part de l'usager. Un apprentissage à vie permet, par ailleurs, de toujours s'adapter aux modifications du monde et des préférences utilisateur. L'apprentissage par renforcement est un paradigme d'apprentissage qui peut être une solution à notre problème, à condition de l'adapter aux contraintes liées à notre cadre d'application.",
    "uri": "https://theses.hal.science/tel-00497656v1",
    "texte_nettoye": "Cette thèse étudie l'acquisition automatique par apprentissage d'un modèle de contexte pour un utilisateur dans un environnement ubiquitaire. Dans un tel environnement, les dispositifs peuvent communiquer et coopérer afin de former un espace informatique cohérent. Certains appareils ont des capacités de perception, utilisées par l'environnement pour détecter la situation - le contexte - de l'utilisateur. D'autres appareils sont capables d'exécuter des actions. La problématique que nous nous sommes posée est de déterminer les associations optimales pour un utilisateur donné entre les situations et les actions. L'apprentissage apparaît comme une bonne approche car il permet de personnaliser l'environnement sans spécification explicite de la part de l'usager. Un apprentissage à vie permet, par ailleurs, de toujours s'adapter aux modifications du monde et des préférences utilisateur. L'apprentissage par renforcement est un paradigme d'apprentissage qui peut être une solution à notre problème, à condition de l'adapter aux contraintes liées à notre cadre d'application."
  },
  {
    "title": "Analyse automatique d’arguments et apprentissage multi-tâches : un cas d’étude",
    "authors": [
      "Jean-Christophe Mensonides",
      "Sébastien Harispe",
      "Jacky Montmain",
      "Véronique Thireau"
    ],
    "year": 2022,
    "abstract": "Nous proposons une étude sur l’analyse automatique d’arguments via des techniques d’apprentissage supervisé exploitant le paradigme de l’apprentissage multi-tâches. Nous définissons pour cela une approche multi-tâches à base d’apprentissage profond que nous évaluons sur un cas d’étude spécifique portant sur l’extraction d’arguments dans un corpus de dissertations. Les résultats obtenus permettent de discuter l’intérêt de définir un modèle multi-tâches unique – optimisé sur différents critères en tirant parti de la diversité des tâches d’apprentissage auxquelles il est confronté – par rapport à un ensemble de classifieurs entraînés de manière indépendante et spécifique. Nous montrons en particulier l’impact de l’ajout de tâches auxiliaires de bas niveau, telles que l’étiquetage morpho-syntaxique et l’analyse de dépendances grammaticales, pour l’obtention de classifieurs multi-tâches performants. Nous observons aussi que l’apprentissage multi-tâches permet l’obtention de modèles efficaces de performances semblables à l’état de l’art pour le cas d’étude traité.",
    "uri": "https://imt-mines-ales.hal.science/hal-03638222v1",
    "texte_nettoye": "Nous proposons une étude sur l’analyse automatique d’arguments via des techniques d’apprentissage supervisé exploitant le paradigme de l’apprentissage multi-tâches. Nous définissons pour cela une approche multi-tâches à base d’apprentissage profond que nous évaluons sur un cas d’étude spécifique portant sur l’extraction d’arguments dans un corpus de dissertations. Les résultats obtenus permettent de discuter l’intérêt de définir un modèle multi-tâches unique – optimisé sur différents critères en tirant parti de la diversité des tâches d’apprentissage auxquelles il est confronté – par rapport à un ensemble de classifieurs entraînés de manière indépendante et spécifique. Nous montrons en particulier l’impact de l’ajout de tâches auxiliaires de bas niveau, telles que l’étiquetage morpho-syntaxique et l’analyse de dépendances grammaticales, pour l’obtention de classifieurs multi-tâches performants. Nous observons aussi que l’apprentissage multi-tâches permet l’obtention de modèles efficaces de performances semblables à l’état de l’art pour le cas d’étude traité."
  },
  {
    "title": "Apprentissage de Fonctions de Classification et d'Ordonnacement avec des Données Partiellement Etiquetées",
    "authors": [
      "Massih-Reza Amini"
    ],
    "year": 2007,
    "abstract": "Avec le développement des technologies d'information on assiste depuis quelques années à une nouvelle impulsion pour la conception de nouveaux cadres d'apprentissage automatique. C'est le cas par exemple du paradigme semi-supervisé qui a vu le jour vers la fin des années 90 dans la communauté apprentissage. Les premiers travaux dans ce cadre ont été motivés par le développement du \\textit{web} qui a entraîné une production massive de données textuelles très hétérogènes. Ces masses de données sont généralement livrées sous forme brute, sans étiquetage a priori et pour les exploiter on était alors réduit à utiliser des techniques non-supervisées. Ces approches bien que totalement génériques ne permettent cependant qu'une analyse limitée des informations de contenu et ne répondent pas ainsi aux demandes de nombreuses tâches de Recherche d'Information (RI). L'idée pragmatique développée pour l'apprentissage \\textit{semi-supervisé} était née de la question; \"comment réduire l'effort d'étiquetage et utiliser simultanément une petite quantité de données étiquetée avec la masse de données non-étiquetées pour apprendre?\" Un autre exemple de l'émergence de nouveaux cadres d'apprentissage concerne le développement de méthodes automatiques pour la recherche et l'ordonnancement d'entités d'information sur des corpus de grandes tailles. Récemment beaucoup de travaux se sont intéressés à la formulation des différentes formes de la tâche d'ordonnancement. Ces travaux ont proposé des algorithmes et développé des cadres théoriques pour la prédiction d'ordres totaux ou partiels sur les exemples. La Recherche d'Information est une fois encore le domaine par excellence où les algorithmes d'apprentissage de fonctions d'ordonnancement jouent un rôle prépondérant. Dans notre étude nous nous sommes intéressés à deux cadres d'ordonnancement d'instances et d'alternatives. Dans le premier cas il s'agit d'ordonner les exemples (où instances) d'une collection donnée de façon à ce que les exemples jugés pertinents soient ordonnés au--dessus des exemples non--pertinents et dans le second cas nous cherchons à ordonner les alternatives d'une collection donnée par rapport à chaque exemple d'entrée. Ce mémoire présente mes travaux de recherche depuis ma thèse soutenue en 2001 suivant les deux axes apprentissage semi-supervisé et apprentissage de fonctions d'ordonnancement évoqués plus haut. J'ai commencé à m'intéresser à la problématique d'apprentisage semi-supervisé pour la classification à la fin de ma thèse jusqu'à fin 2003. En 2004 et 2005 j'ai abordé la problématique d'apprentissage supervisé de fonctions d'ordonnancement avec comme application phare le résumé automatique de textes. En 2006 je me suis intéressé à l'apprentissage actif de fonctions d'ordonnancement et nous avons été parmi les premiers à proposer un cadre théorique pour l'apprentissage actif de fonctions d'ordonnancement d'alternatives.",
    "uri": "https://hal.science/tel-04814059v1",
    "texte_nettoye": "Avec le développement des technologies d'information on assiste depuis quelques années à une nouvelle impulsion pour la conception de nouveaux cadres d'apprentissage automatique. C'est le cas par exemple du paradigme semi-supervisé qui a vu le jour vers la fin des années 90 dans la communauté apprentissage. Les premiers travaux dans ce cadre ont été motivés par le développement du \\textit{web} qui a entraîné une production massive de données textuelles très hétérogènes. Ces masses de données sont généralement livrées sous forme brute, sans étiquetage a priori et pour les exploiter on était alors réduit à utiliser des techniques non-supervisées. Ces approches bien que totalement génériques ne permettent cependant qu'une analyse limitée des informations de contenu et ne répondent pas ainsi aux demandes de nombreuses tâches de Recherche d'Information (RI). L'idée pragmatique développée pour l'apprentissage \\textit{semi-supervisé} était née de la question; \"comment réduire l'effort d'étiquetage et utiliser simultanément une petite quantité de données étiquetée avec la masse de données non-étiquetées pour apprendre?\" Un autre exemple de l'émergence de nouveaux cadres d'apprentissage concerne le développement de méthodes automatiques pour la recherche et l'ordonnancement d'entités d'information sur des corpus de grandes tailles. Récemment beaucoup de travaux se sont intéressés à la formulation des différentes formes de la tâche d'ordonnancement. Ces travaux ont proposé des algorithmes et développé des cadres théoriques pour la prédiction d'ordres totaux ou partiels sur les exemples. La Recherche d'Information est une fois encore le domaine par excellence où les algorithmes d'apprentissage de fonctions d'ordonnancement jouent un rôle prépondérant. Dans notre étude nous nous sommes intéressés à deux cadres d'ordonnancement d'instances et d'alternatives. Dans le premier cas il s'agit d'ordonner les exemples (où instances) d'une collection donnée de façon à ce que les exemples jugés pertinents soient ordonnés au--dessus des exemples non--pertinents et dans le second cas nous cherchons à ordonner les alternatives d'une collection donnée par rapport à chaque exemple d'entrée. Ce mémoire présente mes travaux de recherche depuis ma thèse soutenue en 2001 suivant les deux axes apprentissage semi-supervisé et apprentissage de fonctions d'ordonnancement évoqués plus haut. J'ai commencé à m'intéresser à la problématique d'apprentisage semi-supervisé pour la classification à la fin de ma thèse jusqu'à fin 2003. En 2004 et 2005 j'ai abordé la problématique d'apprentissage supervisé de fonctions d'ordonnancement avec comme application phare le résumé automatique de textes. En 2006 je me suis intéressé à l'apprentissage actif de fonctions d'ordonnancement et nous avons été parmi les premiers à proposer un cadre théorique pour l'apprentissage actif de fonctions d'ordonnancement d'alternatives."
  },
  {
    "title_s": [
      "Langages documentaires et nouvelles technologies : l'avenir des langages et leur positionnement au cœur des systèmes d'informations dans le contexte de la presse."
    ],
    "keyword_s": [
      "Langage documentaire",
      "Langage naturel",
      "Accès à l'information",
      "Système de recherche d'information",
      "TAL traitement automatique du langage",
      "Indexation automatique",
      "Surindexation",
      "Moteur de recherche",
      "Thésaurus",
      "Presse"
    ],
    "abstract_s": [
      "L'auteur définit dans un premier temps les fonctions du langage documentaire (normalisation, désambiguïsation, organisation des connaissances et structuration), et le fonctionnement de l'indexation automatique et des différents traitements linguistiques, statistiques et sémantiques. Puis il replace la problématique de la recherche d'information dans le contexte particulier de la presse et décrit les systèmes d'informations de Bayard, du Monde, des Échos et du Nouvel Observateur. Ensuite, en partant des besoins d'informations des journalistes, il tente de définir les rôles et les fonctions des langages documentaires face à l'utilisation des nouvelles technologies en fonction d'une typologie de l'indexation. Dans les systèmes d'informations presse, les outils d'indexation automatique et de recherche en texte intégral peuvent prendre en charge le traitement linguistique et sémantique, la classification automatique, etc. Pourtant le rôle des langages documentaires et la place de l'indexation humaine restent importants. Un usage sélectif de l'indexation manuelle permet une meilleure conceptualisation du sujet et une véritable structuration des documents en fonction des types d'information. L'intégration des langages améliore les performances de la recherche sémantique et permet une meilleure normalisation et désambiguïsation du langage naturel. Les fonctions d'organisation des connaissances, de médiation et d'aide à la recherche des langages documentaires viennent en complémentarité des classifications et de l'hypertexte. Le langage documentaire apparaît en complémentarité de la recherche en texte intégral. Même s'il devient de plus en plus invisible, intégré aux nouvelles technologies, ces fonctions sont préservées et demeurent indispensables."
    ],
    "authFullName_s": [
      "Odile Contat"
    ],
    "halId_s": "mem_00000057",
    "producedDateY_i": 2003,
    "texte_nettoye": "L'auteur définit dans un premier temps les fonctions du langage documentaire (normalisation, désambiguïsation, organisation des connaissances et structuration), et le fonctionnement de l'indexation automatique et des différents traitements linguistiques, statistiques et sémantiques. Puis il replace la problématique de la recherche d'information dans le contexte particulier de la presse et décrit les systèmes d'informations de Bayard, du Monde, des Échos et du Nouvel Observateur. Ensuite, en partant des besoins d'informations des journalistes, il tente de définir les rôles et les fonctions des langages documentaires face à l'utilisation des nouvelles technologies en fonction d'une typologie de l'indexation. Dans les systèmes d'informations presse, les outils d'indexation automatique et de recherche en texte intégral peuvent prendre en charge le traitement linguistique et sémantique, la classification automatique, etc. Pourtant le rôle des langages documentaires et la place de l'indexation humaine restent importants. Un usage sélectif de l'indexation manuelle permet une meilleure conceptualisation du sujet et une véritable structuration des documents en fonction des types d'information. L'intégration des langages améliore les performances de la recherche sémantique et permet une meilleure normalisation et désambiguïsation du langage naturel. Les fonctions d'organisation des connaissances, de médiation et d'aide à la recherche des langages documentaires viennent en complémentarité des classifications et de l'hypertexte. Le langage documentaire apparaît en complémentarité de la recherche en texte intégral. Même s'il devient de plus en plus invisible, intégré aux nouvelles technologies, ces fonctions sont préservées et demeurent indispensables."
  },
  {
    "title_s": [
      "Découverte automatique des textes littéraires qui présentent les caractéristiques statistiques d'un texte de qualité"
    ],
    "keyword_s": [
      "Automatic language processing",
      "Automatic evaluation of texts",
      "Learning",
      "Apprentissage",
      "Classification",
      "Évaluation automatique de textes",
      "Traitement automatique de la langue"
    ],
    "abstract_s": [
      "Le domaine du traitement automatique des langues naturelles a connu des évolutions très rapides ces dernières années, et spécialement les méthodes de statistique textuelle. Elles ont été mises en lumière par plusieurs disciplines : l'étude des textes, la linguistique, l'analyse du discours, la statistique, l'informatique, le traitement des enquêtes. Ce projet de recherche s'inscrit dans le cadre de la problématique de Short Edition qui concerne l'éditeur communautaire de littérature courte. L'objectif est d'assister le travail du comité de lecture en effectuant une première catégorisation des textes. Notre travail implique la conception et la mise en œuvre d'un prototype permettant de repérer les textes qui présentent les caractéristiques d'un texte de qualité et de trouver une méthode de classification en nous fondant sur les principes de la fouille de données permettant de bien classer nos textes.",
      "The field of natural language processing has witnessed very rapid developments in recent years, particularly with respect to methods used for statistical text analysis. These methods have been brought into focus by several disciplines in particular : the study of texts, linguistics, discours analysis, statistics, computer sciences, and survey processing. This research project develops within the framework of an issue that concerns the publishing company Short Editions. It relies on contributions in a field that employs a vast variety of designations (lexical statistics, statistical linguistics, quantitative linguistics, etcetera). Our work involves the creation of a prototype that allows for the identification of texts that present the characteristics of a quality text and to find appropriate methods of classification of these texts based on data and text mining principles."
    ],
    "authFullName_s": [
      "Hamza Maaouia"
    ],
    "halId_s": "dumas-01066867",
    "producedDateY_i": 2014,
    "texte_nettoye": "Le domaine du traitement automatique des langues naturelles a connu des évolutions très rapides ces dernières années, et spécialement les méthodes de statistique textuelle. Elles ont été mises en lumière par plusieurs disciplines : l'étude des textes, la linguistique, l'analyse du discours, la statistique, l'informatique, le traitement des enquêtes. Ce projet de recherche s'inscrit dans le cadre de la problématique de Short Edition qui concerne l'éditeur communautaire de littérature courte. L'objectif est d'assister le travail du comité de lecture en effectuant une première catégorisation des textes. Notre travail implique la conception et la mise en œuvre d'un prototype permettant de repérer les textes qui présentent les caractéristiques d'un texte de qualité et de trouver une méthode de classification en nous fondant sur les principes de la fouille de données permettant de bien classer nos textes."
  },
  {
    "title_s": [
      "Gestion de corpus linguistiques : fusion de la Linguistique et du Traitement Automatique des Langues dans le projet PREFAB"
    ],
    "keyword_s": [
      "Natural language processing",
      "Similarity calculation",
      "Data analysis",
      "Translation",
      "Parallel corpus",
      "Prefabricated sentences",
      "Traitement automatique des langues",
      "Calcul de similarité",
      "Analyse de données",
      "Traduction",
      "Corpus parallèle",
      "Phrases préfabriquées"
    ],
    "abstract_s": [
      "Ce mémoire présente une partie des actions indispensables dans le cadre du projet PREFAB, basé sur l'analyse de corpus bilingues. Il met en lumière le fonctionnement de différents outils existants pour réaliser des analyses puissantes, ainsi que les limites rencontrées et les corrections nécessaires. Il souligne également la fusion prometteuse entre la linguistique et le traitement automatique des langues pour modéliser un phénomène linguistique peu documenté. Le mémoire offre une description globale du projet et une présentation de la suite des opérations en cours. Parmi les nombreuses réalisations, le repérage, l'annotation, l'analyse et l'inventaire des phrases préfabriquées du français ont été initiés avec succès. Le projet PREFAB, entamé en octobre 2022 pour une durée de 48 mois, ouvre de nouvelles perspectives pour la compréhension des phrases préfabriquées et suscite un fort intérêt pour l'avenir de la recherche en linguistique et traitement automatique des langues.",
      "This dissertation presents some of the actions required as part of the PREFAB project, based on the analysis of bilingual corpora. It highlights the workings of various existing tools for performing powerful analyses, as well as the limitations encountered, and corrections needed. It also highlights the promising fusion between linguistics and natural language processing to model a little-documented linguistic phenomenon. The dissertation offers an overall description of the project and a presentation of the work in progress. Among the many achievements, the identification, annotation, analysis, and inventory of French prefabricated sentences have been successfully initiated. The PREFAB project, which began in October 2022 and will run for 48 months, is opening new perspectives for the understanding of prefabricated sentences and is of great interest for future research in linguistics and natural language processing."
    ],
    "authFullName_s": [
      "Florine Hecquet"
    ],
    "halId_s": "dumas-04260553",
    "producedDateY_i": 2023,
    "texte_nettoye": "Ce mémoire présente une partie des actions indispensables dans le cadre du projet PREFAB, basé sur l'analyse de corpus bilingues. Il met en lumière le fonctionnement de différents outils existants pour réaliser des analyses puissantes, ainsi que les limites rencontrées et les corrections nécessaires. Il souligne également la fusion prometteuse entre la linguistique et le traitement automatique des langues pour modéliser un phénomène linguistique peu documenté. Le mémoire offre une description globale du projet et une présentation de la suite des opérations en cours. Parmi les nombreuses réalisations, le repérage, l'annotation, l'analyse et l'inventaire des phrases préfabriquées du français ont été initiés avec succès. Le projet PREFAB, entamé en octobre 2022 pour une durée de 48 mois, ouvre de nouvelles perspectives pour la compréhension des phrases préfabriquées et suscite un fort intérêt pour l'avenir de la recherche en linguistique et traitement automatique des langues."
  },
  {
    "title_s": [
      "Prétraitement de données et création d'un segmenteur de l'arabe pour un système de traduction probabiliste vers le français"
    ],
    "keyword_s": [
      "Statistical Machine Translation",
      "Maxent",
      "Language Models",
      "Preprocessing and Tokenization of data",
      "Translation Models",
      "Maxent",
      "Traitement automatique de la langue arabe",
      "Modèles de traduction",
      "Traduction Automatique Probabiliste",
      "Prétraitement et segmentation de données",
      "Modèle de langage"
    ],
    "abstract_s": [
      "Le domaine du traitement automatique des langues naturelles a connu des évolutions très rapides ces dernières années, et spécialement dans la traduction automatique, c'est pourquoi les demandes en matière de traducteurs automatiques fiables augmentent sans cesse. De ce fait, nous nous sommes intéressés à ce domaine afin de concevoir un traducteur automatique de la langue arabe vers le français, basé sur un modèle probabiliste. Les performances de traduction des systèmes probabilistes dépendent considérablement de la qualité et de la quantité des données d'apprentissage disponibles. Néanmoins la langue arabe compte encore parmi les langues dites \"peu dotées\", c'est pourquoi la plupart des travaux sur cette langue sont basés sur les données libre d'accès qui proviennent d'organisations internationales (ONU, etc.). Nous présentons dans ce travail, une approche d'optimisation des performances d'un système de traduction de l'arabe. Compte tenu du manque de données et d'outils accessibles, nous avons cherché à moindre coût la meilleure combinaison de prétraitements à appliquer sur nos données en arabe pour améliorer la traduction vers le français.",
      "In recent years, Natural Language Processing has rapidly evolved, especially in the domain of Statistical Machine Translation causing the need for reliable automatic translations to skyrocket with no sign of slowing. Due to this increased need, we have taken a special interest in this domain with the goal of creating a translation machine capable of translating Arabic into French, based on statistical models. The performance of Statistical Machine Translation relies heavily on the quality and the quantity of available training data. However, the Arabic language remains one of the languages with the fewest available resources which is why most of the available works in this language are based on open access data from international organizations, such as the U.N. In this work, we will present our approach to optimizing the performance quality of our Arabic translator. Taking into account the lack of data and available resources, we were able to find a low-cost solution to search for the best pre-processing combinations to apply to our Arabic database in order to obtain the highest quality French translation."
    ],
    "authFullName_s": [
      "Sahnoun Ben Taamallah"
    ],
    "halId_s": "dumas-00757706",
    "producedDateY_i": 2012,
    "texte_nettoye": "Le domaine du traitement automatique des langues naturelles a connu des évolutions très rapides ces dernières années, et spécialement dans la traduction automatique, c'est pourquoi les demandes en matière de traducteurs automatiques fiables augmentent sans cesse. De ce fait, nous nous sommes intéressés à ce domaine afin de concevoir un traducteur automatique de la langue arabe vers le français, basé sur un modèle probabiliste. Les performances de traduction des systèmes probabilistes dépendent considérablement de la qualité et de la quantité des données d'apprentissage disponibles. Néanmoins la langue arabe compte encore parmi les langues dites \"peu dotées\", c'est pourquoi la plupart des travaux sur cette langue sont basés sur les données libre d'accès qui proviennent d'organisations internationales (ONU, etc.). Nous présentons dans ce travail, une approche d'optimisation des performances d'un système de traduction de l'arabe. Compte tenu du manque de données et d'outils accessibles, nous avons cherché à moindre coût la meilleure combinaison de prétraitements à appliquer sur nos données en arabe pour améliorer la traduction vers le français."
  },
  {
    "title_s": [
      "Traduction automatique de documents manuscrits et typographiés en arabe par couplage étroit entre systèmes"
    ],
    "keyword_s": [
      "Optical character recognition",
      "Word embedding",
      "Word lattice",
      "Statistical machine translation",
      "Natural language processing",
      "Plongement de mots",
      "Reconnaissance optique de caractères",
      "Graphes de mots treillis de mots",
      "Traduction automatique probabiliste",
      "Traitement automatique des langues naturelles"
    ],
    "abstract_s": [
      "Durant ces dernières années, le domaine du traitement automatique des langues naturelles (TALN) a connu des évolutions rapides, et spécialement la recherche des informations dans les documents numérisés, qui nécessite deux domaines connexes : la reconnaissance optique de caractères et la traduction automatique. Dans ce mémoire de recherche, nous nous sommes intéressés à la traduction automatique des documents numérisés, soit manuscrite ou typographie. En premier lieu, nous avons créé un système de traduction arabe français. En deuxième lieu, nous avons amélioré notre système par la traduction des graphes de mots qui sont construits à partir des sorties OCR bruitées (N-best). Et en dernier lieu, nous avons ajouté un traitement spécifique de mots hors vocabulaire en se basant sur l’approche de plongement de mots (word2vec).",
      "During the last few years, the field of automatic processing of natural languages has seen rapid developments, especially in research on scanned documents, which involves two interrelated fields: Optical character recognition (OCR) and machine translation (MT). In this research paper, we investigate machine translation of scanned documents, whether handwritten or typed. First of all, we created a Arabic-French machine translation system. Next we improved our system by translating a words lattice constructed from noisy OCR outputs (N-best). Finally, we added a specific preprocessing for out-of-vocabulary (OOV) words using word-embeddings (word2vec)."
    ],
    "authFullName_s": [
      "Kamel Bouzidi"
    ],
    "halId_s": "dumas-01494465",
    "producedDateY_i": 2016,
    "texte_nettoye": "Durant ces dernières années, le domaine du traitement automatique des langues naturelles (TALN) a connu des évolutions rapides, et spécialement la recherche des informations dans les documents numérisés, qui nécessite deux domaines connexes : la reconnaissance optique de caractères et la traduction automatique. Dans ce mémoire de recherche, nous nous sommes intéressés à la traduction automatique des documents numérisés, soit manuscrite ou typographie. En premier lieu, nous avons créé un système de traduction arabe français. En deuxième lieu, nous avons amélioré notre système par la traduction des graphes de mots qui sont construits à partir des sorties OCR bruitées (N-best). Et en dernier lieu, nous avons ajouté un traitement spécifique de mots hors vocabulaire en se basant sur l’approche de plongement de mots (word2vec)."
  },
  {
    "title_s": [
      "Constitution d'un corpus de traduction de la parole : augmentation du corpus LibriSpeech"
    ],
    "keyword_s": [
      "Speech translation",
      "Machine translation",
      "Parallel corpus",
      "Alignment",
      "Natural language processing",
      "Traduction automatique de la parole",
      "Traduction automatique neuronale",
      "Corpus parallèle",
      "Alignement",
      "Traitement automatique des langues"
    ],
    "abstract_s": [
      "Il existe des corpus parallèles en grande quantité tels que <i>Europarl</i>, <i>OpenSubtitles, etc</i>. pour les systèmes de traduction automatique. Toutefois, dans le domaine de la traduction automatique de la parole, le nombre de corpus disponibles est très restreint. Dans ce mémoire de recherche, nous nous sommes intéressés à la constitution d’un corpus réel de grande taille (236h) pour les systèmes de traduction automatique de la parole à partir du corpus LibriSpeech. Tout d'abord, nous avons récupéré les livres électroniques français correspondant aux livres audios présents dans LibriSpeech en anglais. Ensuite, après avoir aligné les œuvres en français avec les œuvres en anglais correspondantes, nous avons également aligné les segments de paroles des livres audio avec les livres électroniques en anglais, au niveau de la phrase (<i>utterance</i>). Ceci nous a permis d’obtenir l’alignement des segments de parole avec la traduction du texte original en français. Finalement, nous avons évalué manuellement 200 alignements et avons ajouté des scores de correspondance entre les transcriptions et les traductions pour trier le corpus en fonction de ces scores.",
      "Large quantities of parallel corpora such as <i>Europarl</i>, <i>OpenSubitles</i>, etc. is available for machine translation systems. However, for speech translation systems, number of available corpora is very limited. In this research paper, we investigate building a large scale (236h) non-synthetic corpus for speech translation systems from prepared speech recordings of LibriSpeech project. First, we gathered available French e-books corresponding to the English audio-books from LibriSpeech. Then, after aligning English and French texts at the sentence level, we also aligned speech segments at the sentence level with their respective translations. This allowed us to obtain an alignment at the utterance level aligned with their translations. Lastly, we manually evaluated 200 alignments and added alignment scores between transcriptions and their respective translations to sort the corpus according to those scores."
    ],
    "authFullName_s": [
      "Ali Can Kocabiyikoğlu"
    ],
    "halId_s": "dumas-01712400",
    "producedDateY_i": 2017,
    "texte_nettoye": "Il existe des corpus parallèles en grande quantité tels que <i>Europarl</i>, <i>OpenSubtitles, etc</i>. pour les systèmes de traduction automatique. Toutefois, dans le domaine de la traduction automatique de la parole, le nombre de corpus disponibles est très restreint. Dans ce mémoire de recherche, nous nous sommes intéressés à la constitution d’un corpus réel de grande taille (236h) pour les systèmes de traduction automatique de la parole à partir du corpus LibriSpeech. Tout d'abord, nous avons récupéré les livres électroniques français correspondant aux livres audios présents dans LibriSpeech en anglais. Ensuite, après avoir aligné les œuvres en français avec les œuvres en anglais correspondantes, nous avons également aligné les segments de paroles des livres audio avec les livres électroniques en anglais, au niveau de la phrase (<i>utterance</i>). Ceci nous a permis d’obtenir l’alignement des segments de parole avec la traduction du texte original en français. Finalement, nous avons évalué manuellement 200 alignements et avons ajouté des scores de correspondance entre les transcriptions et les traductions pour trier le corpus en fonction de ces scores."
  },
  {
    "title_s": [
      "Mise en place d’un système d’acquisition semi-automatique d’un corpus de données hétérogènes (images et textes) : application à la problématique de la sécurité alimentaire en Afrique de l'Ouest"
    ],
    "keyword_s": [
      "Food safety",
      "Corpus",
      "Machine learning",
      "Natural language processing",
      "Text mining",
      "Sécurité alimentaire",
      "Fouille de texte",
      "Corpus",
      "Apprentissage automatique",
      "Traitement automatique du langage"
    ],
    "abstract_s": [
      "Des systèmes d’alerte précoce sur la gestion des risques liés à la sécurité alimentaire sont mis en place pour informer les acteurs, afin d’adapter et améliorer le développement de l’agriculture. Ces systèmes analysent principalement des images satellitaires (par exemple des images de champs agricoles) et des données quantitatives (par exemple le prix du marché alimentaire). Cependant, peu de données textuelles sont intégrées dans leur traitement, or elles sont de plus en plus abondantes et accessibles sur Internet. Elles ne décrivent pas parfaitement la situation géographique, mais elles pourraient apporter une information ou une observation complémentaire aux images satellitaires. Ce mémoire propose donc d’acquérir un corpus d’articles de sites d’actualités en lien avec le thème de la sécurité alimentaire au Burkina Faso, d’extraire les informations spatio-temporelles présentes dans les articles, puis de les mettre en relation avec les données présentes dans les systèmes d’alerte précoce de sécurité alimentaire.",
      "Food security early warning systems are set up to inform stakeholders in order to adapt and improve agricultural development. These systems mainly analyze satellite images (e. g. images of agricultural fields) and quantitative data (e. g. food market prices). However, few textual data are integrated into their processing, yet they are increasingly abundant and accessible on the Internet. They do not perfectly describe the geographical situation, but they could provide additional information or observations to satellite images. This thesis therefore proposes to acquire a corpus of articles from newspapers sites related to the theme of food security in Burkina Faso, to extract the spatio-temporal information contained in the corpus, and then to link them to the data contained in food security early warning systems."
    ],
    "authFullName_s": [
      "Camille Schaeffer"
    ],
    "halId_s": "dumas-02302235",
    "producedDateY_i": 2019,
    "texte_nettoye": "Des systèmes d’alerte précoce sur la gestion des risques liés à la sécurité alimentaire sont mis en place pour informer les acteurs, afin d’adapter et améliorer le développement de l’agriculture. Ces systèmes analysent principalement des images satellitaires (par exemple des images de champs agricoles) et des données quantitatives (par exemple le prix du marché alimentaire). Cependant, peu de données textuelles sont intégrées dans leur traitement, or elles sont de plus en plus abondantes et accessibles sur Internet. Elles ne décrivent pas parfaitement la situation géographique, mais elles pourraient apporter une information ou une observation complémentaire aux images satellitaires. Ce mémoire propose donc d’acquérir un corpus d’articles de sites d’actualités en lien avec le thème de la sécurité alimentaire au Burkina Faso, d’extraire les informations spatio-temporelles présentes dans les articles, puis de les mettre en relation avec les données présentes dans les systèmes d’alerte précoce de sécurité alimentaire."
  },
  {
    "title_s": [
      "Intégration des expressions polylexicales dans un système de traduction statistique"
    ],
    "keyword_s": [
      "Multiword expression",
      "Translation models",
      "Statistical machine translation",
      "Specific corpus",
      "Natural language processing",
      "Traitement automatique des langues naturelles",
      "Expressions polylexicales",
      "Corpus spécifiques",
      "Traduction automatique statistique",
      "Modèles de traduction"
    ],
    "abstract_s": [
      "Les expressions polylexicales (EPL) constituent un champ de recherche intéressant dans le domaine de la linguistique computationnelle. Elles peuvent être considérées comme un vrai défi pour les systèmes de traduction automatique statistique (TAS). Dans ce mémoire, nous avons enrichi une ressource textuelle littéraire existante en vue d'extraire des corpus spécifiques pour évaluer le problème des expressions polylexicales en traduction automatique statistique. Par ailleurs, dans le but d'améliorer les performances d'un système de traduction automatique, nous avons testé une stratégie d'intégration des EPL en anglais. Les résultats obtenus au niveau du score BLEU sont encourageants.",
      "Multiword Expressions constitute an interesting field of research in Computational Linguistics. They can be a real challenge for Statistical Machine Translation systems (SMT). In this paper, we have improved an existing literary lexical resource in order to extract a corpus in order to assess the problem of Multiword Expressions in SMT. Moreover, in order to improve the performance of an automatic translation system, we integrated English EPL. As regards the BLEU score, the results were encouraging."
    ],
    "authFullName_s": [
      "Zied Elloumi"
    ],
    "halId_s": "dumas-01063275",
    "producedDateY_i": 2014,
    "texte_nettoye": "Les expressions polylexicales (EPL) constituent un champ de recherche intéressant dans le domaine de la linguistique computationnelle. Elles peuvent être considérées comme un vrai défi pour les systèmes de traduction automatique statistique (TAS). Dans ce mémoire, nous avons enrichi une ressource textuelle littéraire existante en vue d'extraire des corpus spécifiques pour évaluer le problème des expressions polylexicales en traduction automatique statistique. Par ailleurs, dans le but d'améliorer les performances d'un système de traduction automatique, nous avons testé une stratégie d'intégration des EPL en anglais. Les résultats obtenus au niveau du score BLEU sont encourageants."
  },
  {
    "title_s": [
      "Fact-checking dans les médias"
    ],
    "keyword_s": [
      "Artificial intelligence",
      "Fact-checking",
      "Data set",
      "Natural language processing",
      "Jeu de données",
      "Traitement automatique des langues",
      "Vérification des faits",
      "Intelligence artificielle"
    ],
    "abstract_s": [
      "Durant cinq mois j’ai travaillé sur une thématique portant sur l’intelligence artificielle qui consistait à vérifier des faits dans des articles de presse majoritairement. C’est en intégrant l’entreprise Buster.AI, dont le but premier étant de réaliser mon stage de fin d’études, que j’ai pu traiter ce sujet. Tout au long de mon stage, j’ai travaillé avec des bases de données importantes et ai construit des jeux de données qui ont permis de tester, d’entrainer et d’améliorer les modèles de l’entreprise. Ce travail a développé de nouvelles performances au produit de Buster grâce aux résultats obtenus. Ce stage a également approfondi mes compétences et m’en a appris des nouvelles dans le domaine du traitement automatique des langues. J’ai aussi, grâce à cette expérience professionnelle, pu découvrir le monde professionnel et l’organisation d’une entreprise.",
      "During five months I worked on a topic related to artificial intelligence which was about the fact-checking in newspaper mainly. It is by joining the company Buster.AI, whose first goal was to carry out my end of studies internship, that I was able to study this subject. Throughout my internship, I worked with large databases and built many datasets that allowed me to test, train and improve the company's models. This work developed new performances to the Buster product thanks to the new results obtained. This internship has also improved my skills and taught me new ones in the domain of natural language processing. Thanks to this work experience, I was also able to discover the professional world and the organization of a company."
    ],
    "authFullName_s": [
      "Sanda Hachana"
    ],
    "halId_s": "dumas-03643250",
    "producedDateY_i": 2021,
    "texte_nettoye": "Durant cinq mois j’ai travaillé sur une thématique portant sur l’intelligence artificielle qui consistait à vérifier des faits dans des articles de presse majoritairement. C’est en intégrant l’entreprise Buster.AI, dont le but premier étant de réaliser mon stage de fin d’études, que j’ai pu traiter ce sujet. Tout au long de mon stage, j’ai travaillé avec des bases de données importantes et ai construit des jeux de données qui ont permis de tester, d’entrainer et d’améliorer les modèles de l’entreprise. Ce travail a développé de nouvelles performances au produit de Buster grâce aux résultats obtenus. Ce stage a également approfondi mes compétences et m’en a appris des nouvelles dans le domaine du traitement automatique des langues. J’ai aussi, grâce à cette expérience professionnelle, pu découvrir le monde professionnel et l’organisation d’une entreprise."
  },
  {
    "title_s": [
      "Micro-expressions audio-visuelles dans la communication expressive : enjeux pluri-culturels"
    ],
    "keyword_s": [
      "Speech micro-expressions",
      "Social affective attitudes",
      "NLP",
      "Natural Language Processing",
      "Applications strategies",
      "Enjeux applicatifs",
      "Traitement Automatique de la Langue",
      "TAL",
      "Attitudes socio-affectifs",
      "Perception",
      "Synergologie",
      "Micro-expressions de la parole"
    ],
    "abstract_s": [
      "L'évolution des habitudes sociales crée chaque jour de nouveaux besoins dans le domaine du Traitement Automatique de la Langue. Plus particulièrement dans le cas des technologies de la parole, l'un des enjeux majeurs est l'analyse des bruits non lexicalisés, accompagnés de gestes spécifiques, prémisses du langage humain. Ces objets sont désignés sous le terme de micro-expressions de la parole dans le cadre de cette étude. En reconnaissance, ces objets permettent de comprendre les attitudes socio-affectives de l'humain qui sont au cœur des nouveaux enjeux applicatifs actuels. Ainsi, si nous avons dors et déjà des pistes d'investigation sur les aspects acoustiques de ces micro-expressions, l'analyse visuelle en est une toute autre affaire. Il est alors intéressant de voir comment les nouveaux courants à la mode tels que la synergologie peut servir à une recherche scientifique. L'étude en cours vise à comprendre tous ces mécanismes et ces enjeux à travers une expérience de perception des micro-expressions de la parole en contraste français-japonais.",
      "The changing of social habits creates every day new needs in the area of Natural Language Processing. Especially in the case of speech technologies, one of the major challenges is the analysis of non-lexicalized sounds, accompanied by specific gestures, premises of human language. These objects are referred to as speech micro-expressions in this study. In recognition, these objects provide an understanding of the socio-emotional attitudes of humans which are the heart of the new challenges of modern applications. If we already have some ideas to investigate acoustic aspects of these micro-expressions, visual analysis is another matter. It is interesting to see how new movements in fashion as synergologie can be used in a scientific research. The current study aims to understand these mechanisms and issues through an experiment of speech micro-expressions perception in a French-Japanese contrast."
    ],
    "authFullName_s": [
      "Yuko Sasa"
    ],
    "halId_s": "dumas-00709396",
    "producedDateY_i": 2012,
    "texte_nettoye": "L'évolution des habitudes sociales crée chaque jour de nouveaux besoins dans le domaine du Traitement Automatique de la Langue. Plus particulièrement dans le cas des technologies de la parole, l'un des enjeux majeurs est l'analyse des bruits non lexicalisés, accompagnés de gestes spécifiques, prémisses du langage humain. Ces objets sont désignés sous le terme de micro-expressions de la parole dans le cadre de cette étude. En reconnaissance, ces objets permettent de comprendre les attitudes socio-affectives de l'humain qui sont au cœur des nouveaux enjeux applicatifs actuels. Ainsi, si nous avons dors et déjà des pistes d'investigation sur les aspects acoustiques de ces micro-expressions, l'analyse visuelle en est une toute autre affaire. Il est alors intéressant de voir comment les nouveaux courants à la mode tels que la synergologie peut servir à une recherche scientifique. L'étude en cours vise à comprendre tous ces mécanismes et ces enjeux à travers une expérience de perception des micro-expressions de la parole en contraste français-japonais."
  },
  {
    "title_s": [
      "Natural Language Processing for virtual assistants: what contribution synthetic data could bring to intents classification?",
      "Traitement automatique du langage pour les assistants virtuels : quel apport des jeux de données synthétiques pour la détection d'intentions ?"
    ],
    "keyword_s": [
      "Generation",
      "Human factor",
      "Virtual agent",
      "Natural language understanding",
      "Natural language processing",
      "Aeronautic",
      "Airbus",
      "Facteur humain",
      "Génération automatique",
      "Aéronautique",
      "Traitement automatique du langage naturel",
      "Compréhension du langage naturel",
      "Agent Virtuel"
    ],
    "abstract_s": [
      "This document summarizes a 6 months internship that took place within the Human Factor department of the Airbus Company in Toulouse, France. This internship was centred on a virtual agent research thematic. Our initial hypothesis was that a system of automatic intent categorization could benefit from using synthetic “natural-like” data. In order to validate or invalidate this hypothesis we decided, first, to create a methodology that would help us collect natural questions from end-users. Then we used the “natural” data we previously collected along with a synthetic question generator we designed in order to output synthetic questions that feels “natural-like” if compared to the input ones. Lastly, we experimented on the synthetic datasets using various tools in order to put to the test our initial hypothesis. The results we obtained from these tests allowed us to open new perspectives on the natural language understanding part of the virtual agent system.",
      "Ce document synthétise un stage de 6 mois passé dans le service Facteur Humain du constructeur aéronautique Airbus à Toulouse sur une problématique de développement d'assistant virtuel. L'objet de ce stage était de vérifier l'hypothèse selon laquelle un système de catégorisation de requêtes d'utilisateurs pouvait bénéficier de la création de jeux de données synthétiques. Pour vérifier cette hypothèse nous avons dans un premier temps créé une méthodologie de récolte de questions dites \"naturelles\" auprès des utilisateurs finaux de l'agent virtuel. Ensuite nous avons développé un programme permettant de générer des questions synthétiques en se basant sur le jeu de données naturelles préalablement collectées. Pour finir nous avons expérimenté à l'aide de plusieurs outils sur les jeux de données générés. Les résultats obtenus ont permis d'ouvrir de nouvelles pistes de recherches et d'amélioration sur le sujet de la compréhension du langage naturel par le système."
    ],
    "authFullName_s": [
      "Sylvain Daronnat"
    ],
    "halId_s": "dumas-01695385",
    "producedDateY_i": 2017,
    "texte_nettoye": "This document summarizes a 6 months internship that took place within the Human Factor department of the Airbus Company in Toulouse, France. This internship was centred on a virtual agent research thematic. Our initial hypothesis was that a system of automatic intent categorization could benefit from using synthetic “natural-like” data. In order to validate or invalidate this hypothesis we decided, first, to create a methodology that would help us collect natural questions from end-users. Then we used the “natural” data we previously collected along with a synthetic question generator we designed in order to output synthetic questions that feels “natural-like” if compared to the input ones. Lastly, we experimented on the synthetic datasets using various tools in order to put to the test our initial hypothesis. The results we obtained from these tests allowed us to open new perspectives on the natural language understanding part of the virtual agent system."
  },
  {
    "title_s": [
      "Conception et réalisation d'une chaîne de traitement automatique des langues adaptée à des projets littéraires"
    ],
    "keyword_s": [
      "NLP",
      "XML",
      "Part-of-speech tagging",
      "Textometrics",
      "XML",
      "Textométrie",
      "Étiquetage morphosyntaxique",
      "TAL"
    ],
    "abstract_s": [
      "J’ai développé un outil en Python afin de faciliter l’utilisation de plusieurs outils de traitement morphosyntaxique et textométrique sur plusieurs types de fichiers, y compris les fichiers XML. Cet outil fonctionne à l’aide d’une interface Web développée en PHP dont le but est de simplifier autant que possible l’utilisation de l’outil par un utilisateur non formé à l’utilisation d’outils informatiques complexes.",
      "Using Python, I developped a tool whose goal was to simplify the use of part-of-speech tagging and textometrical tools on multiple types of files, including XML files. This tool uses a Web interface written in PHP in order to ensure that even an user who is untrained in the use of complex technological tools can make use of it."
    ],
    "authFullName_s": [
      "Julien Fagot"
    ],
    "halId_s": "dumas-02987314",
    "producedDateY_i": 2020,
    "texte_nettoye": "J’ai développé un outil en Python afin de faciliter l’utilisation de plusieurs outils de traitement morphosyntaxique et textométrique sur plusieurs types de fichiers, y compris les fichiers XML. Cet outil fonctionne à l’aide d’une interface Web développée en PHP dont le but est de simplifier autant que possible l’utilisation de l’outil par un utilisateur non formé à l’utilisation d’outils informatiques complexes."
  },
  {
    "title_s": [
      "Répartition hommes/femmes dans les systèmes d’IA : une étude pilote sur les grands corpus pour la transcription automatique de la parole"
    ],
    "keyword_s": [
      "Machine learning",
      "Automatic speech processing",
      "Corpus",
      "Gender",
      "Apprentissage automatique",
      "Traitement automatique de la parole",
      "Genre"
    ],
    "abstract_s": [
      "Les systèmes d’IA sont développés sur des grands corpus de données et les technologies du traitement automatique de la parole n’échappent pas à cette règle. Mais ces grands corpus de données peuvent contenir des répartitions de genre non-équilibrées qui peuvent conduire au développement d’algorithmes discriminants. Les systèmes d’IA infiltrant de plus en plus notre quotidien, et la voix s’imposant comme la nouvelle interface homme/machine, il devient nécessaire de pouvoir étudier et quantifier l’impact de la répartition homme/femme dans les données d’apprentissage sur les performances des systèmes. Ce mémoire propose donc dans un premier temps d’étudier la répartition des genres dans les grands corpus du français oral, et dans un second temps, d’évaluer l’impact de cette représentation sur les performances d’un système de reconnaissance automatique de la parole.",
      "AI systems are trained on a huge amount of data, and speech processing technologies are no exception to the rule. However corpora may be statistically imbalanced regarding genders and this can lead to discriminative algorithms. With AI becoming ever more present in our everyday life, it seems more than necessary to be aware of the impact of gender representation in training data on the system’s performances. This masters’ thesis proposes to study gender representation in large spoken french corpora and to estimate the impact of this distribution on the performances of an automatic speech recognition system."
    ],
    "authFullName_s": [
      "Mahault Garnerin"
    ],
    "halId_s": "dumas-01835333",
    "producedDateY_i": 2018,
    "texte_nettoye": "Les systèmes d’IA sont développés sur des grands corpus de données et les technologies du traitement automatique de la parole n’échappent pas à cette règle. Mais ces grands corpus de données peuvent contenir des répartitions de genre non-équilibrées qui peuvent conduire au développement d’algorithmes discriminants. Les systèmes d’IA infiltrant de plus en plus notre quotidien, et la voix s’imposant comme la nouvelle interface homme/machine, il devient nécessaire de pouvoir étudier et quantifier l’impact de la répartition homme/femme dans les données d’apprentissage sur les performances des systèmes. Ce mémoire propose donc dans un premier temps d’étudier la répartition des genres dans les grands corpus du français oral, et dans un second temps, d’évaluer l’impact de cette représentation sur les performances d’un système de reconnaissance automatique de la parole."
  },
  {
    "title_s": [
      "SMS et TAL : kL 1Trè* ? (*SMS et TAL : Quel intérêt ?)"
    ],
    "keyword_s": [
      "Traitement automatique",
      "Transcription",
      "Anonymisation",
      "Corpus",
      "Langage",
      "SMS"
    ],
    "abstract_s": [
      "Ce mémoire présente le travail réalisé en préparation d'une collecte de SMS en France métropolitaine : alpes4science. Ce projet vise à constituer un corpus afin de proposer des données nombreuses et diverses comme outils d'études aux chercheurs travaillant sur le SMS, les pratiques qui lui sont associées et le langage SMS qui peut découler de ce mode de communication. Le SMS peut apparaître comme un mode de communication écrit mais aussi oral, par son immédiateté et le langage utilisé. Dans un premier temps, nous proposons un état de l'art concernant le SMS et ses pratiques. Puis nous présentons les applications élaborées à partir de et pour les SMS, ainsi que les corpus réalisés jusqu'alors. Dans une dernière partie, nous expliquons ce qui a été mis en place pour constituer et exploiter le corpus alpes4science."
    ],
    "authFullName_s": [
      "Gaëlle Chabert"
    ],
    "halId_s": "dumas-00561995",
    "producedDateY_i": 2010,
    "texte_nettoye": "Ce mémoire présente le travail réalisé en préparation d'une collecte de SMS en France métropolitaine : alpes4science. Ce projet vise à constituer un corpus afin de proposer des données nombreuses et diverses comme outils d'études aux chercheurs travaillant sur le SMS, les pratiques qui lui sont associées et le langage SMS qui peut découler de ce mode de communication. Le SMS peut apparaître comme un mode de communication écrit mais aussi oral, par son immédiateté et le langage utilisé. Dans un premier temps, nous proposons un état de l'art concernant le SMS et ses pratiques. Puis nous présentons les applications élaborées à partir de et pour les SMS, ainsi que les corpus réalisés jusqu'alors. Dans une dernière partie, nous expliquons ce qui a été mis en place pour constituer et exploiter le corpus alpes4science."
  },
  {
    "title_s": [
      "Analyse des tendances des thématiques de recherche en réanimation : une approche par apprentissage automatique"
    ],
    "keyword_s": [
      "Bibliométrie",
      "Traitement automatique du langage naturel",
      "Apprentissage automatique",
      "Traitement automatique du langage",
      "Réanimation"
    ],
    "abstract_s": [
      "La réanimation est une spécialité médicale récente formalisée dans les années 1950. Elle a la particularité d’être une spécialité multidisciplinaire et sa littérature scientifique reflète cette multidisciplinarité. Cependant, la représentation de chaque spécialité médicale dans cette littérature et leur évolution au cours du temps ne sont pas connues. L’objectif de cette thèse est d’analyser la littérature de réanimation, d’en extraire les thématiques de recherche et d’observer les tendances en utilisant des algorithmes d’apprentissage automatique. Matériel et Méthodes : Les titres et abstracts des articles originaux des principales revues de réanimation depuis leur création jusqu’au 31 décembre 2019 ont été inclus. Après prétraitement, ce corpus a alimenté un algorithme de structural topic modeling pour extraire les thèmes sémantiques sous-jacents. Les thèmes ont été identifiés par un groupe d’experts composé de médecins réanimateurs de différentes formations. L’évolution temporelle a ensuite été analysée et la présence de tendances a été recherchée par des tests de Mann-Kendall. Résultats : 49276 articles issus de 10 revues ont été inclus. Après extraction et identification des thèmes, 124 sujets de recherche ont été sélectionnés. Les thèmes ont été classés en 19 catégories, les plus représentées étant les catégories respiratoire, expérimentale, neurologie et infectiologie. Des dynamiques de tendance à la hausse ont été observées pour la recherche sur le thème respiratoire et des tendances à la baisse pour la réanimation cardio-pulmonaire. Conclusion : Cette étude a passé en revue de manière exhaustive tous les articles des principales revues de réanimation. Elle permet de mieux comprendre le paysage de la recherche en réanimation en analysant l’évolution temporelle des thèmes de recherche dans la littérature."
    ],
    "authFullName_s": [
      "Benjamin Popoff"
    ],
    "halId_s": "dumas-03372812",
    "producedDateY_i": 2021,
    "texte_nettoye": "La réanimation est une spécialité médicale récente formalisée dans les années 1950. Elle a la particularité d’être une spécialité multidisciplinaire et sa littérature scientifique reflète cette multidisciplinarité. Cependant, la représentation de chaque spécialité médicale dans cette littérature et leur évolution au cours du temps ne sont pas connues. L’objectif de cette thèse est d’analyser la littérature de réanimation, d’en extraire les thématiques de recherche et d’observer les tendances en utilisant des algorithmes d’apprentissage automatique. Matériel et Méthodes : Les titres et abstracts des articles originaux des principales revues de réanimation depuis leur création jusqu’au 31 décembre 2019 ont été inclus. Après prétraitement, ce corpus a alimenté un algorithme de structural topic modeling pour extraire les thèmes sémantiques sous-jacents. Les thèmes ont été identifiés par un groupe d’experts composé de médecins réanimateurs de différentes formations. L’évolution temporelle a ensuite été analysée et la présence de tendances a été recherchée par des tests de Mann-Kendall. Résultats : 49276 articles issus de 10 revues ont été inclus. Après extraction et identification des thèmes, 124 sujets de recherche ont été sélectionnés. Les thèmes ont été classés en 19 catégories, les plus représentées étant les catégories respiratoire, expérimentale, neurologie et infectiologie. Des dynamiques de tendance à la hausse ont été observées pour la recherche sur le thème respiratoire et des tendances à la baisse pour la réanimation cardio-pulmonaire. Conclusion : Cette étude a passé en revue de manière exhaustive tous les articles des principales revues de réanimation. Elle permet de mieux comprendre le paysage de la recherche en réanimation en analysant l’évolution temporelle des thèmes de recherche dans la littérature."
  },
  {
    "title_s": [
      "Du présentiel au distanciel : le TAL pour comparer image voulue et image perçue"
    ],
    "keyword_s": [
      "Digital transition",
      "Digital interfaces",
      "Semantic resources",
      "Opinion mining",
      "NLP",
      "Transition numérique",
      "Interfaces numériques",
      "TAL",
      "Ressources sémantiques"
    ],
    "abstract_s": [
      "Le travail que nous présentons se situe dans le cadre d’un projet de recherche en collaboration entre l’Université Grenoble Alpes et l’entreprise COMONGO dont le cœur de métier est l’accompagnement et la gestion d’image des personnes morales et physiques. Notre démarche a consisté dans un premier temps à transposer une pratique en focus group vers une pratique distancielle numérique. Dans un second temps, il s’est agi d’intégrer les outils du Traitement Automatique des Langues, notamment les ressources sémantiques, à cette démarche professionnelle d’entreprise. Cette transformation d’une pratique métier nous a menée à poser trois grandes hypothèses : la transition numérique a un impact sur la qualité des données ; les ressources sémantiques permettent une meilleure appréhension des données textuelles traitées mais s’avèrent insuffisantes après simulation ; une démarche incrémentale d’amélioration des ressources doit être envisagée afin d’obtenir des traitements optimaux. Cette première expérimentation sur données réelles permet de poser les bases d’un projet de recherche et développement à plus long terme au sein de la société COMONGO alliant les domaines de la linguistique de corpus, du Traitement Automatique des Langues et des sciences de l’information et de la communication.",
      "The work that we present is part of a collaborative research project between the University Grenoble Alpes and the company COMONGO which provides image management of legal and physical persons. In a first step, we have transposed a focus group based approach into a distant digital practice. In a second step, we have investigated the integration of NLP tools, in particular semantic resources, into this professional approach. This digital transformation of a practice led us to make three main assumptions : the digital transition has an impact on the quality of the data ; the semantic resources allow a better understanding of the textual data processed but are insufficient after simulation ; an incremental approach to resources improvement should be considered in order to obtain optimal results. This first experimentation with real data allows us to lay the foundations for a longer-term research and development project within the company COMONGO combining the fields of corpus linguistics, Natural Language Processing and Information and Communication Sciences."
    ],
    "authFullName_s": [
      "Pauline Soutrenon"
    ],
    "halId_s": "dumas-01767534",
    "producedDateY_i": 2017,
    "texte_nettoye": "Le travail que nous présentons se situe dans le cadre d’un projet de recherche en collaboration entre l’Université Grenoble Alpes et l’entreprise COMONGO dont le cœur de métier est l’accompagnement et la gestion d’image des personnes morales et physiques. Notre démarche a consisté dans un premier temps à transposer une pratique en focus group vers une pratique distancielle numérique. Dans un second temps, il s’est agi d’intégrer les outils du Traitement Automatique des Langues, notamment les ressources sémantiques, à cette démarche professionnelle d’entreprise. Cette transformation d’une pratique métier nous a menée à poser trois grandes hypothèses : la transition numérique a un impact sur la qualité des données ; les ressources sémantiques permettent une meilleure appréhension des données textuelles traitées mais s’avèrent insuffisantes après simulation ; une démarche incrémentale d’amélioration des ressources doit être envisagée afin d’obtenir des traitements optimaux. Cette première expérimentation sur données réelles permet de poser les bases d’un projet de recherche et développement à plus long terme au sein de la société COMONGO alliant les domaines de la linguistique de corpus, du Traitement Automatique des Langues et des sciences de l’information et de la communication."
  },
  {
    "title_s": [
      "Amélioration des systèmes de reconnaissance de la parole des personnes âgées"
    ],
    "keyword_s": [
      "Ambiant Assisted Living",
      "Automatic Speech Recognition",
      "Elderly",
      "MLLR adaptation",
      "Acoustic model",
      "Language model",
      "Reconnaissance Automatique de la Parole",
      "Personnes âgées",
      "Assistance à la vie autonome",
      "Adaptation MLLR",
      "Modèle acoustique",
      "Modèle de langage"
    ],
    "abstract_s": [
      "La Reconnaissance Automatique de la Parole est une technologie en plein essor dont l'utilisation pour l'aide aux personnes fragiles apparaît comme un domaine novateur et porteur d'espoirs. Ce mémoire a été réalisé dans ce contexte avec pour objectif d'évaluer l'état du système de reconnaissance de la parole destiné à des personnes âgées dans le cadre du projet CIRDO. Nous avons étudié, à partir des travaux précédents sur le sujet, les différences entre la parole dite âgée et non âgée ; les comportements du système, après adaptations, de manière approfondie sur le plan des phonèmes et classes de phonèmes ainsi que le décodage en mots pour de la parole âgée avec pour métrique le WER. Ces recherches nous ont permis de mieux comprendre le fonctionnement du système Sphinx3 et de trouver un paramétrage qui permette d'obtenir des résultats intéressants sur un corpus de voix âgées de parole lue et de parole spontanée, thème sur lequel seuls de très rares travaux existent.",
      "Recognition Automatic Speech is a burgeoning technology whose use for assistance to frail appears as an innovative area and hope. This dissertation was made in this context with the aim of assessing the state of the system for speech recognition to elderly in the CIRDO project. We studied from previous works on the subject, the differences between the non-elderly speaking and elderly speaking ; behavior of the system, after adjustments, thoroughly in terms of phonemes and classes of phonemes ; words decoding of speech for elderly with the WER metric. This research helped us better understand how the system works and finding a Sphinx3 parameterization which allows to obtain interesting results on a corpus of read speech voice aged and spontaneous speech, topic on which only very few studies exist."
    ],
    "authFullName_s": [
      "Juline Le Grand"
    ],
    "halId_s": "dumas-00736504",
    "producedDateY_i": 2012,
    "texte_nettoye": "La Reconnaissance Automatique de la Parole est une technologie en plein essor dont l'utilisation pour l'aide aux personnes fragiles apparaît comme un domaine novateur et porteur d'espoirs. Ce mémoire a été réalisé dans ce contexte avec pour objectif d'évaluer l'état du système de reconnaissance de la parole destiné à des personnes âgées dans le cadre du projet CIRDO. Nous avons étudié, à partir des travaux précédents sur le sujet, les différences entre la parole dite âgée et non âgée ; les comportements du système, après adaptations, de manière approfondie sur le plan des phonèmes et classes de phonèmes ainsi que le décodage en mots pour de la parole âgée avec pour métrique le WER. Ces recherches nous ont permis de mieux comprendre le fonctionnement du système Sphinx3 et de trouver un paramétrage qui permette d'obtenir des résultats intéressants sur un corpus de voix âgées de parole lue et de parole spontanée, thème sur lequel seuls de très rares travaux existent."
  },
  {
    "title_s": [
      "Recherche automatique d'antériorités de brevets par la recherche de revendications et de segments proches potentiellement invalidants"
    ],
    "keyword_s": [
      "Repeated segments",
      "Patents",
      "Claims",
      "Patent analysis",
      "Natural language processing",
      "Lexical",
      "Syntax",
      "Internal structure of document",
      "Similar document",
      "Similarity",
      "Analyse de brevets",
      "Revendications",
      "Brevets",
      "Structure interne de documents",
      "Traitement automatique des langues",
      "Lexique",
      "Syntaxe",
      "Segments répétés",
      "Similarité de chaines",
      "Similarité",
      "Documents proches"
    ],
    "abstract_s": [
      "Le stage de fin d'études effectué à Tecknowmetrix, société de conseils et services en innovation proposant des outils ou des études de veille technologique, a pour but de mettre en place un moyen permettant de valider ou d'invalider des revendications d'un brevet, et ce de façon automatique ou semi-automatique. L'invalidation est ici restreinte au critère de nouveauté des brevets, c'est-à-dire à la recherche de documents proches pouvant invalider le caractère nouveau. Notre but est donc de créer un outil et une méthode permettant de retrouver au sein de grandes quantités de brevets quels sont les segments qui, étant très proches, peuvent s'invalider. La méthodologie utilisée s'éloigne des chemins classiques qui exploitent les statistiques et se veut plus orientée vers la linguistique en faisant intervenir la structure syntaxique interne des brevets, couplées à des méthodes issues du traitement automatique des langues. Nous définissons dans un premier temps ce qu'est un brevet et analysons les revendications du point de vue lexical et syntaxique afin de trouver des régularités que nous essayons de modaliser informatiquement pour mettre en place un outil de recherche de revendications proches sur un corpus de brevets de langue anglaise.",
      "Our internship took place in Tecknowmetrix; a business specialized in innovation, intellectual property and strategic intelligence. Our aim here was to create a tool which allows validating or invalidating some patent claims, in an automatic or semi-automatic way. Our goal was to invent a priority research tool which finds all the relevant and similar patents in order to find segments which are invalidating \"novel criteria\". Our methodology is different from classical ways which are exploiting statistical methods and is more oriented towards linguistic in basing itself on the internal structure of patents. Our work involved in natural language processing field. To do so, we defined what a patent is and we studied claims by lexical and syntax views in order to discover some regularities. These regularities will help us to figure out how producing a data model which can be applied on our tool that we want to develop. Our tool should be able to extract similar segments of similar claims on a corpus of patents written in the English language."
    ],
    "authFullName_s": [
      "Edmée Marazel"
    ],
    "halId_s": "dumas-00631733",
    "producedDateY_i": 2011,
    "texte_nettoye": "Le stage de fin d'études effectué à Tecknowmetrix, société de conseils et services en innovation proposant des outils ou des études de veille technologique, a pour but de mettre en place un moyen permettant de valider ou d'invalider des revendications d'un brevet, et ce de façon automatique ou semi-automatique. L'invalidation est ici restreinte au critère de nouveauté des brevets, c'est-à-dire à la recherche de documents proches pouvant invalider le caractère nouveau. Notre but est donc de créer un outil et une méthode permettant de retrouver au sein de grandes quantités de brevets quels sont les segments qui, étant très proches, peuvent s'invalider. La méthodologie utilisée s'éloigne des chemins classiques qui exploitent les statistiques et se veut plus orientée vers la linguistique en faisant intervenir la structure syntaxique interne des brevets, couplées à des méthodes issues du traitement automatique des langues. Nous définissons dans un premier temps ce qu'est un brevet et analysons les revendications du point de vue lexical et syntaxique afin de trouver des régularités que nous essayons de modaliser informatiquement pour mettre en place un outil de recherche de revendications proches sur un corpus de brevets de langue anglaise."
  },
  {
    "title_s": [
      "Normalisation des messages issus de la communication électronique médiée"
    ],
    "keyword_s": [
      "Annotated corpus",
      "SMS",
      "Tweets",
      "Automatic normalization",
      "NLP",
      "Traitement automatique de la langue TAL",
      "Normalisation automatique",
      "Corpus annoté"
    ],
    "abstract_s": [
      "Le travail dont ce mémoire rend compte consistait à élaborer un outil de normalisation automatique des textes non standard en français, en particulier les tweets et les SMS. Pour cela, nous avons d’abord annoté un corpus de 1000 tweets et 1000 SMS, en fonction de phénomènes morpho-lexicaux et morpho-syntaxiques, que nous avions au préalable identifiés lors de l’élaboration d’une typologie pour l’annotation de textes non standard. À partir de l’observation de ce corpus, nous avons développé un outil de normalisation automatique qui génère pour chaque token non standard un ensemble de candidats en fonction des phénomènes observés le plus fréquemment dans notre corpus de tweets et de SMS. Ensuite, la normalisation du token non standard est sélectionnée parmi l’ensemble de ces candidats, à l’aide d’un système d’attribution de scores prenant également en compte le contexte immédiat du token traité.",
      "The work reported in this paper consisted in the creation of an automatic normalization tool for non-standard texts written in French, in particular tweets and SMS. In order to do so, we first annotated a corpus of 1000 tweets and 1000 SMS, according to morpho-lexical and morpho-syntactic phenomena, which we had previously identified when elaborating a typology for the annotation of non-standard texts. From the observation of this corpus, we have developed an automatic normalization tool that generates for each non-standard token a set of candidates according to the phenomena observed most frequently in our corpus of tweets and SMS. Then, the normalization of the non-standard token is selected from all of these candidates, using a scoring system that also takes into account the immediate context of the processed token."
    ],
    "authFullName_s": [
      "Louise Tarrade"
    ],
    "halId_s": "dumas-01666146",
    "producedDateY_i": 2017,
    "texte_nettoye": "Le travail dont ce mémoire rend compte consistait à élaborer un outil de normalisation automatique des textes non standard en français, en particulier les tweets et les SMS. Pour cela, nous avons d’abord annoté un corpus de 1000 tweets et 1000 SMS, en fonction de phénomènes morpho-lexicaux et morpho-syntaxiques, que nous avions au préalable identifiés lors de l’élaboration d’une typologie pour l’annotation de textes non standard. À partir de l’observation de ce corpus, nous avons développé un outil de normalisation automatique qui génère pour chaque token non standard un ensemble de candidats en fonction des phénomènes observés le plus fréquemment dans notre corpus de tweets et de SMS. Ensuite, la normalisation du token non standard est sélectionnée parmi l’ensemble de ces candidats, à l’aide d’un système d’attribution de scores prenant également en compte le contexte immédiat du token traité."
  },
  {
    "title_s": [
      "Identification automatique des oronymes dans un corpus de récits d’exploration des Alpes"
    ],
    "keyword_s": [
      "Oronym",
      "NER",
      "Machine learning",
      "NLP",
      "BERT",
      "Oronyme",
      "NER",
      "Apprentissage automatique",
      "TAL",
      "BERT"
    ],
    "abstract_s": [
      "Dans cette tâche, nous avons élaboré un jeu de données en utilisant 1221 oronymes pour affiner les modèles avancés basés sur le Transformer tels que BERT. L'objectif principal est d'identifier les oronymes à partir de notre corpus spécialisé contenant 1,4 million de mots de récits d'explorations des Alpes. Nous avons ensuite utilisé ce jeu de données pour évaluer la performance de spaCy, un modèle général. Notre ambition est d'améliorer la précision de la reconnaissance pour les futurs projets de dénomination géophysique. De plus, nous souhaitons contribuer à l'étude et à l'application du traitement automatique de la langue, en particulier dans le domaine de la reconnaissance des entités nommées.",
      "In this task, we created a dataset using 1221 oronyms to fine-tune advanced Transformer-based models such as BERT, with the aim of identifying oronyms from our specialised corpus of 1.4 million words of Alps exploration narratives. The resultant dataset was also employed to evaluate spaCy, a general-purpose model. We strive to enhance the precision of identification for forthcoming projects in geophysical naming, as well as to support the study and application of natural language processing, particularly in the field of named entity recognition."
    ],
    "authFullName_s": [
      "Xiao Ma"
    ],
    "halId_s": "dumas-04260762",
    "producedDateY_i": 2023,
    "texte_nettoye": "Dans cette tâche, nous avons élaboré un jeu de données en utilisant 1221 oronymes pour affiner les modèles avancés basés sur le Transformer tels que BERT. L'objectif principal est d'identifier les oronymes à partir de notre corpus spécialisé contenant 1,4 million de mots de récits d'explorations des Alpes. Nous avons ensuite utilisé ce jeu de données pour évaluer la performance de spaCy, un modèle général. Notre ambition est d'améliorer la précision de la reconnaissance pour les futurs projets de dénomination géophysique. De plus, nous souhaitons contribuer à l'étude et à l'application du traitement automatique de la langue, en particulier dans le domaine de la reconnaissance des entités nommées."
  },
  {
    "title_s": [
      "Extraction d'expressions polylexicales sur corpus arboré"
    ],
    "keyword_s": [
      "Dependency parsing",
      "Treebank",
      "MWE",
      "Multiword expression",
      "Expression polylexicale",
      "Corpus arboré",
      "Grammaire de dépendances"
    ],
    "abstract_s": [
      "Ce document présente une méthode d'extraction d'expressions polylexicales à partir de corpus analysés syntaxiquement. Ces expressions restent un problème central pour le traitement automatique des langues naturelles, et leur extraction et encodage automatiques sont des tâches encore non résolues. L'approche implémentée permet en particulier d'extraire des expressions de plus de deux mots, et une attention particulière a été portée aux constructions récurrentes imbriquées. Une description morphosyntaxique fine des unités extraites est également générée, en termes de relations syntaxiques, ordres des mots possibles, distance entre mots, flexion ou détermination, informations qui nous semblent nécessaires à leur bonne intégration aux lexiques, pour des applications comme l'extraction d'information ou la traduction automatique.",
      "This document describes a method for extracting multiword expressions from syntactically analyzed corpora. These expressions are still a main issue for NLP applications. Their automatic extraction, and appropriate description, remain largely unsolved problems. This approach most notably allows the extraction of expressions composed of more than two words, and focuses on the issue of nested recurrent structures. It also yields a finegrained morphosyntactic description of the extracted units, including syntactic relations, possible word orders, contiguity, inflection or determination, which we think is necessary to a proper encoding of these expressions in lexicons, for applications such as information extraction or machine translation."
    ],
    "authFullName_s": [
      "Julien Corman"
    ],
    "halId_s": "dumas-00704873",
    "producedDateY_i": 2012,
    "texte_nettoye": "Ce document présente une méthode d'extraction d'expressions polylexicales à partir de corpus analysés syntaxiquement. Ces expressions restent un problème central pour le traitement automatique des langues naturelles, et leur extraction et encodage automatiques sont des tâches encore non résolues. L'approche implémentée permet en particulier d'extraire des expressions de plus de deux mots, et une attention particulière a été portée aux constructions récurrentes imbriquées. Une description morphosyntaxique fine des unités extraites est également générée, en termes de relations syntaxiques, ordres des mots possibles, distance entre mots, flexion ou détermination, informations qui nous semblent nécessaires à leur bonne intégration aux lexiques, pour des applications comme l'extraction d'information ou la traduction automatique."
  },
  {
    "title_s": [
      "Apport des informations visuelles dans la perception et la production du phonème non natif /v/ chez les apprenants thaïlandais"
    ],
    "keyword_s": [
      "V/ consonant",
      "Confusion between /v/ et /w",
      "Audiovisual speech presentation",
      "Speech processing",
      "Thai learners",
      "Traitement de la parole",
      "Apprenants thaïlandais",
      "Présentation audiovisuelle de la parole",
      "Confusion entre /v/ et /w",
      "Consonne /v"
    ],
    "abstract_s": [
      "Cette étude s’inscrivant dans le cadre de l’enseignement et de l’apprentissage d’une LE porte sur des difficultés récurrentes dans le traitement de la consonne non native /v/ en position initiale de syllabe chez les apprenants thaïlandais. Il a été démontré que cette consonne en position initiale de syllabe est très souvent confondue avec /w/. Nous avons effectué une étude expérimentale pour vérifier l’apport des informations audiovisuelles de la parole dans le traitement de la consonne /v/ chez les apprenants thaïlandais du français. Deux tâches (perception et production) ont été effectuées en deux modalités (modalités auditive et audiovisuelle). Les résultats montrent que le score de réussite en modalité audiovisuelle est significativement plus élevé qu’en modalité auditive, quel que soit le type de tâche. La confusion entre /v/ et /w/ est moins présente en modalité audiovisuelle qu’en modalité auditive. De plus, le temps de réponse en perception est significativement moins important en modalité audiovisuelle qu’en modalité auditive.",
      "This study within the framework of foreign language teaching and learning focuses on the recurrent difficulty in processing the non-native consonant /v/ in syllable-initial position among Thai learners. It has been shown that this consonant in syllable-initial position is very often confused with /w/. We therefore conducted an experimental study to verify the contribution of audiovisual speech information in the processing of the consonant /v/ in Thai learners of French. Two tasks (perception and production) were conducted, and two modalities (auditory and audiovisual modalities) were set up. The results show that the percentage of success in the audiovisual modality is significantly higher than that in the auditory modality, whatever the type of task. The confusion between /v/ and /w/ is less frequent in the audiovisual modality than in the auditory modality. Also, the response time in the audiovisual modality of the perception test is significantly less than that of the auditory modality."
    ],
    "authFullName_s": [
      "Supansa Tusnyingyong"
    ],
    "halId_s": "dumas-03794537",
    "producedDateY_i": 2022,
    "texte_nettoye": "Cette étude s’inscrivant dans le cadre de l’enseignement et de l’apprentissage d’une LE porte sur des difficultés récurrentes dans le traitement de la consonne non native /v/ en position initiale de syllabe chez les apprenants thaïlandais. Il a été démontré que cette consonne en position initiale de syllabe est très souvent confondue avec /w/. Nous avons effectué une étude expérimentale pour vérifier l’apport des informations audiovisuelles de la parole dans le traitement de la consonne /v/ chez les apprenants thaïlandais du français. Deux tâches (perception et production) ont été effectuées en deux modalités (modalités auditive et audiovisuelle). Les résultats montrent que le score de réussite en modalité audiovisuelle est significativement plus élevé qu’en modalité auditive, quel que soit le type de tâche. La confusion entre /v/ et /w/ est moins présente en modalité audiovisuelle qu’en modalité auditive. De plus, le temps de réponse en perception est significativement moins important en modalité audiovisuelle qu’en modalité auditive."
  },
  {
    "title_s": [
      "Étude des problèmes de prononciation des consonnes fricatives du français par des apprenants thaïlandais et propositions de correction phonétique"
    ],
    "keyword_s": [
      "System of automatic speech recognition Kaldi",
      "Thai phonetic",
      "French",
      "Pronunciation acquisition",
      "Thai learners",
      "Auditory discrimination",
      "Discrimination auditive",
      "Apprenant thaïlandais",
      "Français",
      "Thaï",
      "FLE",
      "Correction phonétique",
      "Système de reconnaissance automatique de parole Kaldi",
      "Appropriation de la prononciation",
      "Perception",
      "Production"
    ],
    "abstract_s": [
      "Cette recherche se situe dans le cadre des études contrastives. Son objectif est d’examiner et de confirmer l'existence des interférences des consonnes fricatives entre le français et le thaï ; ces interférences influencent en effet de façon assez importante la perception et la prononciation des consonnes fricatives du français chez les apprenants thaïlandais. À partir d’une analyse comparative entre les systèmes phonologiques des consonnes fricatives du français et du thaï, nous avons répertorié les principales difficultés de prononciation des consonnes fricatives chez les apprenants thaïlandais. Nous avons évalué ce phénomène à l’aide d’un test d’identification auprès d’un système de reconnaissance automatique de parole (Kaldi). Nous espérons que les analyses phonologiques des deux langues, les résultats de la recherche et les propositions de correction phonétique pour les apprenants thaïlandais pourraient aider les enseignants de français en Thaïlande à appliquer les méthodes adéquates face aux difficultés des Thaïlandais.",
      "This research is mainly related with contrastive studies. The objectives of this study are to examine the interference of Thai sounds in French fricative consonants. In fact, these interferences influence significantly perception and pronunciation of French fricatives among Thai students. From the analysis of French and Thai phonological systems regarding fricative consonants, the researcher could identify main difficulties of pronunciation of French fricatives faced by Thai students. This phenomenon was assessed by using an identification test with automatic speech recognition system (Kaldi). This study, at last, shows the result of the test and proposes possible phonetic corrections for Thai students. We hope that the comparative study of these two languages can help teachers of French as a foreign language (FLE) in Thailand to apply necessary methods in order to help students with their difficulties."
    ],
    "authFullName_s": [
      "Sakson Promkesa"
    ],
    "halId_s": "dumas-01078534",
    "producedDateY_i": 2014,
    "texte_nettoye": "Cette recherche se situe dans le cadre des études contrastives. Son objectif est d’examiner et de confirmer l'existence des interférences des consonnes fricatives entre le français et le thaï ; ces interférences influencent en effet de façon assez importante la perception et la prononciation des consonnes fricatives du français chez les apprenants thaïlandais. À partir d’une analyse comparative entre les systèmes phonologiques des consonnes fricatives du français et du thaï, nous avons répertorié les principales difficultés de prononciation des consonnes fricatives chez les apprenants thaïlandais. Nous avons évalué ce phénomène à l’aide d’un test d’identification auprès d’un système de reconnaissance automatique de parole (Kaldi). Nous espérons que les analyses phonologiques des deux langues, les résultats de la recherche et les propositions de correction phonétique pour les apprenants thaïlandais pourraient aider les enseignants de français en Thaïlande à appliquer les méthodes adéquates face aux difficultés des Thaïlandais."
  },
  {
    "title_s": [
      "Moteurs de recherche et restitution de l'information dans les grandes entreprises : l'exemple du portail Cyberthèque de la Direction des Systèmes d'Information de la Société Générale"
    ],
    "keyword_s": [
      "CLASSIFICATION AUTOMATIQUE",
      "RÉSUMÉ AUTOMATIQUE",
      "CATÉGORISATION AUTOMATIQUE",
      "ÉTIQUETAGE",
      "LEMMATISATION",
      "SEGMENTATION",
      "ANALYSE LINGUISTIQUE",
      "TALN",
      "ONTOLOGIE",
      "TOPIC MAPS",
      "TAXONOMIE",
      "THÉSAURUS",
      "LANGAGE DOCUMENTAIRE",
      "INDEXATION AUTOMATIQUE",
      "VERITY",
      "MOTEUR DE RECHERCHE"
    ],
    "abstract_s": [
      "Après avoir replacé les moteurs de recherche dans le contexte de la recherche d'information et des langages documentaires (notamment le thésaurus), compte tenu des mutations de ces dernières années (taxonomies, ontologies, Topic Maps), la première partie du mémoire se propose de décrire le fonctionnement de ces outils issus de la recherche en traitement automatique du langage (TALN). La définition du TALN, en soulignant les apports de chaque discipline impliquée avec un éclairage particulier sur la linguistique, est suivie d'une typologie des produits présents dans les grandes entreprises. Le fonctionnement des moteurs de recherche est décrit ensuite à travers les opérations effectuées par les moteurs de recherche linguistiques pour traiter la masse d'information textuelle lors de l'indexation. Cette description prend pour exemple un produit particulier : \"K2 Enterprise\" de la société Verity. La seconde partie retrace l'audit effectué afin de mettre en place des améliorations dans un portail d'entreprise dont la recherche est gérée par le moteur de recherche Verity K2 : la Cyberthèque de la Direction des Systèmes d'Information de la branche Banque de Détail de la Société Générale, portail de veille technologique et concurrentielle."
    ],
    "authFullName_s": [
      "Alina Ivanciuc Deniau"
    ],
    "halId_s": "mem_00000013",
    "producedDateY_i": 2003,
    "texte_nettoye": "Après avoir replacé les moteurs de recherche dans le contexte de la recherche d'information et des langages documentaires (notamment le thésaurus), compte tenu des mutations de ces dernières années (taxonomies, ontologies, Topic Maps), la première partie du mémoire se propose de décrire le fonctionnement de ces outils issus de la recherche en traitement automatique du langage (TALN). La définition du TALN, en soulignant les apports de chaque discipline impliquée avec un éclairage particulier sur la linguistique, est suivie d'une typologie des produits présents dans les grandes entreprises. Le fonctionnement des moteurs de recherche est décrit ensuite à travers les opérations effectuées par les moteurs de recherche linguistiques pour traiter la masse d'information textuelle lors de l'indexation. Cette description prend pour exemple un produit particulier : \"K2 Enterprise\" de la société Verity. La seconde partie retrace l'audit effectué afin de mettre en place des améliorations dans un portail d'entreprise dont la recherche est gérée par le moteur de recherche Verity K2 : la Cyberthèque de la Direction des Systèmes d'Information de la branche Banque de Détail de la Société Générale, portail de veille technologique et concurrentielle."
  },
  {
    "title_s": [
      "Conception et développement d'un module de traitement des lemmes comportant des lettres dérivatives"
    ],
    "keyword_s": [
      "Silent letters",
      "Derivational letters",
      "NLP",
      "Spelling",
      "TAL",
      "Lettres dérivatives",
      "Lettres muettes",
      "Orthographe"
    ],
    "abstract_s": [
      "L'une des difficultés les plus fréquentes lors de l'apprentissage du français est l'orthographe des lettres muettes qui sont très présentes à l'écrit. En effet, la majorité des mots en français se terminent par une lettre muette. Ces lettres muettes sont principalement dues à la morphologie dérivationnelle et marquent l'appartenance à une même famille de mots (le t de 'chat' dans 'chaton'). Ce mémoire présente le processus de la conception d'un outil de traitement automatique du langage permettant de détecter automatiquement ces lettres dérivatives. Cet outil permettra ensuite d'étudier les performances des élèves face aux lettres muettes selon leur nature dérivationnelle ou non.",
      "The spelling of silent letters is one of the most common difficulties for French learners. Moreover these silent letters are very common in French. Indeed, most of the words in French end with a silent letter. These silent letters are mainly due to derivational morphology and specify the belonging of the same semantic and morphological family (the t in 'chat' in 'chaton'). This master thesis introduces the process of designing a natural language processing tool to detect these derivational letters. This tool will then be used to study students' skills beside silent letters according to their derivational or non-derivational nature."
    ],
    "authFullName_s": [
      "Cynthia Rakotoarisoa"
    ],
    "halId_s": "dumas-03485502",
    "producedDateY_i": 2021,
    "texte_nettoye": "L'une des difficultés les plus fréquentes lors de l'apprentissage du français est l'orthographe des lettres muettes qui sont très présentes à l'écrit. En effet, la majorité des mots en français se terminent par une lettre muette. Ces lettres muettes sont principalement dues à la morphologie dérivationnelle et marquent l'appartenance à une même famille de mots (le t de 'chat' dans 'chaton'). Ce mémoire présente le processus de la conception d'un outil de traitement automatique du langage permettant de détecter automatiquement ces lettres dérivatives. Cet outil permettra ensuite d'étudier les performances des élèves face aux lettres muettes selon leur nature dérivationnelle ou non."
  },
  {
    "title_s": [
      "Analyse syntaxique automatique d'un corpus du français médiéval"
    ],
    "keyword_s": [
      "Part-of-speech tagging",
      "Phraseological units",
      "Parsing",
      "NLP",
      "Étiquetage morphosyntaxique",
      "Unités phraséologiques",
      "Analyse syntaxique",
      "TAL"
    ],
    "abstract_s": [
      "Pour explorer le rôle de la phraséologie étendue dans la structuration des genres textuels littéraires en français médiéval, les techniques de TAL et la linguistique de corpus prennent une part décisive, car les corpus informatisés et les programmes qui les utilisent permettent d'identifier les unités phraséologiques. Dans cet article, nous présentons un travail de traitement du corpus à l'aide de LGeRM, une plateforme de lemmatisation du français médiéval et de l'analyseur syntaxique Hops, incluant le traitement de lemmatiser, étiqueter et analyser en dépendances syntaxiques. Cet article détaille également comment le corpus traité peut être intégré dans le Lexicoscope, l'outil de fouille textuelle. L'objectif est de pouvoir constituer le corpus sur le Lexicoscope et d'utiliser les fonctionnalités de cet outil pour mener des recherches sur l'exploration des liens entre la phraséologie étendue et les genres textuels en français médiéval.",
      "For explore the role of extended phraseology in the structuring of literary textual genres in medieval French, NLP techniques and corpus linguistics play a decisive role, as computerized corpus and the programs allow the identification of phraseological units. In this article, we present a corpus processing work using LGeRM, a lemmatization platform for medieval French, and the syntactic parser Hops, including lemmatization, tagging and syntactic dependency parsing. This article also details how the processed corpus can be integrated into the Lexicoscope, tool of text mining. The aim is to be able to build the corpus on the Lexicoscope and to use the functionalities of this tool to conduct research on the exploration of the links between extended phraseology and textual genres in medieval French."
    ],
    "authFullName_s": [
      "Jingyu Liu"
    ],
    "halId_s": "dumas-03485357",
    "producedDateY_i": 2021,
    "texte_nettoye": "Pour explorer le rôle de la phraséologie étendue dans la structuration des genres textuels littéraires en français médiéval, les techniques de TAL et la linguistique de corpus prennent une part décisive, car les corpus informatisés et les programmes qui les utilisent permettent d'identifier les unités phraséologiques. Dans cet article, nous présentons un travail de traitement du corpus à l'aide de LGeRM, une plateforme de lemmatisation du français médiéval et de l'analyseur syntaxique Hops, incluant le traitement de lemmatiser, étiqueter et analyser en dépendances syntaxiques. Cet article détaille également comment le corpus traité peut être intégré dans le Lexicoscope, l'outil de fouille textuelle. L'objectif est de pouvoir constituer le corpus sur le Lexicoscope et d'utiliser les fonctionnalités de cet outil pour mener des recherches sur l'exploration des liens entre la phraséologie étendue et les genres textuels en français médiéval."
  },
  {
    "title_s": [
      "Création d’une intelligence artificielle capable de détecter les impacts de qualité de vie liée à la santé à partir de données de vie réelle",
      "Creating an artificial intelligence capable of detecting health-related quality of life impacts from real-life data"
    ],
    "keyword_s": [
      "Intelligence artificielle en médecine",
      "Patients -- Satisfaction -- Évaluation",
      "Réseaux sociaux Internet",
      "Traitement automatique du langage naturel"
    ],
    "abstract_s": [
      "L’objet de ce travail était de réaliser un algorithme d’Intelligence artificielle, ou plus précisément, un modèle de traitement automatisé du langage, capable d’identifier dans des témoignages libres de patients, les verbatims reflétant un impact de qualité de vie. Il existe une synergie entre le patient centrisme et l’essor des données de vie réelle. Ensemble de données médicalement contextualisées, elles reflètent et objectivent la réalité patiente, par définition non captée lors d'essais cliniques. Sur les réseaux sociaux et forums médicaux, les patients forment des communautés en ligne dans un but de soutien, d’information et de partage d’expériences médicales. Ces commentaires publics sont récupérables informatiquement, dans le respect des réglementations pour la protection des données. C’est là que le traitement automatisé du langage entre en jeu. En étant capable d’analyser et de traduire le langage patient en ontologie médicale, il devient possible de gagner en information sur la réalité patiente, telle que décrite directement et sans filtre par eux-mêmes. Ainsi, notre algorithme répond à l’enjeu de compréhension fine de ce qui peut réellement impacter la qualité de vie des patients, en vie réelle.",
      "The purpose of this work was to develop an artificial intelligence algorithm, or more precisely, an automated language processing model, capable of identifying in free patient testimonies, the verbatims reflecting an impact on quality of life. There is a synergy between patient centrism and the rise of real-life data. Medically contextualized data sets reflect and objectify patient reality, which by definition is not captured in clinical trials. On social networks and medical forums, patients form online communities for support, information and sharing of medical experiences. These public comments can be retrieved electronically, in compliance with data protection regulations. This is where automated language processing comes in. By being able to analyze and translate patient language into medical ontology, it becomes possible to gain information about patient reality, as described directly and unfiltered by themselves. Thus, our algorithm meets the challenge of fine understanding of what can really impact the quality of life of patients, in real life."
    ],
    "authFullName_s": [
      "Tom Marty"
    ],
    "halId_s": "dumas-03956122",
    "producedDateY_i": 2022,
    "texte_nettoye": "L’objet de ce travail était de réaliser un algorithme d’Intelligence artificielle, ou plus précisément, un modèle de traitement automatisé du langage, capable d’identifier dans des témoignages libres de patients, les verbatims reflétant un impact de qualité de vie. Il existe une synergie entre le patient centrisme et l’essor des données de vie réelle. Ensemble de données médicalement contextualisées, elles reflètent et objectivent la réalité patiente, par définition non captée lors d'essais cliniques. Sur les réseaux sociaux et forums médicaux, les patients forment des communautés en ligne dans un but de soutien, d’information et de partage d’expériences médicales. Ces commentaires publics sont récupérables informatiquement, dans le respect des réglementations pour la protection des données. C’est là que le traitement automatisé du langage entre en jeu. En étant capable d’analyser et de traduire le langage patient en ontologie médicale, il devient possible de gagner en information sur la réalité patiente, telle que décrite directement et sans filtre par eux-mêmes. Ainsi, notre algorithme répond à l’enjeu de compréhension fine de ce qui peut réellement impacter la qualité de vie des patients, en vie réelle."
  },
  {
    "title_s": [
      "Étude des modèles neuronaux profonds pour la compréhension automatique du langage naturel dans les habitats intelligents"
    ],
    "keyword_s": [
      "Corpus generation",
      "Natural language understanding",
      "Deep learning",
      "Machine learning",
      "Smart home",
      "Génération du corpus",
      "Modèles profonds",
      "Apprentissage automatique",
      "Habitat intelligent",
      "Compréhension automatique de la langue"
    ],
    "abstract_s": [
      "Le but du projet VocADom est de créer un système de compréhension automatique des ordres vocaux qui pourrait être fonctionnel dans un habitat intelligent pour les personnes âgées. L’approche par apprentissage automatique nous semble plus flexible que l’approche par règles, ce dernier ne prenant pas en compte les énoncés qui ne se conforment pas à la grammaire ; et les études ont montré que les personnes âgées sont enclines à s’écarter de la grammaire imposée. Pourtant il y a un manque de données d’apprentissage dans ce domaine. Pour contourner ce problème, une grammaire générant le corpus artificiel pour le domaine de l’habitat intelligent a été créé. Un des problèmes traités par ce mémoire est l’amélioration de la grammaire existante. Après amélioration, le corpus artificiel résultant compte 42195 phrases annotées. Il a été ensuite évalué en entraînant avec lui trois modèles états de l’art - Tri-CRF, att-RNN, RASA - ainsi que Tf-seq2seq, modèle séquence-vers-séquence. Ces modèles ont ensuite été testés sur un petit corpus d’ordres naturels enregistrés dans le cadre du projet VocADom. Cela nous a permis de voir les limites du corpus artificiel. Ces modèles ont ensuite été comparés aux modèles entraînés et testés sur le corpus de données réelles PORTMEDIA qui relève du domaine du tourisme. Cette comparaison nous a permis de savoir que la performance des modèles entraînés sur le corpus artificiel est due aux limitations du corpus artificiel plutôt qu’aux limitations des modèles eux-mêmes. Tous les modèles entraînés et testés sur PORTMEDIA avaient rendu de bons résultats mais les modèles entraînés sur le corpus artificiel et testés sur le corpus naturel VocADom ont été bien moins performants. Ceci montre la difficulté à prendre en charge la diversité lexicale et syntaxique d’un corpus réel. Tous les modèles ont des performances comparables, mais Tf-seq2seq, contrairement à Tri-CRF, att-RNN et RASA, ne requiert pas de données alignées et est plus facile à entraîner.",
      "The goal of the VocADom project is to create a natural language understanding system for processing voice orders in a smart home for the elderly. Machine learning approach seems more flexible than the rule-based approach, which does not take into account orders that do not conform to grammar ; and studies have shown that older people are inclined to deviate from imposed grammar. However there is a lack of learning data in this domain. To remedy the problem, a grammar generating the artificial corpus for the smart home domain was created. One of the goals of this work is to improve this grammar. After the improvement, the resulting artificial corpus has 42195 annotated sentences. It was then evaluated by training three state-of-the-art models on it - Tri-CRF, att-RNN, RASA, as well as Tf-seq2seq, a sequence-to-sequence model. The models were then tested on a small natural existing corpus recorded as part of the VocADom project. This allowed us to see the limitations of the artificial corpus. These models were then compared to the models that were trained and tested on the PORTMEDIA corpus of touristic domain. This comparison allowed us to know that the performance of the models trained on the artificial corpus is due to the limitations of the artificial corpus rather than the limitations of the models themselves. All the models trained and tested on PORTMEDIA gave good results but the models trained on the artificial corpus and tested on the VocADom corpus were much less efficient. This shows the difficulty of accounting for the lexical and syntactic diversity of the real data. All models show comparable performances but Tf-seq2seq, unlike Tri-CRF, att-RNN et RASA, doesn’t require aligned data and is easier to train."
    ],
    "authFullName_s": [
      "Anastasiia Mishakova"
    ],
    "halId_s": "dumas-01846913",
    "producedDateY_i": 2018,
    "texte_nettoye": "Le but du projet VocADom est de créer un système de compréhension automatique des ordres vocaux qui pourrait être fonctionnel dans un habitat intelligent pour les personnes âgées. L’approche par apprentissage automatique nous semble plus flexible que l’approche par règles, ce dernier ne prenant pas en compte les énoncés qui ne se conforment pas à la grammaire ; et les études ont montré que les personnes âgées sont enclines à s’écarter de la grammaire imposée. Pourtant il y a un manque de données d’apprentissage dans ce domaine. Pour contourner ce problème, une grammaire générant le corpus artificiel pour le domaine de l’habitat intelligent a été créé. Un des problèmes traités par ce mémoire est l’amélioration de la grammaire existante. Après amélioration, le corpus artificiel résultant compte 42195 phrases annotées. Il a été ensuite évalué en entraînant avec lui trois modèles états de l’art - Tri-CRF, att-RNN, RASA - ainsi que Tf-seq2seq, modèle séquence-vers-séquence. Ces modèles ont ensuite été testés sur un petit corpus d’ordres naturels enregistrés dans le cadre du projet VocADom. Cela nous a permis de voir les limites du corpus artificiel. Ces modèles ont ensuite été comparés aux modèles entraînés et testés sur le corpus de données réelles PORTMEDIA qui relève du domaine du tourisme. Cette comparaison nous a permis de savoir que la performance des modèles entraînés sur le corpus artificiel est due aux limitations du corpus artificiel plutôt qu’aux limitations des modèles eux-mêmes. Tous les modèles entraînés et testés sur PORTMEDIA avaient rendu de bons résultats mais les modèles entraînés sur le corpus artificiel et testés sur le corpus naturel VocADom ont été bien moins performants. Ceci montre la difficulté à prendre en charge la diversité lexicale et syntaxique d’un corpus réel. Tous les modèles ont des performances comparables, mais Tf-seq2seq, contrairement à Tri-CRF, att-RNN et RASA, ne requiert pas de données alignées et est plus facile à entraîner."
  },
  {
    "title_s": [
      "Développement d'un module de génération de paraphrases pour la data augmentation"
    ],
    "keyword_s": [
      "Semantics",
      "Syntax",
      "Natural langage processing",
      "Paraphrase",
      "Data augmentation",
      "Sémantique",
      "Syntaxe",
      "Data augmentation",
      "Traitement automatique de la langue",
      "Paraphrase"
    ],
    "abstract_s": [
      "Ce document résume 6 mois de travail au sein de l’équipe Recherche et Développement de Linagora Toulouse. Le but de ce projet était de développer un module de paraphrase permettant d’enrichir le corpus d’apprentissage de l’agent conversationnel LinTO. Nous avons commencé par analyser les différentes commandes présentes dans notre corpus initial : les structures syntaxiques récurrentes et les mécanismes de paraphrasage qu’on peut leur appliquer. À partir de ces observations, nous avons créé une grammaire à base de règles pour générer plusieurs paraphrases d’une commande en entrée.",
      "This paper summarizes the work completed during a six-month internship in the Research & Development team of Linagora. The aim of this project is to develop a paraphrasing tool able to expand the training datasets of the smart vocal assistant LinTO. We started by analyzing the existing commands in the original corpus : the recurrent syntactic structures and the paraphrasing mecanisms that can be applied to them. From these observations, we created a rule-based grammar to generate semantically and synctactically correct sentences."
    ],
    "authFullName_s": [
      "Sonia Ratsiandavana"
    ],
    "halId_s": "dumas-02294883",
    "producedDateY_i": 2019,
    "texte_nettoye": "Ce document résume 6 mois de travail au sein de l’équipe Recherche et Développement de Linagora Toulouse. Le but de ce projet était de développer un module de paraphrase permettant d’enrichir le corpus d’apprentissage de l’agent conversationnel LinTO. Nous avons commencé par analyser les différentes commandes présentes dans notre corpus initial : les structures syntaxiques récurrentes et les mécanismes de paraphrasage qu’on peut leur appliquer. À partir de ces observations, nous avons créé une grammaire à base de règles pour générer plusieurs paraphrases d’une commande en entrée."
  },
  {
    "title_s": [
      "<i>DeCorScol</i> : conception d'un outil d'assistance à l'annotation des chaînes de coréférence dans les écrits scolaires"
    ],
    "keyword_s": [
      "Cohésion",
      "Cohérence",
      "Traitement automatique des langues",
      "Coréférence",
      "Écrits scolaires"
    ],
    "abstract_s": [
      "Quelles sont les qualités qui distinguent un texte « bien écrit » d’un texte « mal écrit », « incompréhensible », « peu ou pas cohérent » ? Divers facteurs entrent en jeu lors de la compréhension (et écriture) d’un texte sur plusieurs plans langagiers. Entre les différents éléments qui rendent un texte cohérent et cohésif, les chaînes de coréférence jouent sans doute un rôle de premier plan. Elles permettent aux lecteurs de suivre le cheminement des personnages impliqués dans les actions évoquées par le récit et, aux scripteurs plus avancés, d’attribuer des qualités à leurs personnages, ainsi que de les faire agir dans l’univers fictionnel qu’ils décrivent. Cependant, comment se réalise la coréférence dans les écrits scolaires et comment est-il possible de la décrire de manière à fournir des critères d’évaluation objectifs aux enseignants ? L’un des buts de ce travail est de fournir un outil TAL d’assistance à l’annotation de ce phénomène, de manière à contribuer à cette « cartographie de l’emploi des formes linguistiques qui déterminent la cohérence et la cohésion textuelles » déjà initiée en France (Garcia-Debanc et al., 2021). Nous allons introduire dans le chapitre 1 la notion de coréférence en linguistique et en Traitement Automatique des Langues. Nous décrirons ensuite le domaine des corpus scolaires dans le chapitre 2 puis nous aborderons les approches utilisées pour l’annotation de la coréférence sur des corpus variés dans le chapitre 3. La question de la coréférence et de l’annotation de la continuité référentielle dans les corpus scolaires sera traitée dans le chapitre 4. Depuis les premières recherches en TAL sur l’écriture des apprenants dans les années 80, ce sont des questions didactiques qui pilotent les travaux linguistiques sur les textes d’élèves (Doquet et Ponton, 2021). En nous positionnant dans cette tradition, nous souhaitons ici pouvoir répondre à certains questionnements partagés avec la didactique, en recourant au développement et à l’application d’un outil de Traitement Automatique des Langues. Celui-ci servira d’aide à l’annotation des chaînes de coréférence sur les données du corpus scolaire Scoledit (CE2). Cet outil, nommé <i>DeCorScol</i> (<i>Détection des Coréférences sur les écrits Scolaires</i>), a comme but d’offrir une aide automatisée pour l’annotation des chaînes de coréférences dans le cadre du corpus Scoledit. La méthodologie de travail que nous avons adoptée sur le corpus Scoledit sera présentée dans le chapitre 5. Nous décrirons ensuite l’architecture de l’outil dans le chapitre 6. Cet outil nous a déjà permis d’effectuer des tests d’annotation. Sur la base de ces données, nous avons pu tirer nos premières réflexions que nous présenterons plus en détail dans le chapitre 7. Elles portent à la fois sur les configurations des chaînes de coréférence dans les écrits scolaires de niveau CE2 et sur la manière dont la modélisation linguistique et le TAL nous permettent de cerner cette configuration à savoir : 1. la nature facultative des ressources sémantiques pour la détection des mentions nominales dans des productions de CE2, donc l’observation d’une variété lexicale limitée dans ces types de mentions. 2. le postulat d’une prépondérance de continuité thématique dans la construction des récits par la part des élèves ; 3. la productivité de certaines règles déjà exploitées pour la détection de coréférences dans des textes rédigés par des scripteurs experts (comme les verbes pronominaux) et l’absence d’autre structures syntaxiques plus complexes, également exploitées afin de résoudre la coréférence dans des textes de scripteurs experts, dans les écrits scolaires du niveau objet de notre analyse. L’outil que nous proposons est fonctionnel pour l’étude du développement des notions de textualité et de cohérence textuelle tout au long du parcours d’apprentissage de l’écriture chez l’enfant. Nous prévoyons effectivement que cet outil puisse être davantage utilisé sur les niveaux restants du corpus objet de nos études, de manière à fournir par la suite une cartographie longitudinale des éléments qui composent la notion de coréférence dans les écrits scolaires de ces niveaux."
    ],
    "authFullName_s": [
      "Martina Barletta"
    ],
    "halId_s": "dumas-03826763",
    "producedDateY_i": 2022,
    "texte_nettoye": "Quelles sont les qualités qui distinguent un texte « bien écrit » d’un texte « mal écrit », « incompréhensible », « peu ou pas cohérent » ? Divers facteurs entrent en jeu lors de la compréhension (et écriture) d’un texte sur plusieurs plans langagiers. Entre les différents éléments qui rendent un texte cohérent et cohésif, les chaînes de coréférence jouent sans doute un rôle de premier plan. Elles permettent aux lecteurs de suivre le cheminement des personnages impliqués dans les actions évoquées par le récit et, aux scripteurs plus avancés, d’attribuer des qualités à leurs personnages, ainsi que de les faire agir dans l’univers fictionnel qu’ils décrivent. Cependant, comment se réalise la coréférence dans les écrits scolaires et comment est-il possible de la décrire de manière à fournir des critères d’évaluation objectifs aux enseignants ? L’un des buts de ce travail est de fournir un outil TAL d’assistance à l’annotation de ce phénomène, de manière à contribuer à cette « cartographie de l’emploi des formes linguistiques qui déterminent la cohérence et la cohésion textuelles » déjà initiée en France (Garcia-Debanc et al., 2021). Nous allons introduire dans le chapitre 1 la notion de coréférence en linguistique et en Traitement Automatique des Langues. Nous décrirons ensuite le domaine des corpus scolaires dans le chapitre 2 puis nous aborderons les approches utilisées pour l’annotation de la coréférence sur des corpus variés dans le chapitre 3. La question de la coréférence et de l’annotation de la continuité référentielle dans les corpus scolaires sera traitée dans le chapitre 4. Depuis les premières recherches en TAL sur l’écriture des apprenants dans les années 80, ce sont des questions didactiques qui pilotent les travaux linguistiques sur les textes d’élèves (Doquet et Ponton, 2021). En nous positionnant dans cette tradition, nous souhaitons ici pouvoir répondre à certains questionnements partagés avec la didactique, en recourant au développement et à l’application d’un outil de Traitement Automatique des Langues. Celui-ci servira d’aide à l’annotation des chaînes de coréférence sur les données du corpus scolaire Scoledit (CE2). Cet outil, nommé <i>DeCorScol</i> (<i>Détection des Coréférences sur les écrits Scolaires</i>), a comme but d’offrir une aide automatisée pour l’annotation des chaînes de coréférences dans le cadre du corpus Scoledit. La méthodologie de travail que nous avons adoptée sur le corpus Scoledit sera présentée dans le chapitre 5. Nous décrirons ensuite l’architecture de l’outil dans le chapitre 6. Cet outil nous a déjà permis d’effectuer des tests d’annotation. Sur la base de ces données, nous avons pu tirer nos premières réflexions que nous présenterons plus en détail dans le chapitre 7. Elles portent à la fois sur les configurations des chaînes de coréférence dans les écrits scolaires de niveau CE2 et sur la manière dont la modélisation linguistique et le TAL nous permettent de cerner cette configuration à savoir : 1. la nature facultative des ressources sémantiques pour la détection des mentions nominales dans des productions de CE2, donc l’observation d’une variété lexicale limitée dans ces types de mentions. 2. le postulat d’une prépondérance de continuité thématique dans la construction des récits par la part des élèves ; 3. la productivité de certaines règles déjà exploitées pour la détection de coréférences dans des textes rédigés par des scripteurs experts (comme les verbes pronominaux) et l’absence d’autre structures syntaxiques plus complexes, également exploitées afin de résoudre la coréférence dans des textes de scripteurs experts, dans les écrits scolaires du niveau objet de notre analyse. L’outil que nous proposons est fonctionnel pour l’étude du développement des notions de textualité et de cohérence textuelle tout au long du parcours d’apprentissage de l’écriture chez l’enfant. Nous prévoyons effectivement que cet outil puisse être davantage utilisé sur les niveaux restants du corpus objet de nos études, de manière à fournir par la suite une cartographie longitudinale des éléments qui composent la notion de coréférence dans les écrits scolaires de ces niveaux."
  },
  {
    "title_s": [
      "Outiller la description de la morphologie adjectivale du primaire à l'université"
    ],
    "keyword_s": [
      "Inflectional morphology",
      "Linguistic modeling",
      "NLP Natural Language Processing",
      "Spelling",
      "TAL Traitement Automatique du Langage",
      "Modélisation linguistique",
      "Morphologie flexionnelle",
      "Orthographe"
    ],
    "abstract_s": [
      "En ce qui concerne les compétences orthographiques des élèves, il n’existe pas de réelle description à cause du manque de données. Le projet E-Calm dans lequel s’inscrit ce mémoire cherche donc à rassembler un ensemble de ressources pour chaque niveau scolaire afin de pouvoir se lancer dans des analyses linguistiques à des fins didactiques, et grâce à la mise en place d’outils de TAL. En effet, la quantité de données récoltée ne permet pas les analyses manuelles. Le travail décrit ici est centré sur les aptitudes des apprenants concernant la morphologie adjectivale. Dans un premier temps, ce mémoire propose une modélisation linguistique du comportement morphologique flexionnel de l’adjectif en se basant sur des productions d’élèves et d’étudiants. Dans un second temps, il s’intéresse à la conception d’un outil permettant une réalisation automatique de cette modélisation et enfin à l’analyse des résultats du système présenté.",
      "Concerning french pupils an students’ spelling skills, there is no real description due to a lack of data. The E-Calm project in which this dissertation takes part, therefore, seeks to bring together a set of resources for each school level in order to be able to undertake linguistic analyses for teaching purposes. This is made possible thanks to the implementation of NLP tools; indeed, the amount of data collected does not allow manual analyses. The work described here is focused on the learners' abilities concerning adjectival morphology. Initially, this masters’ thesis proposes a linguistic modeling of the adjective's inflectional morphological behavior based on students’ productions. In a second step, it focuses on the design of a tool allowing an automatic realization of this modeling and finally on the analysis of the results of the presented system."
    ],
    "authFullName_s": [
      "Rachel Gaubil"
    ],
    "halId_s": "dumas-02978282",
    "producedDateY_i": 2020,
    "texte_nettoye": "En ce qui concerne les compétences orthographiques des élèves, il n’existe pas de réelle description à cause du manque de données. Le projet E-Calm dans lequel s’inscrit ce mémoire cherche donc à rassembler un ensemble de ressources pour chaque niveau scolaire afin de pouvoir se lancer dans des analyses linguistiques à des fins didactiques, et grâce à la mise en place d’outils de TAL. En effet, la quantité de données récoltée ne permet pas les analyses manuelles. Le travail décrit ici est centré sur les aptitudes des apprenants concernant la morphologie adjectivale. Dans un premier temps, ce mémoire propose une modélisation linguistique du comportement morphologique flexionnel de l’adjectif en se basant sur des productions d’élèves et d’étudiants. Dans un second temps, il s’intéresse à la conception d’un outil permettant une réalisation automatique de cette modélisation et enfin à l’analyse des résultats du système présenté."
  },
  {
    "title_s": [
      "E-Lexic : Extraction lexicale à partir de textes bilingues alignés"
    ],
    "keyword_s": [
      "Word alignment",
      "Word extractor",
      "Dictionnary",
      "Translation",
      "Dictionnaire",
      "Extracteur de mots",
      "Alignement lexical",
      "Traduction"
    ],
    "abstract_s": [
      "On a vu naître, avec l’émergence de nombreux outils de traitement automatique des langues, de nombreux domaines en linguistique computationnelle. Parmi eux on retrouve la traduction assistée par ordinateur. Dans ce sous-domaine de la traduction, on a là encore observé l’apparition de différents outils numériques comme la mémoire de traduction ou ce qui nous intéressera dans ce mémoire, les aligneurs syntagmatiques et lexicaux. Ce mémoire rend compte d’un travail fourni lors du développement d’un extracteur de mots basé sur l’alignement lexical de textes bilingues. L’objectif étant la création grâce à cette extraction d’un dictionnaire bilingue spécifique au texte traité consultable par un utilisateur.",
      "With the emergence of numerous automatic language processing tools, many fields in computational linguistics have been brought to light. Among them we can find computerassisted translation. In this sub-domain of translation, we have again observed the appearance of various digital tools such as translation memory or, what will interest us in this thesis, syntactic and lexical aligners. This dissertation reports on work done in the development of a word extractor based on the lexical alignment of bilingual texts. The objective being the creation, thanks to this extraction, of a bilingual dictionary. This dictionary will be specific to the text that is being processed and can be consulted by a user."
    ],
    "authFullName_s": [
      "Florent Marié"
    ],
    "halId_s": "dumas-02978581",
    "producedDateY_i": 2020,
    "texte_nettoye": "On a vu naître, avec l’émergence de nombreux outils de traitement automatique des langues, de nombreux domaines en linguistique computationnelle. Parmi eux on retrouve la traduction assistée par ordinateur. Dans ce sous-domaine de la traduction, on a là encore observé l’apparition de différents outils numériques comme la mémoire de traduction ou ce qui nous intéressera dans ce mémoire, les aligneurs syntagmatiques et lexicaux. Ce mémoire rend compte d’un travail fourni lors du développement d’un extracteur de mots basé sur l’alignement lexical de textes bilingues. L’objectif étant la création grâce à cette extraction d’un dictionnaire bilingue spécifique au texte traité consultable par un utilisateur."
  },
  {
    "title_s": [
      "Découverte non supervisée de lexique à partir d'un corpus multimodal pour la documentation des langues en danger"
    ],
    "keyword_s": [
      "Endangered languages",
      "Documentation",
      "UTD",
      "Unsupervised term discovery",
      "Multimodal corpus",
      "NLP",
      "Langues en danger",
      "Lexique",
      "Découverte non supervisée",
      "Corpus multimodal",
      "TAL"
    ],
    "abstract_s": [
      "De nombreuses langues disparaissent tous les ans et ce à un rythme jamais atteint auparavant. Les linguistes de terrain manquent de temps et de moyens afin de pouvoir toutes les documenter et décrire avant qu’elles ne disparaissent à jamais. L’objectif de notre travail est donc de les aider dans leur tâche en facilitant le traitement des données. Nous proposons dans ce mémoire des méthodes d’extraction non supervisées de lexique à partir de corpus multimodaux incluant des signaux de parole et des images. Nous proposons également une méthode issue de la recherche d’information afin d’émettre des hypothèses de signification sur les éléments lexicaux découverts. Ce mémoire présente en premier lieu la constitution d’un corpus multimodal parole-image de grande taille. Ce corpus simulant une langue en danger permet ainsi de tester les approches computationnelles de découverte non supervisée de lexique. Dans une seconde partie, nous appliquons un algorithme de découverte non supervisée de lexique utilisant de l’alignement dynamique temporel segmental (S-DTW) sur un corpus multimodal synthétique de grande taille ainsi que sur un corpus multimodal d’une vraie langue en danger, le Mboshi.",
      "Many languages are on the brink of extinction and many disappear each and every year at a rate never seen before. Field linguists lack the time and the means to document and describe all of them before they die out. The goal of our work is to help them in their task, make it easier and speed up the data processing and annotation tasks. In this dissertation, we propose methods to use an unsupervised term discovery (UTD) system to extract lexicon from multimodal corpora consisting of speech and images. We also propose a method using information retrieval techniques to hypothesise the meaning of the discovered lexical items. In the first place, this dissertation presents the creation of a large multimodal corpus which includes speech and images. This corpus simulating that of an endangered language will allow us evaluate the performances of an unsupervised term discovery system. In the second place, we apply an unsupervised term discovery system based on segmental dynamic time warping (S-DTW) to a large synthetic multimodal corpus and also to the multimodal corpus of a real endangered language called Mboshi, spoken in Congo-Brazzaville."
    ],
    "authFullName_s": [
      "William N Havard"
    ],
    "halId_s": "dumas-01562024",
    "producedDateY_i": 2017,
    "texte_nettoye": "De nombreuses langues disparaissent tous les ans et ce à un rythme jamais atteint auparavant. Les linguistes de terrain manquent de temps et de moyens afin de pouvoir toutes les documenter et décrire avant qu’elles ne disparaissent à jamais. L’objectif de notre travail est donc de les aider dans leur tâche en facilitant le traitement des données. Nous proposons dans ce mémoire des méthodes d’extraction non supervisées de lexique à partir de corpus multimodaux incluant des signaux de parole et des images. Nous proposons également une méthode issue de la recherche d’information afin d’émettre des hypothèses de signification sur les éléments lexicaux découverts. Ce mémoire présente en premier lieu la constitution d’un corpus multimodal parole-image de grande taille. Ce corpus simulant une langue en danger permet ainsi de tester les approches computationnelles de découverte non supervisée de lexique. Dans une seconde partie, nous appliquons un algorithme de découverte non supervisée de lexique utilisant de l’alignement dynamique temporel segmental (S-DTW) sur un corpus multimodal synthétique de grande taille ainsi que sur un corpus multimodal d’une vraie langue en danger, le Mboshi."
  },
  {
    "title_s": [
      "« Qu'est ce qu'il dit ? » : analyse de marqueurs audiovisuels de l'incompréhension"
    ],
    "keyword_s": [
      "Incomprehension",
      "Nonverbal",
      "Back-channel",
      "Non-verbal",
      "Rétroaction",
      "Dialogue"
    ],
    "abstract_s": [
      "Ce mémoire porte sur les signaux verbaux et non verbaux qu'émet le récepteur d'un message lors d'un dialogue face à face lorsque qu'il est placé en situation d'incompréhension. Cette recherche se concentre sur deux aspects : Quels sont parmi les signaux émis, ceux auto-adressés et ceux à but communicatif et est ce que le degré d'incompréhension a une influence sur le type de signal produit. Cette recherche a mené à la constitution d'un corpus audio-visuel annoté avec les comportements de quinze sujets. Des analyses statistiques effectuées sur ces annotations ont permis d'isoler neuf comportements relatifs à l'incompréhension. À partir des comportements faciaux isolés nous avons pu extraire un ensemble de paramètres grâce à un outil de détection de mouvements faciaux pour identifier des paramètres pertinents dans l’optique de développer un système de détection.",
      "This thesis addresses the verbal and nonverbal signals that the listener of a message produces during a face to face dialogue in an incomprehension situation. This research focuses on two aspects : identifying among these behaviors, which that are self-adressed and those with a communicative purpose and does the incomprehension degree influence the type of behavior? For this research we built an audio-visual corpus annotated with the behaviors of fourteen subjects. A statistical analysis of these annotations led to singling out nine behaviors related to incomprehension. From these behaviors, we extracted parameters with a facial behavior analysis toolkit in order to check our results from these low-level descriptors."
    ],
    "authFullName_s": [
      "Éric Le Ferrand"
    ],
    "halId_s": "dumas-01901904",
    "producedDateY_i": 2018,
    "texte_nettoye": "Ce mémoire porte sur les signaux verbaux et non verbaux qu'émet le récepteur d'un message lors d'un dialogue face à face lorsque qu'il est placé en situation d'incompréhension. Cette recherche se concentre sur deux aspects : Quels sont parmi les signaux émis, ceux auto-adressés et ceux à but communicatif et est ce que le degré d'incompréhension a une influence sur le type de signal produit. Cette recherche a mené à la constitution d'un corpus audio-visuel annoté avec les comportements de quinze sujets. Des analyses statistiques effectuées sur ces annotations ont permis d'isoler neuf comportements relatifs à l'incompréhension. À partir des comportements faciaux isolés nous avons pu extraire un ensemble de paramètres grâce à un outil de détection de mouvements faciaux pour identifier des paramètres pertinents dans l’optique de développer un système de détection."
  },
  {
    "title_s": [
      "Évaluation de la pertinence des citations dans les articles scientifiques"
    ],
    "keyword_s": [
      "Scientific paper",
      "Similarity of text",
      "Citation accuracy",
      "Articles scientifiques",
      "Similarité textuelle",
      "Pertinence de citations"
    ],
    "abstract_s": [
      "Les citations jouent un rôle important dans la recherche scientifique. Cependant, des études récentes ont révélé un problème préoccupant : de nombreuses citations inexactes sont présentes dans les articles scientifiques. Ces références erronées peuvent entraîner des interprétations erronées, une déformation des intentions de l’auteur original et potentiellement même des conséquences plus graves. Pour faire face à cette préoccupation, notre article présente deux méthodes conçues pour évaluer l’exactitude des citations en utilisant des techniques de traitement automatique du langage (TAL). La première méthode consiste à mesurer les similitudes textuelles à l’aide de représentations vectorielles. La deuxième méthode implique l’utilisation d’une approche de classification de texte affinée pour trouver les citations fiables et les citations erronées. Ces deux méthodologies adoptent des modèles de langage. Selon nos résultats expérimentaux, il semble que la méthode de classification de texte affinée a démontré la meilleure performance.",
      "Citations play an important role in scientific research. However, recent studies have unveiled a concerning issue : numerous inaccurate citations are present within scientific papers. These erroneous references can result in misinterpretations, distortion of the original author’s intentions, and potentially even more serious consequences. To address this concern, our paper introduces two methods designed to assess citation accuracy using Natural Language Processing (NLP) techniques. The first method involves measuring text similarities through vectorized text representations. The second method entails employing a fine-tuned text classification approach to distinguish between reliable and flawed citations. Both of these methodologies adopt language models. Based on our experimental findings, it appears that the fine-tuned text classification method demonstrated the most optimal performance."
    ],
    "authFullName_s": [
      "Qinyue Liu"
    ],
    "halId_s": "dumas-04260594",
    "producedDateY_i": 2023,
    "texte_nettoye": "Les citations jouent un rôle important dans la recherche scientifique. Cependant, des études récentes ont révélé un problème préoccupant : de nombreuses citations inexactes sont présentes dans les articles scientifiques. Ces"
  },
  {
    "title_s": [
      "Apport du TAL à la constitution et l'exploitation d'un corpus scolaire de cours préparatoire"
    ],
    "keyword_s": [
      "Spell checking",
      "Spelling errors",
      "Learner corpora",
      "Détection d'erreurs",
      "Annotation",
      "Orthographe",
      "Corpus scolaire"
    ],
    "abstract_s": [
      "L'intérêt pour l'étude des corpus scolaires, tout en étant grandissant, se heurte à la taille de ces corpus et donc à la difficulté d'une analyse entièrement manuelle. Utiliser des méthodes empruntées au traitement automatique des langues (TAL) pourrait aider à l'exploitation de ces corpus. Cela représente cependant un défi pour le TAL du fait de l'éloignement de ces corpus à la norme. L'objectif de notre travail est d'adapter certaines techniques du TAL, éprouvées par ailleurs, afin de faciliter la constitution et l'exploitation d'un corpus recueilli en classe de CP. L'enjeu est donc double. Il s'agit à la fois de proposer une première définition d'un outil répondant aux besoins de la recherche en linguistique et en didactique. Mais il s'agit également, pour le TAL, de caractériser et de modéliser un type d'écrit distant de la norme. Nous proposerons dans ce mémoire un premier schéma d'annotation d'erreurs et des pistes pour l'analyse automatique de ce type de corpus.",
      "Whereas interest for learner has corpora increased, this research deals with the size of those corpora. Difficulties exist from manual treatments. Therefore we propose to use NLP (Natural Language Processing) methods to help exploit those corpora. This represents a challenge for NLP due to numerous errors from the age level. Our work aims to adapt some verified methods from NLP to build and exploit a first grade elementary school corpus. Our project has two goals in mind. First we hope to construct a framework which can deal with needs in didactic's and linguistic's research. And secondly we aim to model this particular writing type which is far from standard spelling. In this master's thesis we will present a proposition of annotation schema and suggestions for future research."
    ],
    "authFullName_s": [
      "Claire Wolfarth"
    ],
    "halId_s": "dumas-01167286",
    "producedDateY_i": 2015,
    "texte_nettoye": "L'intérêt pour l'étude des corpus scolaires, tout en étant grandissant, se heurte à la taille de ces corpus et donc à la difficulté d'une analyse entièrement manuelle. Utiliser des méthodes empruntées au traitement automatique des langues (TAL) pourrait aider à l'exploitation de ces corpus. Cela représente cependant un défi pour le TAL du fait de l'éloignement de ces corpus à la norme. L'objectif de notre travail est d'adapter certaines techniques du TAL, éprouvées par ailleurs, afin de faciliter la constitution et l'exploitation d'un corpus recueilli en classe de CP. L'enjeu est donc double. Il s'agit à la fois de proposer une première définition d'un outil répondant aux besoins de la recherche en linguistique et en didactique. Mais il s'agit également, pour le TAL, de caractériser et de modéliser un type d'écrit distant de la norme. Nous proposerons dans ce mémoire un premier schéma d'annotation d'erreurs et des pistes pour l'analyse automatique de ce type de corpus."
  },
  {
    "title_s": [
      "Étude de faisabilité de mise en place d'une indexation semi-automatique avec un thésaurus spécialisé en archéologie"
    ],
    "keyword_s": [
      "Langage naturel",
      "Indexation semi-automatique",
      "TAL traitement automatique du langage",
      "Thésaurus",
      "Articles scientifiques",
      "Archéologie"
    ],
    "abstract_s": [
      "Le réseau de bibliothèques et de laboratoires FRANTIQ a entre autres, comme but de fédérer les documents d'archéologie et de sciences de l'Antiquité (papier ou électronique) dans un catalogue commun indexé afin de les retrouver plus facilement. Chaque document fait l'objet d'une notice et d'une indexation à l'aide du thésaurus spécialisé en archéologie PACTOLS. Or le nombre de documents ne cesse d'accroître et l'indexation manuelle est une tâche très coûteuse en temps/homme. De plus, en archéologie, les informations sont cumulatives, c'est-à-dire que les informations ne deviennent jamais obsolètes mais s'ajoutent au fil du temps et les documents sont des ressources importantes pour le travail des chercheurs et des étudiants. En 2009, pour alléger le travail d'indexation des documentalistes et bibliothécaires et étendre la couverture du catalogue, les premiers essais d'indexation semi-automatique à l'aide du thésaurus PACTOLS ont été réalisés par l'informaticien du réseau FRANTIQ aidé de stagiaires. Le résultat n'étant pas très concluant, notamment à cause d'ambiguïtés dans les mots-clés proposés par l'algorithme, et par manque de temps et de moyens pour améliorer les résultats, ce projet a été temporairement abandonné. Ce mémoire a pour but de reprendre ce projet et d'analyser les méthodes modernes et anciennes de l'indexation automatique et semi-automatique afin d'améliorer les résultats et son taux d'acceptation par les indexeurs en poste. Deux systèmes ont étés sélectionnés afin d'en comparer les résultats : un système à base de règles crée avec python et natural language toolkit (NLTK), et un système d'apprentissage basé sur KEA++."
    ],
    "authFullName_s": [
      "Anita Mazur"
    ],
    "halId_s": "mem_00737359",
    "producedDateY_i": 2012,
    "texte_nettoye": "Le réseau de bibliothèques et de laboratoires FRANTIQ a entre autres, comme but de fédérer les documents d'archéologie et de sciences de l'Antiquité (papier ou électronique) dans un catalogue commun indexé afin de les retrouver plus facilement. Chaque document fait l'objet d'une notice et d'une indexation à l'aide du thésaurus spécialisé en archéologie PACTOLS. Or le nombre de documents ne cesse d'accroître et l'indexation manuelle est une tâche très coûteuse en temps/homme. De plus, en archéologie, les informations sont cumulatives, c'est-à-dire que les informations ne deviennent jamais obsolètes mais s'ajoutent au fil du temps et les documents sont des ressources importantes pour le travail des chercheurs et des étudiants. En 2009, pour alléger le travail d'indexation des documentalistes et bibliothécaires et étendre la couverture du catalogue, les premiers essais d'indexation semi-automatique à l'aide du thésaurus PACTOLS ont été réalisés par l'informaticien du réseau FRANTIQ aidé de stagiaires. Le résultat n'étant pas très concluant, notamment à cause d'ambiguïtés dans les"
  },
  {
    "title_s": [
      "TALN, Text-Mining et ontologie pour la maintenance de panneaux photovoltaïques"
    ],
    "keyword_s": [
      "Text-Mining",
      "Ontology",
      "Maintenance",
      "Solar parks",
      "NLP",
      "Parcs solaires",
      "Fouille de texte",
      "TAL",
      "Maintenance",
      "Ontologies"
    ],
    "abstract_s": [
      "L’abondance de données qui circulent dans le monde, conséquence de l’avènement du numérique, s’accompagne de questionnements quant aux usages qui peuvent être fait de ces données. C’est ce besoin qui a conduit EDF à lancer des projets de recherches afin d’exploiter les données à sa disposition. Parmi ces projets, l’un se penche sur les données textuelles rendant compte des opérations de maintenance effectuées dans les parcs électriques solaires. Le travail réalisé dans ce mémoire a été accompli dans le cadre de ce projet. L’objectif est d’exploiter les données textuelles et d’en extraire des informations afin d’optimiser le processus de maintenance dans les parcs photovoltaïques. La problématique est la suivante : Comment le traitement automatique des langues peut-il permettre d’optimiser le processus de conception, d’exploitation et de maintenance d’installation photovoltaïques ? Pour répondre à cette problématique plusieurs actions ont été menées afin d’adapter une application d’analyse textuelle au domaine solaire. L’application repose sur une ontologie et des règles. Les principales actions décrites dans ce mémoire ont pour but de mettre en forme les données de départ et d’aboutir à l’enrichissement de l’ontologie afin que l’application soit compatible avec le traitement de données textuelles provenant de comptes rendus de maintenance de panneaux solaire.",
      "The very large amount of circulating data, around the world is related to the rise of the digital. Many questions stand out as to the uses that can be made of these data. As a resort EDF aims to launch research projects to make use of the data at his disposal. Among these projects, one looks at textual data. The analysis conducted in this thesis was established as part of this project. The aim is to use textual data and extract information to optimize the maintenance process in photovoltaic parks. The issue is: How can natural language processing optimize the process of designing, operating and maintaining photovoltaic systems? To answer this, several actions have been carried out in order to adapt a textual analysis application to the solar domain. The application lays on an on an ontology and rules. The main actions described in this thesis are intended to format the initial data in order to extend the ontology, so that the application is compatible with the processing of textual data, from maintenance reports of solar panels."
    ],
    "authFullName_s": [
      "Sami Bouhouche"
    ],
    "halId_s": "dumas-02322096",
    "producedDateY_i": 2019,
    "texte_nettoye": "L’abondance de données qui circulent dans le monde, conséquence de l’avènement du numérique, s’accompagne de questionnements quant aux usages qui peuvent être fait de ces données. C’est ce besoin qui a conduit EDF à lancer des projets de recherches afin d’exploiter les données à sa disposition. Parmi ces projets, l’un se penche sur les données textuelles rendant compte des opérations de maintenance effectuées dans les parcs électriques solaires. Le travail réalisé dans ce mémoire a été accompli dans le cadre de ce projet. L’objectif est d’exploiter les données textuelles et d’en extraire des informations afin d’optimiser le processus de maintenance dans les parcs photovoltaïques. La problématique est la suivante : Comment le traitement automatique des langues peut-il permettre d’optimiser le processus de conception, d’exploitation et de maintenance d’installation photovoltaïques ? Pour répondre à cette problématique plusieurs actions ont été menées afin d’adapter une application d’analyse textuelle au domaine solaire. L’application repose sur une ontologie et des règles. Les principales actions décrites dans ce mémoire ont pour but de mettre en forme les données de départ et d’aboutir à l’enrichissement de l’ontologie afin que l’application soit compatible avec le traitement de données textuelles provenant de comptes rendus de maintenance de panneaux solaire."
  },
  {
    "title_s": [
      "Modélisation auto-supervisée de la parole affective spontanée"
    ],
    "keyword_s": [
      "Self-Supervised Representation Learning",
      "Speech Emotion Recognition",
      "Apprentissage des représentations",
      "Représentation auto-supervisée",
      "Apprentissage auto-supervisé",
      "Reconnaissance des émotions de la parole"
    ],
    "abstract_s": [
      "Les modèles auto-supervisés pré-entraîné utilisant des données non étiquetées pour extraire des représentations ont été largement exploré dans le domaine du traitement automatique de la parole. Ce mémoire explore une nouvelle approche consistant à extraire des représentations linguistiques des transcriptions à partir des modèles auto-supervisés pré-entraîné pour la reconnaissance des émotions de la parole spontanée en temps continu. Nous avons examiné une méthode d’alignement pour accorder les représentations linguistiques avec du temps. Les résultats des expérimentations montrent que les représentations auto-supervisées linguistiques peuvent prédire les émotions en dimension arousal et valence aussi bien que les représentations auto-supervisées acoustiques.",
      "Self-supervised pre-trained models using unlabeled data to extract representations have been widely explored in the field of automatic speech processing. This paper explores a new approach of extracting linguistic representations from transcriptions using pre-trained self-supervised models for time-continuous emotion recognition. We examined an alignment method to fit linguistic representations with time. Experimental results show that linguistic self-supervised representations can predict emotions in arousal and valence dimensions and achieve excellent results as acoustic self-supervised representations."
    ],
    "authFullName_s": [
      "Ziyi Tong"
    ],
    "halId_s": "dumas-03516512",
    "producedDateY_i": 2022,
    "texte_nettoye": "Les modèles auto-supervisés pré-entraîné utilisant des données non étiquetées pour extraire des représentations ont été largement exploré dans le domaine du traitement automatique de la parole. Ce mémoire explore une nouvelle approche consistant à extraire des représentations linguistiques des transcriptions à partir des modèles auto-supervisés pré-entraîné pour la reconnaissance des émotions de la parole spontanée en temps continu. Nous avons examiné une méthode d’alignement pour accorder les représentations linguistiques avec du temps. Les résultats des expérimentations montrent que les représentations auto-supervisées linguistiques peuvent prédire les émotions en dimension arousal et valence aussi bien que les représentations auto-supervisées acoustiques."
  },
  {
    "title_s": [
      "Développement phonologique typique français d'enfants multilingues de CP"
    ],
    "keyword_s": [
      "Facteurs extralinguistiques",
      "Facteurs linguistiques",
      "Traitement de la parole",
      "Développement phonologique",
      "Multilinguisme"
    ],
    "abstract_s": [
      "Le projet EULALIES concerne l’évaluation et le dépistage des troubles du développement des sons de la parole. Les objectifs de ce projet sont de créer un outil d’évaluation du développement phonologique en français hexagonal, puis de récolter des données de référence, et enfin de recueillir des données sur les troubles du développement des sons dela parole pour identifier des marqueurs cliniques spécifiques au français. Ce mémoire a été réalisé dans le cadre de ce projet, dans le but de recueillir des données permettant de caractériser la variabilité du développement phonologique typique chez des enfants francophones multilingues de CP. Nous proposons tout d’abord un ancrage théorique autour des études déjà menées sur le développement phonologique multilingue. Grâce à ces études, nous avons ensuite mis en évidence plusieurs facteurs qui ont une influence sur le développement phonologique multilingue. Puis, nous détaillons la méthodologie utilisée dans le cadre du projet EULALIES, avant de présenter et discuter les résultats analysés en fonction de l’âge d’acquisition des langues, des langues parlées et du statut socioéconomique."
    ],
    "authFullName_s": [
      "Clarisse Puissant"
    ],
    "halId_s": "dumas-02949296",
    "producedDateY_i": 2020,
    "texte_nettoye": "Le projet EULALIES concerne l’évaluation et le dépistage des troubles du développement des sons de la parole. Les objectifs de ce projet sont de créer un outil d’évaluation du développement phonologique en français hexagonal, puis de récolter des données de référence, et enfin de recueillir des données sur les troubles du développement des sons dela parole pour identifier des marqueurs cliniques spécifiques au français. Ce mémoire a été réalisé dans le cadre de ce projet, dans le but de recueillir des données permettant de caractériser la variabilité du développement phonologique typique chez des enfants francophones multilingues de CP. Nous proposons tout d’abord un ancrage théorique autour des études déjà menées sur le développement phonologique multilingue. Grâce à ces études, nous avons ensuite mis en évidence plusieurs facteurs qui ont une influence sur le développement phonologique multilingue. Puis, nous détaillons la méthodologie utilisée dans le cadre du projet EULALIES, avant de présenter et discuter les résultats analysés en fonction de l’âge d’acquisition des langues, des langues parlées et du statut socioéconomique."
  },
  {
    "title_s": [
      "L'escape game comme outil d'expérience : un essai dans le domaine de la robotique sociale"
    ],
    "keyword_s": [
      "Rob’air",
      "Escape game",
      "Robotics",
      "Social navigation",
      "Navigation sociale",
      "Rob’air",
      "Escape game",
      "Robotique"
    ],
    "abstract_s": [
      "Nous avons conçu un escape game qui permettra d’observer les possibles changements de distance sociale entre des personnes utilisant un robot de téléprésence, rob’air, pour communiquer. Deux joueurs et/ou joueuses auront pour but de récupérer des objets et documents tout en résolvant des énigmes dans un espace de jeu délimité : l’appartement intelligent et le plateau d’expérimentation de DOMUS, ainsi qu’une fausse salle de contrôle. L’escape game a été testé dans les anciens locaux de DOMUS et est prêt à être installé à la MACI, où DOMUS doit s’installer en septembre 2019. Les autres missions de notre stage sont aussi décrites dans ce rapport, à savoir : notre participation à la réalisation d’une expérience sur l’effet Lombard et des recherches sur l’enregistrement de sons émis par des animaux et leur perception par les humains.",
      "We created an escape game to test the potential changes of social distance between interlocutors using the telepresence robot Rob’Air to communicate. Two players will have to gather objects and papers while solving riddles. The game will take place in the DOMUS apartment, the DOMUS experimentation platform, and a false control room. The game was tested in the CTL building and is ready to be set up at MACI, where DOMUS will settle in September 2019. We also described in this document the two other missions we had during our internship: our involvement in an experimentation on the Lombard effect and our research on recording sounds emitted by animals and their perception by humans."
    ],
    "authFullName_s": [
      "Émeline Le Goff",
      "Zoé Giorgis"
    ],
    "halId_s": "dumas-02302102",
    "producedDateY_i": 2019,
    "texte_nettoye": "Nous avons conçu un escape game qui permettra d’observer les possibles changements de distance sociale entre des personnes utilisant un robot de téléprésence, rob’air, pour communiquer. Deux joueurs et/ou joueuses auront pour but de récupérer des objets et documents tout en résolvant des énigmes dans un espace de jeu délimité : l’appartement intelligent et le plateau d’expérimentation de DOMUS, ainsi qu’une fausse salle de contrôle. L’escape game a été testé dans les anciens locaux de DOMUS et est prêt à être installé à la MACI, où DOMUS doit s’installer en septembre 2019. Les autres missions de notre stage sont aussi décrites dans ce rapport, à savoir : notre participation à la réalisation d’une expérience sur l’effet Lombard et des recherches sur l’enregistrement de sons émis par des animaux et leur perception par les humains."
  },
  {
    "title_s": [
      "Extraction d'informations en contexte de Petites et Moyennes Entreprises"
    ],
    "keyword_s": [
      "Internship report",
      "Tools selection",
      "Information extraction",
      "Extractions d'informations",
      "Perl",
      "Rapport de stage",
      "Sélection d'outils"
    ],
    "abstract_s": [
      "Le Traitement Automatique du Langage (Naturel) est une discipline jeune qui a vu le jour dans les années 60. Elle est jeune également dans sa représentation en entreprise. L'ensemble de ces caractères définissent sans doute des schémas d'intervention au sein de ces sociétés dont l'expérience au sein de la société N5 peut être un reflet. Ces services sont pour partie prévisibles. Ce sont tout d'abord toutes les missions à visée informative qui permettent à l'entreprise de réaliser son positionnement logiciel voire matériel, au regard de ses besoins. Ces missions d'information sont en effet immanquablement suivies d'une phase de sélection d'outils. Les étapes à suivre voient la mise en place et l'expérimentation des dits outils ou au contraire la commande de missions plus légères, telle la réalisation de missions d'extraction. Ce sont des missions légères au point de vue des ressources engagées, puisqu'elles ne requièrent pas d'outils spécialement coûteux. Toute la spécificité de cette phase réside dans la particularité des besoins à satisfaire. Bien que là encore, des motifs récurrents s'observent. Les outils de base du linguiste n'étant pas toujours les mêmes, le vocabulaire du langage ?",
      "Automatic (Natural) Language Processing is a new discipline, born in the 60's. the discipline is new too in companies' practice. One can presume how ANLP can work for businesses. What happened in N5 is somehow significant of what kinds of interventions are required by small and medium-sized enterprises. These actions are quite predictable. First they are informative missions that define these companies' software and material environment. Selecting tools and trying them is often a natural end in this stage but are not always achieved. Next missions command extraction services. These last activities are financially light: they don't require a lot of money, no more expensive materials. The enterprise's needs made this stage specificity. Although, even here, there are some regular patterns because the linguist's bricks are still the same lowest level: (human) vocabulary."
    ],
    "authFullName_s": [
      "Mylène Joséphine"
    ],
    "halId_s": "dumas-00865992",
    "producedDateY_i": 2012,
    "texte_nettoye": "Le Traitement Automatique du Langage (Naturel) est une discipline jeune qui a vu le jour dans les années 60. Elle est jeune également dans sa représentation en entreprise. L'ensemble de ces caractères définissent sans doute des schémas d'intervention au sein de ces sociétés dont l'expérience au sein de la société N5 peut être un reflet. Ces services sont pour partie prévisibles. Ce sont tout d'abord toutes les missions à visée informative qui permettent à l'entreprise de réaliser son positionnement logiciel voire matériel, au regard de ses besoins. Ces missions d'information sont en effet immanquablement suivies d'une phase de sélection d'outils. Les étapes à suivre voient la mise en place et l'expérimentation des dits outils ou au contraire la commande de missions plus légères, telle la réalisation de missions d'extraction. Ce sont des missions légères au point de vue des ressources engagées, puisqu'elles ne requièrent pas d'outils spécialement coûteux. Toute la spécificité de cette phase réside dans la particularité des besoins à satisfaire. Bien que là encore, des motifs récurrents s'observent. Les outils de base du linguiste n'étant pas toujours les mêmes, le vocabulaire du langage ?"
  },
  {
    "title_s": [
      "Normalisation automatique de variables issues de bases de données en agroécologie"
    ],
    "keyword_s": [
      "Distance de Levenshtein",
      "Développement de systèmes agricoles durables",
      "Données agricoles",
      "Corpus",
      "Canonisation",
      "Cosinus",
      "Similarité",
      "BERT",
      "TF-IDF",
      "Ontologies",
      "Traitement automatique du langage naturel",
      "Correspondance",
      "Agroécologie"
    ],
    "abstract_s": [
      "Ce rapport de stage présente une étude réalisée au sein de l’UMR TETIS, située à la Maison De la Télédétection sur le campus Agropolis de Montpellier, en collaboration avec l’UR AIDA. Le stage s’est focalisé sur l’importance de la correspondance des variables sources et candidates en agroécologie. L’objectif principal de ce stage était de résoudre la problématique liée à l’hétérogénéité des variables utilisées par les chercheurs en agroécologie. Cependant, chaque chercheur a sa propre méthode de nomination et de description des variables sources, ce qui rend la correspondance complexe et sujette à des erreurs. Pour aborder cette problématique, différentes méthodes de représentation des données textuelles ont été explorées, telles que TF-IDF [1] et des approches basées sur des modèles de langues tels que BERT-base (section 3.3.2), BERT-large (section 3.3.2), RoBERTa (section 3.3.4) et XLNet (section 3.3.3), pour la vectorisation des noms et des descriptions des variables. Des mesures de similarité, telles que la distance de Levenshtein [2] et le cosinus [3], ont été appliquées pour évaluer la proximité entre les variables. Les résultats obtenus ont démontré des améliorations significatives par rapport aux approches précédentes [5]. Cependant, certaines limites ont été identifiées, notamment le nombre limité de variables en anglais, la formulation non canonique des variables, les descriptions courtes et l’absence de prise en compte des ontologies associées. Des recommandations ont été formulées pour surmonter ces limites, telles que la traduction des variables dans la même langue que les ontologies, la canonisation des variables non canoniques, l’extension du corpus avec des Données multilingues et hétérogènes, et l’utilisation de Méthodes de plongement de mots et de mesure de similarité. Ce rapport met en évidence l’importance de la correspondance des variables en agroécologie. Les résultats obtenus offrent de nouvelles perspectives pour une meilleure utilisation et compréhension des données agricoles."
    ],
    "authFullName_s": [
      "Oussama Mechhour"
    ],
    "halId_s": "hal-05326124",
    "producedDateY_i": 2023,
    "texte_nettoye": "Ce rapport de stage présente une étude réalisée au sein de l’UMR TETIS, située à la Maison De la Télédétection sur le campus Agropolis de Montpellier, en collaboration avec l’UR AIDA. Le stage s’est focalisé sur l’importance de la correspondance des variables sources et candidates en agroécologie. L’objectif principal de ce stage était de résoudre la problématique liée à l’hétérogénéité des variables utilisées par les chercheurs en agroécologie. Cependant, chaque chercheur a sa propre méthode de nomination et de description des variables sources, ce qui rend la correspondance complexe et sujette à des erreurs. Pour aborder cette problématique, différentes méthodes de représentation des données textuelles ont été explorées, telles que TF-IDF [1] et des approches basées sur des modèles de langues tels que BERT-base (section 3.3.2), BERT-large (section 3.3.2), RoBERTa (section 3.3.4) et XLNet (section 3.3.3), pour la vectorisation des noms et des descriptions des variables. Des mesures de similarité, telles que la distance de Levenshtein [2] et le cosinus [3], ont été appliquées pour évaluer la proximité entre les variables. Les résultats obtenus ont démontré des améliorations significatives par rapport aux approches précédentes [5]. Cependant, certaines limites ont été identifiées, notamment le nombre limité de variables en anglais, la formulation non canonique des variables, les descriptions courtes et l’absence de prise en compte des ontologies associées. Des recommandations ont été formulées pour surmonter ces limites, telles que la traduction des variables dans la même langue que les ontologies, la canonisation des variables non canoniques, l’extension du corpus avec des Données multilingues et hétérogènes, et l’utilisation de Méthodes de plongement de mots et de mesure de similarité. Ce rapport met en évidence l’importance de la correspondance des variables en agroécologie. Les résultats obtenus offrent de nouvelles perspectives pour une meilleure utilisation et compréhension des données agricoles."
  },
  {
    "title_s": [
      "Constitution et évaluation d'un jeu de données linguistiques en français pour l'analyse des fonctions lexicales encodées dans les modèles neuronaux de type FlauBERT"
    ],
    "keyword_s": [
      "FlauBERT",
      "Polysemy",
      "Contextual vectors",
      "Word embeddings",
      "Vecteurs contextuels",
      "Plongement de mots",
      "FlauBERT",
      "Polysémie"
    ],
    "abstract_s": [
      "Chaque langue est constituée de mots qui lui sont propres. Dans la plupart des cas, ceux-ci sont polysémiques - ils possèdent plusieurs sens. La modélisation de la polysémie en Traitement Automatique de la Langue est une tâche difficile lorsqu’il s’agit de vecteurs de mots ; les systèmes de plongements de mots traditionnels ont certaines difficultés à traiter la polysémie. À l’aide de FlauBERT, qui est un nouveau modèle de langue développé en 2019, nous verrons qu’il est maintenant plus facile de traiter de la polysémie, notamment grâce à des vecteurs de mots contextualisés. Le contexte entier d’une phrase est pris en compte par FlauBERT afin de représenter chaque mot sous forme de vecteur. Après une brève analyse des différents domaines en jeu, je présenterai dans ce mémoire les différentes expérimentations que j’ai effectuées à l’aide des vecteurs de mots du système FlauBERT.",
      "Each language is made up of its own words. In most cases, these are polysemic, they have several meanings. Modeling polysemy in Automatic Language Processing is a difficult task when it comes to word vectors and traditional word embeddings systems have some difficulties in dealing with polysemy. Using FlauBERT, which is a new language model developed in 2019, we will see that it is now easier to deal with polysemy, especially with contextualized word vectors. The entire context of a sentence is taken into account by FlauBERT in order to represent each word as a vector. After a brief analysis of the different domains involved, I will present in this paper the different experiments I have performed using FlauBERT word vectors."
    ],
    "authFullName_s": [
      "Vincent Bellue"
    ],
    "halId_s": "dumas-02978401",
    "producedDateY_i": 2020,
    "texte_nettoye": "Chaque langue est constituée de mots qui lui sont propres. Dans la plupart des cas, ceux-ci sont polysémiques - ils possèdent plusieurs sens. La modélisation de la polysémie en Traitement Automatique de la Langue est une tâche difficile lorsqu’il s’agit de vecteurs de mots ; les systèmes de plongements de mots traditionnels ont certaines difficultés à traiter la polysémie. À l’aide de FlauBERT, qui est un nouveau modèle de langue développé en 2019, nous verrons qu’il est maintenant plus facile de traiter de la polysémie, notamment grâce à des vecteurs de mots contextualisés. Le contexte entier d’une phrase est pris en compte par FlauBERT afin de représenter chaque mot sous forme de vecteur. Après une brève analyse des différents domaines en jeu, je présenterai dans ce mémoire les différentes expérimentations que j’ai effectuées à l’aide des vecteurs de mots du système FlauBERT."
  },
  {
    "title_s": [
      "Modélisation d'une ontologie de domaine et des outils d'extraction de l'information associés pour l'anglais et le français"
    ],
    "keyword_s": [
      "Ontology",
      "Named entities",
      "Text mining",
      "Information extraction",
      "Natural Language Processing NLP",
      "Entités nommées",
      "Ontologie",
      "Fouille de textes",
      "Extraction d'information",
      "Traitement Automatique de la Langue TAL"
    ],
    "abstract_s": [
      "Aujourd'hui, l'abondance des sources d'information publiques (sites internet, presse, radio, télévision, etc.) a fait émergé le besoin de \" fouiller \" cette masse de documents afin d'en extraire des connaissances pertinentes dans un but donné. L'équipe IPCC, au sein d'EADS Defence & Security, est chargée de l'innovation en matière de traitement de l'information. Ce mémoire présente une ontologie de domaine et les outils d'extraction de l'information associés pour l'anglais et le français. Après une brève analyse des outils et techniques existants en modélisation d'ontologie et extraction d'information, nous présentons les différents travaux réalisés durant notre stage. Nous avons modélisé, grâce au logiciel Protégé, une petite ontologie de domaine au format OWL, dédiée au renseignement militaire. Afin de repérer dans un texte les différents éléments d'intérêt, nous avons développé, grâce à l'environnement GATE, un outil d'extraction d'entités nommées, évènements et relations. Nous détaillons ici la méthode choisie, les étapes de réalisation ainsi que l'évaluation quantitative et qualitative des résultats obtenus.",
      "Nowadays, the increasing of information sources (websites, newspapers, radio, TV, etc.) has led to \"dig\" these documents in order to extract relevant knowledge considering a set purpose. The IPCC team at EADS Defence & Security is responsible for invention in media mining. This thesis introduces a domain ontology and the associated tools for information extraction in English and French texts. After a brief analysis of existing tools and techniques in ontology development and information extraction, we present the work done during our training course. First, we used the Protégé tool to create a small OWL domain ontology dedicated to military intelligence. In order to recognize the elements of interest, we have built, through GATE architecture, a system to extract named entities, events and relations. We present here our methodology, the different stages of implementation as well as a quantitative and qualitative evaluation of our results."
    ],
    "authFullName_s": [
      "Laurie Serrano"
    ],
    "halId_s": "dumas-00569002",
    "producedDateY_i": 2010,
    "texte_nettoye": "Aujourd'hui, l'abondance des sources d'information publiques (sites internet, presse, radio, télévision, etc.) a fait émergé le besoin de \" fouiller \" cette masse de documents afin d'en extraire des connaissances pertinentes dans un but donné. L'équipe IPCC, au sein d'EADS Defence & Security, est chargée de l'innovation en matière de traitement de l'information. Ce mémoire présente une ontologie de domaine et les outils d'extraction de l'information associés pour l'anglais et le français. Après une brève analyse des outils et techniques existants en modélisation d'ontologie et extraction d'information, nous présentons les différents travaux réalisés durant notre stage. Nous avons modélisé, grâce au logiciel Protégé, une petite ontologie de domaine au format OWL, dédiée au renseignement militaire. Afin de repérer dans un texte les différents éléments d'intérêt, nous avons développé, grâce à l'environnement GATE, un outil d'extraction d'entités nommées, évènements et relations. Nous détaillons ici la méthode choisie, les étapes de réalisation ainsi que l'évaluation quantitative et qualitative des résultats obtenus."
  },
  {
    "title_s": [
      "Extraction d'évènements au sein d'une plateforme de veille"
    ],
    "keyword_s": [
      "Web Intelligence",
      "Market Intelligence",
      "Event Extraction",
      "Text Mining",
      "Information Extraction",
      "Veille sur internet",
      "Market Intelligence",
      "Fouille de textes",
      "Extraction d’information",
      "Extraction d’évènements"
    ],
    "abstract_s": [
      "L’extraction d’évènements est une application du Traitement Automatique des Langues (TAL), et plus précisément une application de fouille de textes, qui consiste à extraire de manière structurée des informations sur des évènements présents de manière non-structurée dans des textes. Lors de ce stage, nous avons travaillé à l’élaboration d’une maquette d’un outil d’extraction d’évènements qui repose sur une méthode de reconnaissance de motifs. Nous avons principalement travaillé avec le corpus ACE (Automatic Content Extraction) issu de la campagne d’évaluation du même nom. Ce stage s’inscrit dans le cadre de l’enrichissement continue des fonctionnalités d’AMI Enterprise Intelligence, la solution de veille stratégique développée et maintenue dans le centre R&D de la société Bertin IT à Montpellier. Les principales contributions de ce mémoire sont : — état de l’art sur l’extraction d’évènements ; — étude de plusieurs outils d’extraction d’évènements disponibles en OpenSource ; — développement d’une maquette d’un outil d’extraction d’évènements.",
      "Event extraction is an application of Natural Language Processing (NLP), more precisely of text mining, consisting in extracting structured information about events present in texts in an unstructured way. During the course of this internship, we worked on the development of the model version of an event extraction tool. This tool is based on a pattern-matching method. We mainly worked with the ACE (Automatic Content Extraction) corpus, from the evaluation conference of the same name. This internship and the developement of this model are part of enhancing AMI Enterprise Intelligence (AMI EI), the business intelligence solution developed and maintained by Bertin IT within its research center in Montpellier. The main contributions of this project are : — state of the art on event extraction ; — review of several Open Source tools ; — development of a model version of an event extraction tool."
    ],
    "authFullName_s": [
      "Capucine Antoine"
    ],
    "halId_s": "dumas-03245021",
    "producedDateY_i": 2020,
    "texte_nettoye": "L’extraction d’évènements est une application du Traitement Automatique des Langues (TAL), et plus précisément une application de fouille de textes, qui consiste à extraire de manière structurée des informations sur des évènements présents de manière non-structurée dans des textes. Lors de ce stage, nous avons travaillé à l’élaboration d’une maquette d’un outil d’extraction d’évènements qui repose sur une méthode de reconnaissance de motifs. Nous avons principalement travaillé avec le corpus ACE (Automatic Content Extraction) issu de la campagne d’évaluation du même nom. Ce stage s’inscrit dans le cadre de l’enrichissement continue des fonctionnalités d’AMI Enterprise Intelligence, la solution de veille stratégique développée et maintenue dans le centre R&D de la société Bertin IT à Montpellier. Les principales contributions de ce mémoire sont : — état de l’art sur l’extraction d’évènements ; — étude de plusieurs outils d’extraction d’évènements disponibles en OpenSource ; — développement d’une maquette d’un outil d’extraction d’évènements."
  },
  {
    "title_s": [
      "L'analyse des adjectifs axiologiques dans les ouvrages touristiques sur la Thaïlande"
    ],
    "keyword_s": [
      "Axiological adjective",
      "Semantic",
      "Tourism discourse",
      "Adjectif axiologique",
      "Corpus",
      "Discours touristiques",
      "Sémantique"
    ],
    "abstract_s": [
      "S’appuyant sur le corpus de 432546 mots, des adjectifs axiologiques dans les discours touristiques servent de support de notre travail fondé sur l’analyse sémantique. Nous nous interrogeons sur la difficulté à catégoriser et à identifier la valeur axiologique ainsi qu’à la polarité des adjectifs axiologiques. La désambiguïsation et l’annotation sont réalisées ainsi en reposant sur la théorie taxonomique de l’Appraisal, à l’aide de quelques outils du traitement automatique des langues, pour parvenir à atteindre l’objectif de cette étude. En réalité, l’objectif final de ce travail s’oriente vers une élaboration de la ressource lexicale des adjectifs axiologiques dans le contexte touristique. Un autre objectif porte sur une analyse des stéréotypes de pensée. Se basant sur la culture et la tradition de chaque communauté, certains adjectifs sont jugés différemment.",
      "The current study based on a 432,546 words corpus provides a semantic analysis of axiological adjectives in tourism discourse. Here, we consider one of the difficulties in categorising and identifying an axiological value and a polarity of axiological adjectives. To achieve the objective of this study, disambiguation and annotation are accomplished relying on the taxonomical theory of Appraisal, by use of tools for Automatic Treatment of Language. In fact, the final expectation of this research is to expand on a lexical resource of axiological adjectives in the context of tourism. Another objective is reached on the analysis of the stereotypic thinking. Certain adjectives are judged differently depending on the culture and tradition of each community."
    ],
    "authFullName_s": [
      "Jitwongnan Jarukan"
    ],
    "halId_s": "dumas-01084118",
    "producedDateY_i": 2014,
    "texte_nettoye": "S’appuyant sur le corpus de 432546 mots, des adjectifs axiologiques dans les discours touristiques servent de support de notre travail fondé sur l’analyse sémantique. Nous nous interrogeons sur la difficulté à catégoriser et à identifier la valeur axiologique ainsi qu’à la polarité des adjectifs axiologiques. La désambiguïsation et l’annotation sont réalisées ainsi en reposant sur la théorie taxonomique de l’Appraisal, à l’aide de quelques outils du traitement automatique des langues, pour parvenir à atteindre l’objectif de cette étude. En réalité, l’objectif final de ce travail s’oriente vers une élaboration de la ressource lexicale des adjectifs axiologiques dans le contexte touristique. Un autre objectif porte sur une analyse des stéréotypes de pensée. Se basant sur la culture et la tradition de chaque communauté, certains adjectifs sont jugés différemment."
  },
  {
    "title_s": [
      "L’intelligence artificielle au service des géomètres-experts : optimiser le contrôle et la rédaction documentaire en copropriété"
    ],
    "keyword_s": [
      "Data structuring",
      "Land Surveying",
      "Master's thesis",
      "Normative phrasing",
      "Natural language processing",
      "Fine-tuning",
      "Artificial intelligence",
      "AI",
      "Co-ownership",
      "Descriptive Statement of Division",
      "DSD",
      "Surveyor-Expert",
      "Automated drafting",
      "Property document",
      "Language model",
      "Supervised learning",
      "Corpus-based optimization",
      "Text generation",
      "EDD",
      "Master Foncier",
      "ESGT",
      "Mémoire de Fin d’Études",
      "Formulation normative",
      "Structuration des données",
      "Traitement automatique du langage Naturel",
      "Génération de texte",
      "Optimisation-fine",
      "Optimisation sur corpus",
      "Apprentissage supervisé",
      "Modèle de langage",
      "Document foncier",
      "Automatisation de la rédaction",
      "Géomètre-Expert",
      "État Descriptif de Division",
      "Copropriété",
      "IA",
      "Intelligence Artificielle"
    ],
    "abstract_s": [
      "L’intégration de l’intelligence artificielle dans le domaine de la copropriété ouvre la voie à une amélioration significative des processus de rédaction et de vérification documentaire. L’État Descriptif de Division (EDD), document structurant par excellence, se prête particulièrement bien à ce type d’approche. En mobilisant des modèles de langage optimisés sur des exemples structurés, il devient possible de générer et contrôler des textes normés, cohérents et fidèles aux attentes professionnelles. Cette démarche permet non seulement de réduire les temps de traitement, mais aussi de sécuriser juridiquement la production grâce à une restitution systématisée des formulations attendues. Elle marque ainsi l’émergence d’une nouvelle manière de produire les documents techniques, plus rapide, plus fiable, et mieux adaptée aux exigences du terrain. À travers cette étude, l’IA apparaît comme un outil opérationnel au service des géomètres-experts, capable de renforcer la qualité, la constance et l’efficience dans la gestion des dossiers de copropriété.",
      "The integration of artificial intelligence into the field of co-ownership paves the way for a significant improvement in the processes of drafting and verifying technical documents. The Descriptive Statement of Division (DSD), a cornerstone document, is particularly well suited to this type of approach. By leveraging language models optimized on structured examples, it becomes possible to generate and validate standardized texts that are coherent and aligned with professional expectations. This method not only reduces processing time but also enhances legal reliability through the systematic reproduction of expected legal phrasing. It thus signals the emergence of a new way of producing technical documents—faster, more reliable, and better suited to on-site requirements. Through this study, AI emerges as a practical tool for land surveyors, capable of strengthening quality, consistency, and efficiency in the management of co-ownership cases."
    ],
    "authFullName_s": [
      "Maxime Mayet-Foulgoc"
    ],
    "halId_s": "dumas-05327446",
    "producedDateY_i": 2025,
    "texte_nettoye": "L’intégration de l’intelligence artificielle dans le domaine de la copropriété ouvre la voie à une amélioration significative des processus de rédaction et de vérification documentaire. L’État Descriptif de Division (EDD), document structurant par excellence, se prête particulièrement bien à ce type d’approche. En mobilisant des modèles de langage optimisés sur des exemples structurés, il devient possible de générer et contrôler des textes normés, cohérents et fidèles aux attentes professionnelles. Cette démarche permet non seulement de réduire les temps de traitement, mais aussi de sécuriser juridiquement la production grâce à une restitution systématisée des formulations attendues. Elle marque ainsi l’émergence d’une nouvelle manière de produire les documents techniques, plus rapide, plus fiable, et mieux adaptée aux exigences du terrain. À travers cette étude, l’IA apparaît comme un outil opérationnel au service des géomètres-experts, capable de renforcer la qualité, la constance et l’efficience dans la gestion des dossiers de copropriété."
  },
  {
    "title_s": [
      "Identification des patients dans un Entrepôt de Données de Santé Hospitalier : impact de la détection de contexte sur l’extraction des concepts médicaux",
      "Patient identification in a Hospital Health Data Warehouse: impact of context detection on medical concept extraction"
    ],
    "keyword_s": [
      "Hospital health data warehouse",
      "Secondary use of data",
      "Phenotyping",
      "Context detection",
      "Natural language processing",
      "Utilisation secondaire des données",
      "Identification de patient",
      "Détection de contexte",
      "Traitement du langage naturel",
      "Entrepôt de données de santé hospitalier"
    ],
    "abstract_s": [
      "Les entrepôts de données de santé hospitaliers (EDSH) permettent une exploitation secondaire des données médicales à des fins de recherche clinique et épidémiologique. Cependant, l’analyse des données non structurées demeure une tâche complexe, nécessitant l’application de techniques avancées de traitement automatique du langage naturel (TAL), notamment pour la détection de contexte. Cette thèse évalue l’impact de la prise en compte du contexte sur l’identification des concepts médicaux au sein de l’EDSH du CHU de Bordeaux. Deux bibliothèques, EDS-NLP et MedSpaCy, ont été comparées quant à leur capacité à détecter la négation, l’hypothèse, ainsi que les notions d’historique et de non-patient. L’évaluation des performances révèle que EDS-NLP présente un meilleur rappel, tandis que MedSpaCy affiche une meilleure précision, bien que les performances globales restent limitées. Une validation en conditions réelles a été réalisée sur la cohorte ArthroVIH, montrant une réduction du bruit dans l’identification des patients de 3 à 7 %, avec une précision avoisinant 80 %. Toutefois, ces résultats ne permettent pas encore d’automatiser intégralement la sélection des patients dans un EDSH pour une étude clinique. Une utilisation prudente des algorithmes à base de règles est recommandée.",
      "Hospital health data warehouses (EDSH) enable the secondary use of medical data for clinical and epidemiological research. However, analyzing unstructured data remains a complex task, requiring advanced natural language processing (NLP) techniques, particularly for context detection. This thesis assesses the impact of context detection on the identification of medical concepts within the EDSH of Bordeaux University Hospital. Two libraries, EDS-NLP and MedSpaCy, were compared for their ability to detect negation, hypothesis, as well as historical and non-patient contexts. Performance evaluation shows that EDS-NLP achieves higher recall, while MedSpaCy provides greater precision, though overall performance remains modest. A real-world validation was conducted on the ArthroVIH cohort, demonstrating a 3 to 7% reduction in noise in patient identification, with an accuracy of approximately 80%. However, these results do not yet allow for the full automation of patient selection in an EDSH for clinical research. It is recommended to adopt a cautious approach when using rule-based algorithms."
    ],
    "authFullName_s": [
      "Matisse Decilap"
    ],
    "halId_s": "dumas-05064981",
    "producedDateY_i": 2025,
    "texte_nettoye": "Les entrepôts de données de santé hospitaliers (EDSH) permettent une exploitation secondaire des données médicales à des fins de recherche clinique et épidémiologique. Cependant, l’analyse des données non structurées demeure une tâche complexe, nécessitant l’application de techniques avancées de traitement automatique du langage naturel (TAL), notamment pour la détection de contexte. Cette thèse évalue l’impact de la prise en compte du contexte sur l’identification des concepts médicaux au sein de l’EDSH du CHU de Bordeaux. Deux bibliothèques, EDS-NLP et MedSpaCy, ont été comparées quant à leur capacité à détecter la négation, l’hypothèse, ainsi que les notions d’historique et de non-patient. L’évaluation des performances révèle que EDS-NLP présente un meilleur rappel, tandis que MedSpaCy affiche une meilleure précision, bien que les performances globales restent limitées. Une validation en conditions réelles a été réalisée sur la cohorte ArthroVIH, montrant une réduction du bruit dans l’identification des patients de 3 à 7 %, avec une précision avoisinant 80 %. Toutefois, ces résultats ne permettent pas encore d’automatiser intégralement la sélection des patients dans un EDSH pour une étude clinique. Une utilisation prudente des algorithmes à base de règles est recommandée."
  },
  {
    "title_s": [
      "Dynamique de la glu socio-relationnelle en Interaction Humain-Robot : ébauche méthodologique du calcul des coûts langagiers"
    ],
    "keyword_s": [
      "Social robotics",
      "Human-robot interaction",
      "Socio-affective glue",
      "Elderly",
      "Distance weighting",
      "Robotique sociale",
      "Interaction humain-robot",
      "Glu socio-affective",
      "Personnes âgées",
      "Distance d’édition"
    ],
    "abstract_s": [
      "L’interaction humain-robot pose le problème de l’attachement que l’interaction vocale créée dynamiquement entre l’humain et le robot. La notion de glu socio-affective déplace le problème de l’interaction sur le lien social résultant de l’interaction. Nous nous intéressons dans cette étude à la description et la modélisation de la nature et l’évolution de la glu socio-relationnelle et aux variations langagières pluri-dimensionnelles qui concourent à la glu dans les interactions. Notre travail s’appuie sur le corpus Elderly Emox Expression, recueilli en magicien d’Oz, qui s’inscrit dans le contexte des personnes âgées socialement isolées. Les conséquences de l’isolement sont un désentrainement à la dynamique de l’interaction que l’interaction avec le robot Emox pourrait aider à réentrainer. Plus précisément, cette étude permet de déterminer les paramètres langagiers, dans EEE, participant à la création de la glu socio-affective et de tester quels coûts d’édition peuvent être attribués à ces paramètres dans le but de produire un coût global qui serait une mesure de la dynamique de la glu socio-relationnelle.",
      "The human-robot interaction raises the problem of the attachment that the vocal interaction creates dynamically between the human and the robot. The concept of socio-emotional glue focuses on the problem of the interaction on the social link resultant from the interaction. We are interested in the description and the establishment of a model of the nature and the evolution of the socio-affective glue and of the multidimensional language variations enable to create the glue in the elderly interactions. Our work is based on the Elderly Emox Expression corpus (EEE corpus) which has been collected in the Wizard of Oz context and which joins in the context of social isolated elderly. The consequence of the social isolation is the loss of the practice in the dynamics of the interaction and we think that the robot Emox could help to train the elderly interaction skills. Indeed, this study allows determining the language parameters in the EEE corpus which are part of the process of creation of the socio-emotional glue, moreover to try to relate a weight to these parameters with the aim of giving a global weight which is the measure of the dynamics of the socio-relational glue."
    ],
    "authFullName_s": [
      "Liliya Tsvetanova"
    ],
    "halId_s": "dumas-01178695",
    "producedDateY_i": 2015,
    "texte_nettoye": "L’interaction humain-robot pose le problème de l’attachement que l’interaction vocale créée dynamiquement entre l’humain et le robot. La notion de glu socio-affective déplace le problème de l’interaction sur le lien social résultant de l’interaction. Nous nous intéressons dans cette étude à la description et la modélisation de la nature et l’évolution de la glu socio-relationnelle et aux variations langagières pluri-dimensionnelles qui concourent à la glu dans les interactions. Notre travail s’appuie sur le corpus Elderly Emox Expression, recueilli en magicien d’Oz, qui s’inscrit dans le contexte des personnes âgées socialement isolées. Les conséquences de l’isolement sont un désentrainement à la dynamique de l’interaction que l’interaction avec le robot Emox pourrait aider à réentrainer. Plus précisément, cette étude permet de déterminer les paramètres langagiers, dans EEE, participant à la création de la glu socio-affective et de tester quels coûts d’édition peuvent être attribués à ces paramètres dans le but de produire un coût global qui serait une mesure de la dynamique de la glu socio-relationnelle."
  },
  {
    "title_s": [
      "Étude et caractérisation des disfluences scripturales dans les manuscrits de Stendhal"
    ],
    "keyword_s": [
      "Literature",
      "Enunciation",
      "Scriptural disflencies",
      "Digital humanities",
      "Manuscripts",
      "Criticism",
      "Literary",
      "Computer sciences",
      "Linguistics",
      "Disfluences scripturales",
      "Énonciation",
      "Linguistique",
      "Informatique",
      "Littérature",
      "Génétique de texte",
      "Manuscrits",
      "Stendhal",
      "Humanités numériques"
    ],
    "abstract_s": [
      "L'objet du projet de recherche que nous présentons dans ce mémoire porte sur des phénomènes de l'écrit constatés sur les brouillons d'auteurs qui sont analogues aux disfluences de l'oral. Nous donnons le terme <i>disfluences scripturales</i> pour décrire ces phénomènes que nous étudions et caractérisons au sein du corpus des manuscrits de Stendhal. Notre étude porte spécifiquement sur le corpus <i>Journaux</i> de cette collection d'écrits. Nous nous appuyons sur les principes linguistiques et utilisons les méthodes informatiques pour étudier ce corpus. Notre approche, d'un point de vue linguistique, s'appuie sur l'apport de la linguistique de l'énonciation à l'étude des disfluences scripturales. Cette analyse en corpus se fonde sur une démarche de traitement automatique des langues - au sens de l'outil et la modélisation informatique comme micro-macro-scope sur un corpus de matériau langagier numérisé. Nous proposons une caractérisation et une discussion sur la notion de disfluences scripturales, à l'aune des analyses plus présentes dans la littérature scientifiques des disfluences de l'oral.",
      "The research project that we present in this memoir concerns phenomena observed in written language and that we specifically observe in the rough drafts of an author's works. This language phenomena is analogous to what are known as oral disfluencies. We attribute the term <i>scriptural disfluencies</i> to the objects of our study, which we characterise within the corpus of Stendhal's manuscripts. Our study focuses specifically on the corpus <i>Journaux</i> (<i>Journals</i>) of this collection of writing. We rely on linguistic principles and use computational methods for this study. Our approach from the linguistic perspective acknowledges the contribution of the theory of enunciation to the study of scriptural disfluencies. This corpus analysis is also founded in an approach for natural language processing - specifically in the adaptation of an instrument of observation designed for application on a corpus of digitised manuscripts. We propose a linguistic characterisation of the phenomenon along with a discussion around the notion of scriptural disfluencies in light of existing analyses of oral disfluencies in scientific literature."
    ],
    "authFullName_s": [
      "Anne Vikhrova"
    ],
    "halId_s": "dumas-01007699",
    "producedDateY_i": 2014,
    "texte_nettoye": "L'objet du projet de recherche que nous présentons dans ce mémoire porte sur des phénomènes de l'écrit constatés sur les brouillons d'auteurs qui sont analogues aux disfluences de l'oral. Nous donnons le terme <i>disfluences scripturales</i> pour décrire ces phénomènes que nous étudions et caractérisons au sein du corpus des manuscrits de Stendhal. Notre étude porte spécifiquement sur le corpus <i>Journaux</i> de cette collection d'écrits. Nous nous appuyons sur les principes linguistiques et utilisons les méthodes informatiques pour étudier ce corpus. Notre approche, d'un point de vue linguistique, s'appuie sur l'apport de la linguistique de l'énonciation à l'étude des disfluences scripturales. Cette analyse en corpus se fonde sur une démarche de traitement automatique des langues - au sens de l'outil et la modélisation informatique comme micro-macro-scope sur un corpus de matériau langagier numérisé. Nous proposons une caractérisation et une discussion sur la notion de disfluences scripturales, à l'aune des analyses plus présentes dans la littérature scientifiques des disfluences de l'oral."
  },
  {
    "title_s": [
      "Étude onomasiologique et motivationnelle de quelques désignations de phénomènes atmosphériques dans l'État du Minas Gerais (Brésil)"
    ],
    "keyword_s": [
      "Phénomènes atmosphériques",
      "Géolinguistique",
      "Motivation lexicale",
      "Fenômenos atmosféricos",
      "Geolingüística",
      "Motivação lexical"
    ],
    "abstract_s": [
      "Ce mémoire a pour objectif d’explorer la motivation des termes désignant la rosée, le brouillard, la bruine et la grêle dans les parlers du Minas Gerais, au Brésil. Dans un premier temps, nous nous concentrons sur les aspects théoriques autour du signe linguistique et de la motivation lexicale, ainsi que sur leur traitement à travers les atlas linguistiques. Nous proposons ensuite un aperçu de la formation de l’aire linguistique avant de nous concentrer sur l’Esboço de um atlas linguistico de Minas Gerais (EALMG) qui constitue l’outil principal de notre analyse. Enfin, nous examinons les termes utilisés pour désigner ces phénomènes atmosphériques, en tenant compte des traditions et des croyances non seulement au Minas Gerais, mais aussi aux régions historiquement ou linguistiquement liées à cette aire. Cette étude aboutit à une proposition de classification des motivations sémantiques et à la cartographie de leur répartition géographique.",
      "Este trabalho tem como objetivo explorar a motivação dos termos que designam o orvalho, a neblina, a garoa e a granizo nos falares de Minas Gerais, Brasil. Inicialmente, nos concentramos nos aspectos teóricos relacionados ao signo linguístico e à motivação lexical, bem como no tratamento desses conceitos a partir dos atlas linguísticos. Em seguida, apresentamos uma visão geral da área linguística, antes de focarmos no Esboço de um Atlas Linguístico de Minas Gerais (EALMG), que constitui a principal ferramenta da nossa análise. Por fim, examinamos os termos usados para designar esses fenômenos atmosféricos, levando em consideração as tradições e crenças associadas não apenas de Minas Gerais, mas também das regiões historicamente ou linguisticamente relacionadas a essa área. Este estudo resulta em uma proposta de classificação das motivações semânticas desses termos, acompanhada de uma cartografia que ilustra sua distribuição geográfica."
    ],
    "authFullName_s": [
      "Renata dos Santos"
    ],
    "halId_s": "dumas-05425686",
    "producedDateY_i": 2025,
    "texte_nettoye": "Ce mémoire a pour objectif d’explorer la motivation des termes désignant la rosée, le brouillard, la bruine et la grêle dans les parlers du Minas Gerais, au Brésil. Dans un premier temps, nous nous concentrons sur les aspects théoriques autour du signe linguistique et de la motivation lexicale, ainsi que sur leur traitement à travers les atlas linguistiques. Nous proposons ensuite un aperçu de la formation de l’aire linguistique avant de nous concentrer sur l’Esboço de um atlas linguistico de Minas Gerais (EALMG) qui constitue l’outil principal de notre analyse. Enfin, nous examinons les termes utilisés pour désigner ces phénomènes atmosphériques, en tenant compte des traditions et des croyances non seulement au Minas Gerais, mais aussi aux régions historiquement ou linguistiquement liées à cette aire. Cette étude aboutit à une proposition de classification des motivations sémantiques et à la cartographie de leur répartition géographique."
  },
  {
    "title_s": [
      "Detection externalisée de vulnerabilités pour la plateforme Android à l'aide du langage OVAL"
    ],
    "keyword_s": [
      "Vulnérabilités",
      "Analyses",
      "OVAL",
      "Android"
    ],
    "abstract_s": [
      "Nous proposons dans ce rapport une approche novatrice pour analyser les systèmes Android et détecter leurs vulnérabilités de façon légère. Cette approche regroupe les principales composantes du processus d'analyse sous forme de service externalisé que les clients mobiles peuvent ensuite exploiter à l'aide d'un agent minimal. Le langage OVAL est utilisé comme support pour la description et l'analyse de vulnérabilités. En configurant la fréquence des analyses et le pourcentage de vulnérabilités à traiter au cours de chacune d'entre elles, l'approche proposée permet de limiter l'allocation de ressources côté client et de transférer les différents traitements sur des serveurs distants. La stratégie employée consiste à partager et distribuer les analyses à travers le temps pour réduire significativement l'activité sur les systèmes mobiles, tout en assurant un traitement de la totalité des vulnérabilités connues dans un laps de temps fini. De cette méthodologie résulte un processus d'analyse orienté cloud plus léger et plus rapide, pouvant limiter de façon significative la consommation de ressources et d'énergie côté client."
    ],
    "authFullName_s": [
      "Gaëtan Hurel"
    ],
    "halId_s": "hal-00875179",
    "producedDateY_i": 2013,
    "texte_nettoye": "Nous proposons dans ce rapport une approche novatrice pour analyser les systèmes Android et détecter leurs vulnérabilités de façon légère. Cette approche regroupe les principales composantes du processus d'analyse sous forme de service externalisé que les clients mobiles peuvent ensuite exploiter à l'aide d'un agent minimal. Le langage OVAL est utilisé comme support pour la description et l'analyse de vulnérabilités. En configurant la fréquence des analyses et le pourcentage de vulnérabilités à traiter au cours de chacune d'entre elles, l'approche proposée permet de limiter l'allocation de ressources côté client et de transférer les différents traitements sur des serveurs distants. La stratégie employée consiste à partager et distribuer les analyses à travers le temps pour réduire significativement l'activité sur les systèmes mobiles, tout en assurant un traitement de la totalité des vulnérabilités connues dans un laps de temps fini. De cette méthodologie résulte un processus d'analyse orienté cloud plus léger et plus rapide, pouvant limiter de façon significative la consommation de ressources et d'énergie côté client."
  },
  {
    "title_s": [
      "Vers une analyse génétique de textes assistée par l'informatique et le TAL : contextes et pistes exploratoires"
    ],
    "keyword_s": [
      "Manuscripts",
      "Philologists",
      "Textual genetic",
      "NLP",
      "Manuscrits",
      "Généticiens",
      "Génétique de textes",
      "TAL"
    ],
    "abstract_s": [
      "L'apparition de corpus numériques de manuscrits littéraires a enrichi notre patrimoine d'une donnée langagière analysable et traitable automatiquement. La transcription du contenu de ces manuscrits dans un format numérique textuel permet de parcourir en quelques secondes des milliers de mots. L'étude des différentes versions d'une œuvre littéraire donne lieu à des enquêtes fastidieuses de la part des chercheurs en littérature. Se posent alors de nouvelles questions de méthodologie de travail : comment exploiter au mieux l'outil informatique pour assister le chercheur, quelles sont les nouvelles études envisageables grâce aux progrès ? Après un aperçu de l'analyse génétique et d'outils de traitement automatique des langues existants dans le domaine, nous présentons la modélisation sur les manuscrits de Stendhal de trois fonctionnalités à trois niveaux de granularité qui assisteraient les chercheurs en littérature dans leur analyse sur les bibliothèques d'auteurs, le théâtre ou le code-switching.",
      "The rise of digital corpora of literary manuscripts has added a substantial amount of linguistic data to our heritage that can be analyzed and processed automatically. The transcription of the content of these manuscripts in digital text format allows thousands of words to be examined in a few seconds. Studying different versions of a literary work requires time-consuming examination by researchers in the field of literature. New methodological issues thus arise: how can information technology tools be best put to use to help researchers? What new studies are made possible by this progress? After an overview of textual genetic analysis and of the existing tools for natural language processing in this domain, we will outline the modelling, on Stendhal's manuscripts, of three functions that can aid literary researchers in their analysis of author's libraries, theatrical writing and code-switching."
    ],
    "authFullName_s": [
      "Claire Lemaire"
    ],
    "halId_s": "dumas-00516484",
    "producedDateY_i": 2010,
    "texte_nettoye": "L'apparition de corpus numériques de manuscrits littéraires a enrichi notre patrimoine d'une donnée langagière analysable et traitable automatiquement. La transcription du contenu de ces manuscrits dans un format numérique textuel permet de parcourir en quelques secondes des milliers de mots. L'étude des différentes versions d'une œuvre littéraire donne lieu à des enquêtes fastidieuses de la part des chercheurs en littérature. Se posent alors de nouvelles questions de méthodologie de travail : comment exploiter au mieux l'outil informatique pour assister le chercheur, quelles sont les nouvelles études envisageables grâce aux progrès ? Après un aperçu de l'analyse génétique et d'outils de traitement automatique des langues existants dans le domaine, nous présentons la modélisation sur les manuscrits de Stendhal de trois fonctionnalités à trois niveaux de granularité qui assisteraient les chercheurs en littérature dans leur analyse sur les bibliothèques d'auteurs, le théâtre ou le code-switching."
  },
  {
    "title_s": [
      "Parsing Punctuation and Coordination Extragrammatically"
    ],
    "keyword_s": [
      "Interaction grammar",
      "Punctuation",
      "Parsing",
      "Coordination"
    ],
    "abstract_s": [
      "Coordination is a syntactic construction that is extremely frequent in natural language and yet very difficult to analyse: it is highly ambiguous, as different types of constituents and non-constituents can be coordinated in different contexts, and it cannot be easily modelled using the same formal tools used to represent the \"basic\", coordination-less part of natural languages. As for punctuation, often neglected as an object of study by linguists because it is peculiar to written language, it can combine with or substitute conjunctions to play a coordinative role, or play an adjunctive role. Some researchers propose that coordination is not a grammatical phenomenon that is a matter of linguistic competence, but rather a performance issue that should be analysed directly among the syntactic structures. We suggest to use this idea to the benefit of natural language processing by defining an algorithm that deals with coordination and punctuation using graph transformations applied directly to the output of a parser based on a model of \"basic\" language. The syntactic structure of a sentence as proposed by our system takes the shape of a directed acyclic graph in which the constituent sharing phenomenon at the roots of coordination appears sharply. We detail an algorithm working within the framework of interaction grammars (but suggest ways to adapt it to other formalisms, namely tree-adjoining grammars, phrase structure grammars, and dependency syntax) which is able to parse many types of coordinative and adjunctive constructions.",
      "La coordination est une construction syntaxique extrêmement fréquente dans les langues naturelles et néanmoins très difficile à analyser : elle est hautement ambiguë, de nombreux types de constituants ou non-constituants pouvant être coordonnés dans différents contextes, et se prête difficilement à une modélisation à l'aide des outils formels employés pour représenter la portion \" basique \" des langues naturelles, c'est-à-dire dépourvue de coordination. Quant à la ponctuation, objet d'étude souvent occulté par la linguistique car étant propre au langage écrit, elle peut se combiner ou se substituer aux conjonctions pour jouer un rôle coordinatif, ou encore jouer un rôle adjonctif. Certains chercheurs avancent que la coordination ne constitue pas un phénomène grammatical relevant de la compétence linguistique, mais plutôt un fait de performance qui devrait s'analyser au sein même des structures syntaxiques. Nous nous proposons de reprendre cette idée au compte du traitement automatique des langues en définissant un algorithme de traitement de la coordination et de la ponctuation qui opère un processus de transformation de graphes sur la sortie d'un analyseur basé sur un modèle de langage \" basique \". La structure syntaxique d'une phrase telle que proposée par notre système prend la forme d'un graphe acyclique orienté, dans lequel apparaît clairement le phénomène de partage de constituants qui définit la nature de la coordination. Nous détaillons un algorithme dans le cadre des grammaires d'interaction, mais pouvant être étendu à d'autres formalismes (nommément les grammaires d'arbres adjoints, les grammaires de structures de phrase et la syntaxe de dépendance), qui permet d'analyser de nombreux types de constructions coordinatives et adjonctives. Nous comparons celui-ci à une approche classique de modélisation de la coordination dans le formalisme des grammaires d'interaction développée par Le Roux et Perrier (2006) puis à une approche similaire à la nôtre développée dans le cadre des grammaires d'arbres adjoints par Joshi et Schabes (1991)."
    ],
    "authFullName_s": [
      "Valmi Dufour-Lussier"
    ],
    "halId_s": "inria-00634736",
    "producedDateY_i": 2010,
    "texte_nettoye": "Coordination is a syntactic construction that is extremely frequent in natural language and yet very difficult to analyse: it is highly ambiguous, as different types of constituents and non-constituents can be coordinated in different contexts, and it cannot be easily modelled using the same formal tools used to represent the \"basic\", coordination-less part of natural languages. As for punctuation, often neglected as an object of study by linguists because it is peculiar to written language, it can combine with or substitute conjunctions to play a coordinative role, or play an adjunctive role. Some researchers propose that coordination is not a grammatical phenomenon that is a matter of linguistic competence, but rather a performance issue that should be analysed directly among the syntactic structures. We suggest to use this idea to the benefit of natural language processing by defining an algorithm that deals with coordination and punctuation using graph transformations applied directly to the output of a parser based on a model of \"basic\" language. The syntactic structure of a sentence as proposed by our system takes the shape of a directed acyclic graph in which the constituent sharing phenomenon at the roots of coordination appears sharply. We detail an algorithm working within the framework of interaction grammars (but suggest ways to adapt it to other formalisms, namely tree-adjoining grammars, phrase structure grammars, and dependency syntax) which is able to parse many types of coordinative and adjunctive constructions."
  },
  {
    "title_s": [
      "Système de gestion lexicale des ressources termino-ontologiques"
    ],
    "keyword_s": [
      "Data Warehousing",
      "Natural Language Processings",
      "Information Science",
      "Medical Informatics",
      "Science de l’informatique médicale",
      "Sciences de l’information",
      "Traitement automatique du langage naturel",
      "Entrepôt de données"
    ],
    "abstract_s": [
      "L’utilisation de services se basant sur un modèle de gestion lexicale permettrait d’apporter une aide aux processus de recherche d’informations dans les documents en texte libre. Ce document est la synthèse d’un travail réalisé au sein de l’unité IAM du CHU de Bordeaux, ayant pour but l’implémentation d’une structure se basant sur un modèle de gestion lexicale afin que des services et méthodes puissent y être développés. Le choix du modèle de gestion lexicale s’est porte sur Ontolex-Lemon, qui répondait le mieux à nos attentes. L’évaluation de cette structure a été faite en y intégrant quatre ressources termino-ontologiques, et des services et m´méthodes comme la détection de termes synonymes et polysémiques y ont été intègres.",
      "The use of services based on a lexical management model would help in the process of searching for information in free text documents. This document is the synthesis of a work carried out within the IAM unit of the CHU of Bordeaux, with the aim of implementing a structure based on a lexical management model so that services and methods can be developed there. The choice of the lexical management model was Ontolex-Lemon, which best met our expectations. The evaluation of this structure was done by integrating four termino-ontological resources, and services and methods such as synonym and polysemous terms detection were integrated."
    ],
    "authFullName_s": [
      "Guillaume Verdy"
    ],
    "halId_s": "dumas-03844324",
    "producedDateY_i": 2022,
    "texte_nettoye": "L’utilisation de services se basant sur un modèle de gestion lexicale permettrait d’apporter une aide aux processus de recherche d’informations dans les documents en texte libre. Ce document est la synthèse d’un travail réalisé au sein de l’unité IAM du CHU de Bordeaux, ayant pour but l’implémentation d’une structure se basant sur un modèle de gestion lexicale afin que des services et méthodes puissent y être développés. Le choix du modèle de gestion lexicale s’est porte sur Ontolex-Lemon, qui répondait le mieux à nos attentes. L’évaluation de cette structure a été faite en y intégrant quatre ressources termino-ontologiques, et des services et m´méthodes comme la détection de termes synonymes et polysémiques y ont été intègres."
  },
  {
    "title_s": [
      "Utilisation de la télédétection pour la détection et le suivi des algues brunes sur la réserve naturelle des terres australes françaises"
    ],
    "abstract_s": [
      "Cette étude vise à étudier la faisabilité de la détection des algues brunes, et principalement de la macroalgue Macrocystis pyrifera (Linnaeus) C. Agardh 1820, au sein du territoire des TAAF (Terres Australes et Antarctiques Françaises), dans le but d’extraire une donnée exhaustive quant à l’étendue des bancs visibles en surface dans un premier temps, et dans un second temps, de tester un algorithme de traitement automatisé. Notre objectif est de produire une cartographie fine de la couverture de Macrocystis pyrifera (MP) sur l’archipel de Kerguelen. Etant donné la proximité de MP avec Durvilleae Antarctica (DA) (du point de vue de leur signature spectrale), les résultats intègrent aussi cette dernière. Les données d’Observation de la Terre utilisée sont les images satellitaires libres et gratuites Sentinel-2 (A et B). Elles ont été téléchargées sur le portail d’accès de Copernicus. La chaîne de traitement mise en place est entièrement basée sur le langage R au sein de l’interface Rstudio (Version 1.3.1093). Ainsi, pour la totalité des traitements, de la requête des produits disponibles et du téléchargement jusqu’au produit final, la chaîne développée est entièrement automatique, reproductible et open source. Les téléchargements et prétraitements sont effectués grâce à une librairie (ensemble de fonctions R) complète, sen2r, créée par Luigi Ranghetti (L. Ranghetti et al., 2020). Les résultats de détection des algues brunes à partir de la chaîne sont proches de l’exhaustivité avec un Kappa global de 0.993. Des produits finaux clairs et utilisables par un utilisateur n’ayant pas forcément été initié à la télédétection seront favorisés. Plusieurs produits seront créés dans ce but, tel qu’une couche shapefile représentant la présence moyenne d’algue brune annuelle et globale (c.à.d. sur toute l’étendue temporelle disponible) ; ainsi que des couches rasters un peu plus complexes représentant des synthèses temporelles annuelles et globales sur la densité et la fréquence de présence de MP. Cette étude rentre dans le cadre d’un rapport final de stage de fin de Master 2. Les résultats de ce stage sont à destination des TAAF et de leurs partenaires."
    ],
    "authFullName_s": [
      "Alexis Pré"
    ],
    "halId_s": "hal-04198366",
    "producedDateY_i": 2020,
    "texte_nettoye": "Cette étude vise à étudier la faisabilité de la détection des algues brunes, et principalement de la macroalgue Macrocystis pyrifera (Linnaeus) C. Agardh 1820, au sein du territoire des TAAF (Terres Australes et Antarctiques Françaises), dans le but d’extraire une donnée exhaustive quant à l’étendue des bancs visibles en surface dans un premier temps, et dans un second temps, de tester un algorithme de traitement automatisé. Notre objectif est de produire une cartographie fine de la couverture de Macrocystis pyrifera (MP) sur l’archipel de Kerguelen. Etant donné la proximité de MP avec Durvilleae Antarctica (DA) (du point de vue de leur signature spectrale), les résultats intègrent aussi cette dernière. Les données d’Observation de la Terre utilisée sont les images satellitaires libres et gratuites Sentinel-2 (A et B). Elles ont été téléchargées sur le portail d’accès de Copernicus. La chaîne de traitement mise en place est entièrement basée sur le langage R au sein de l’interface Rstudio (Version 1.3.1093). Ainsi, pour la totalité des traitements, de la requête des produits disponibles et du téléchargement jusqu’au produit final, la chaîne développée est entièrement automatique, reproductible et open source. Les téléchargements et prétraitements sont effectués grâce à une librairie (ensemble de fonctions R) complète, sen2r, créée par Luigi Ranghetti (L. Ranghetti et al., 2020). Les résultats de détection des algues brunes à partir de la chaîne sont proches de l’exhaustivité avec un Kappa global de 0.993. Des produits finaux clairs et utilisables par un utilisateur n’ayant pas forcément été initié à la télédétection seront favorisés. Plusieurs produits seront créés dans ce but, tel qu’une couche shapefile représentant la présence moyenne d’algue brune annuelle et globale (c.à.d. sur toute l’étendue temporelle disponible) ; ainsi que des couches rasters un peu plus complexes représentant des synthèses temporelles annuelles et globales sur la densité et la fréquence de présence de MP. Cette étude rentre dans le cadre d’un rapport final de stage de fin de Master 2. Les résultats de ce stage sont à destination des TAAF et de leurs partenaires."
  },
  {
    "title_s": [
      "Détection automatique des infections du site opératoire"
    ],
    "keyword_s": [
      "Supervised machine learning",
      "Natural language processing",
      "Data warehousing",
      "Surgical wound infection",
      "Entreposage de données",
      "Traitement du langage naturel",
      "Apprentissage machine supervisé",
      "Infection de plaie opératoire"
    ],
    "abstract_s": [
      "Introduction - L'amélioration de la surveillance et de la prévention des infections du site opératoire (ISO) fait partie du programme national de lutte contre les infections nosocomiales. Notre objectif était de mettre en place un outil de détection automatique par apprentissage supervisé afin de remplacer le système de surveillance actuel. Méthode - Deux approches ont été menées pour détecter les ISO suite à une chirurgie du rachis et une neurochirurgie correspondant respectivement à 2133 et 2303 interventions. La première approche utilise les multiples sources d’information disponibles dans l’entrepôt de données du CHU de Bordeaux. La seconde approche utilise uniquement le texte libre. Pour chaque approche, nous avons comparé la précision de deux algorithmes, à savoir la régression logistique et les forêts aléatoires, avec un rappel fixé à 100%. Résultats - Le modèle final utilisant toutes les données a obtenu les meilleures performances pour la chirurgie du rachis avec une précision de 94%. Le modèle utilisant des données de texte libre a obtenu des résultats corrects et était meilleur pour la neurochirurgie. Discussion - L'utilisation du texte libre présente l'avantage d'être transposable à d'autres établissements de santé et facilement applicable à diverses spécialités chirurgicales avec des performances stables. Les performances de nos algorithmes doivent être évaluées sur un jeu de données test.",
      "Introduction Improving monitoring and prevention of surgical site infections (SSI) is part of the national nosocomial infection control program. Our goal was to implement an automated detection tool with a supervised machine learning to replace the current manual system. Method Two approaches were conducted to detect SSI following spine surgery and neurosurgery corresponding to 2133 and 2303 procedures respectively. The first approach uses the multiple sources of information available in the data warehouse of Bordeaux University Hospital. The second approach uses only free text. For each approach, we compared the precision of two algorithms namely logistic regression and random forests algorithms for a fixed recall value of 100%. Results The final model using all the data achieved the best performance for spine surgery with a precision of 94%. The model using free text data obtained correct results and was better for neurosurgery. Discussion The use of free text has the advantage of being replicable to other health institutions and applicable to various surgical specialties with stable performance. The performance of our algorithms needs to be evaluated on a test dataset."
    ],
    "authFullName_s": [
      "Marine Quéroué"
    ],
    "halId_s": "dumas-02420229",
    "producedDateY_i": 2019,
    "texte_nettoye": "Introduction - L'amélioration de la surveillance et de la prévention des infections du site opératoire (ISO) fait partie du programme national de lutte contre les infections nosocomiales. Notre objectif était de mettre en place un outil de détection automatique par apprentissage supervisé afin de remplacer le système de surveillance actuel. Méthode - Deux approches ont été menées pour détecter les ISO suite à une chirurgie du rachis et une neurochirurgie correspondant respectivement à 2133 et 2303 interventions. La première approche utilise les multiples sources d’information disponibles dans l’entrepôt de données du CHU de Bordeaux. La seconde approche utilise uniquement le texte libre. Pour chaque approche, nous avons comparé la précision de deux algorithmes, à savoir la régression logistique et les forêts aléatoires, avec un rappel fixé à 100%. Résultats - Le modèle final utilisant toutes les données a obtenu les meilleures performances pour la chirurgie du rachis avec une précision de 94%. Le modèle utilisant des données de texte libre a obtenu des résultats corrects et était meilleur pour la neurochirurgie. Discussion - L'utilisation du texte libre présente l'avantage d'être transposable à d'autres établissements de santé et facilement applicable à diverses spécialités chirurgicales avec des performances stables. Les performances de nos algorithmes doivent être évaluées sur un jeu de données test."
  },
  {
    "title_s": [
      "Caractérisation de la nasalité en contexte de parole : séparation du signal oral et nasal pour la recherche des corrélats de la nasalité dans le signal oral. Application au français et au mandarin"
    ],
    "keyword_s": [
      "Naxi language",
      "Mandarin",
      "French",
      "Na dialectology",
      "Articulatory",
      "Acoustico-perceptive cues",
      "Nasalization",
      "Nasality",
      "Acoustics",
      "Phonology",
      "Phonetics",
      "Dialectologie Na",
      "Phonétique",
      "Phonologie",
      "Acoustique",
      "Nasalité",
      "Nasalisation",
      "Corrélats acoustico-perceptifs",
      "Articulatoire",
      "Langue naxi",
      "Français"
    ],
    "abstract_s": [
      "Toute langue se construit par le jeu des oppositions phonologiques qui permettent de distinguer les phonèmes. Définie comme la caractéristique d’un phonème réalisé avec ouverture du port vélopharyngé, la nasalité donne lieu à des paires minimales dans la langue. La nasalité phonologique est présente massivement dans les langues du monde. 22 % des langues ont au moins une voyelle nasale et 96,5 % ont au moins une consonne nasale (données UPSID). En considérant la capacité de désychronisation du velum par rapport aux articulateurs buccaux, la nasalité est placée à un niveau supérieur de hiérarchie dans la théorie de Clements (1985), et par sa place dans l’inventaire phonologique du français, elle constitue un objet qui passionne la recherche. La complexité articulatori-acoustique du phénomène a souvent rendu difficiles les analyses couplées, d’où l’idée de séparer acoustiquement le signal oral et nasal. Partant ensuite de ce pattern appelé œil des nasales, très visible dans la partie orale d’une voyelle nasale, un procédé de calcul des dérivées de formants au cours du temps a été mis au point sur le signal séparé, de sorte que soient mesurées les variations conjointes des formants liées à l’apparition de l’œil des nasales. Le pincement ascendant des formants haute fréquence et le phénomène de densification basse fréquence est exploité en adossant des critères à la variation des formants, pourvu que les mesures soient précises et le traitement statistique de la variabilité pris en compte. La séparation acoustique oral – nasal permet également d’observer en phonologie de laboratoire les phénomènes d’anticipation et de persévérance du trait nasal, à l’image des mesures de durées de nasalisation différentes sur les codas nasales du mandarin de Taïwan. Le bénéfice attendu au niveau de la linguistique est de disposer d’un outil permettant de mieux décrire et comprendre les phénomènes d’émergence de la nasalité dans les langues du monde et d’en améliorer sa catégorisation.",
      "Any language is built based on the phonological oppositions that make phonemes distinct from one another. Nasality is defined as the opening of the velopharyngeal port, and the nasality feature leads to minimal pairs in language. Phonologically present in 22 % of UPSID as a vowel, and 96 % as a consonant, this feature is massively represented in the world’s languages. Considering that the velum can move in desynchronization with the oral tract articulators, nasality pertains to a higher level of hierarchy in Clements’ (1985) theory. Due to its particular status in the french phonological inventory, it represents a fascinating object for research. The articulatori-acoustic complexity of the phenomenon often made coupled analyses more difficult, hence the idea of separating acoustically the oral and nasal signal. A calculation method has been developed based on the pattern called « eye of the nasals », quite recognizable in the oral part of a nasalized vowel. It determines the formants’ order 1 differentiate value as a function of time, based solely on the oral part of a nasal vowel and from there criteria of joint temporal evolution of the formants are defined, as a characteristic of the appearance of the « eye of the nasals », and therefore nasality. The high frequency pinching of the formants and the low frequency densification phenomena are what causes the appearance of the « eye of the nasals ». In addition, the oral – nasal components separation allows us to observe phenomena of nasal anticipation and perseverance in Taiwan Mandarin Chinese. The benefits expected of this laboratory phonology study are to provide linguistics research with a tool that can help better describe and comprehend phenomena related to the emergence of nasality in the world’s languages."
    ],
    "authFullName_s": [
      "Maxime Fily"
    ],
    "halId_s": "dumas-01847016",
    "producedDateY_i": 2018,
    "texte_nettoye": "Toute langue se construit par le jeu des oppositions phonologiques qui permettent de distinguer les phonèmes. Définie comme la caractéristique d’un phonème réalisé avec ouverture du port vélopharyngé, la nasalité donne lieu à des paires minimales dans la langue. La nasalité phonologique est présente massivement dans les langues du monde. 22 % des langues ont au moins une voyelle nasale et 96,5 % ont au moins une consonne nasale (données UPSID). En considérant la capacité de désychronisation du velum par rapport aux articulateurs buccaux, la nasalité est placée à un niveau supérieur de hiérarchie dans la théorie de Clements (1985), et par sa place dans l’inventaire phonologique du français, elle constitue un objet qui passionne la recherche. La complexité articulatori-acoustique du phénomène a souvent rendu difficiles les analyses couplées, d’où l’idée de séparer acoustiquement le signal oral et nasal. Partant ensuite de ce pattern appelé œil des nasales, très visible dans la partie orale d’une voyelle nasale, un procédé de calcul des dérivées de formants au cours du temps a été mis au point sur le signal séparé, de sorte que soient mesurées les variations conjointes des formants liées à l’apparition de l’œil des nasales. Le pincement ascendant des formants haute fréquence et le phénomène de densification basse fréquence est exploité en adossant des critères à la variation des formants, pourvu que les mesures soient précises et le traitement statistique de la variabilité pris en compte. La séparation acoustique oral – nasal permet également d’observer en phonologie de laboratoire les phénomènes d’anticipation et de persévérance du trait nasal, à l’image des mesures de durées de nasalisation différentes sur les codas nasales du mandarin de Taïwan. Le bénéfice attendu au niveau de la linguistique est de disposer d’un outil permettant de mieux décrire et comprendre les phénomènes d’émergence de la nasalité dans les langues du monde et d’en améliorer sa catégorisation."
  },
  {
    "title_s": [
      "Deux romans, une intertextualité, deux approches : lire Meursault, contre-enquête de Kamel Daoud à la croisée des regards : OEil humain, oeil de la machine.",
      "Two novels, an intertextuality, two approaches: reading Kamel Daoud's Meursault, contre-enquête at the crossroads of human and machine perspectives."
    ],
    "keyword_s": [
      "Hyperbase",
      "Logometry",
      "Quantitative intertextuality",
      "Literary rewriting",
      "Réécriture littéraire",
      "Intertextualité quantitative",
      "Logométrie",
      "Hyperbase"
    ],
    "abstract_s": [
      "Cette recherche explore les territoires de l'intertextualité à travers la réécriture de L'Étranger de Camus par Kamel Daoud dans son roman Meursault, contre-enquête. À la croisée des approches littéraires et logométriques, cette étude conjugue l'analyse littéraire traditionnelle aux potentialités du traitement automatique via le logiciel Hyperbase. L’hybridité méthodologique invite à repenser la notion même d'intertextualité en mettant au jour sa pluralité conceptuelle et les différentes facettes des liens unissant l’hypertexte à son hypotexte. L'enjeu est d'importance : comment l'exploration automatisée du langage peut-elle appréhender un phénomène intimement lié à la mémoire du lecteur ? Ainsi, nous suivrons un parcours d'objectivation par la mesure, tout en exposant les apports de ce choix méthodologique et ses limites dans le cas étudié.",
      "This research explores the territories of intertextuality through Kamel Daoud's rewriting of Camus's L'Étranger in his novel Meursault, contre-enquête. At the crossroads of literary and logometric approaches, this study combines traditional literary analysis with the potential of automatic processing using Hyperbase software. This methodological hybridity invites us to rethink the very notion of intertextuality, bringing to light its conceptual plurality and the different facets of the links uniting hypertext to its hypotext. The stakes are high: how can automated language exploration apprehend a phenomenon intimately linked to the reader's memory? We'll follow a path of objectivation through measurement, while outlining the contributions of this methodological choice and its limitations in the case under study."
    ],
    "authFullName_s": [
      "Marwa Boukra"
    ],
    "halId_s": "hal-04923307",
    "producedDateY_i": 2024,
    "texte_nettoye": "Cette recherche explore les territoires de l'intertextualité à travers la réécriture de L'Étranger de Camus par Kamel Daoud dans son roman Meursault, contre-enquête. À la croisée des approches littéraires et logométriques, cette étude conjugue l'analyse littéraire traditionnelle aux potentialités du traitement automatique via le logiciel Hyperbase. L’hybridité méthodologique invite à repenser la notion même d'intertextualité en mettant au jour sa pluralité conceptuelle et les différentes facettes des liens unissant l’hypertexte à son hypotexte. L'enjeu est d'importance : comment l'exploration automatisée du langage peut-elle appréhender un phénomène intimement lié à la mémoire du lecteur ? Ainsi, nous suivrons un parcours d'objectivation par la mesure, tout en exposant les apports de ce choix méthodologique et ses limites dans le cas étudié."
  },
  {
    "title_s": [
      "Une exploration de l'architecture des réseaux de neurones pour la modélisation de la compositionnalité sémantique"
    ],
    "keyword_s": [
      "Linguistics",
      "Computer linguistics",
      "Compositionality",
      "Computer sciences",
      "Neural networks",
      "Machine learning",
      "Vectors",
      "Semantics",
      "Linguistique",
      "Sémantique",
      "Compositionnalité",
      "Vecteurs",
      "Réseaux de neurones",
      "Apprentissage automatique",
      "Informatique",
      "Traitement automatique des langues"
    ],
    "abstract_s": [
      "Ce mémoire présente une évaluation du modèle de réseau de neurones, appelé autoencodeur, qui permet de capturer le sens de couples adjectif-nom en anglais. Ce modèle fonctionne sur la base de la représentation du sens des mots par un vecteur contenant les indices des lemmes constituant le contexte des mots en question. Ces indices sont attribués aux lemmes en fonction de leur fréquence dans notre corpus issu de Wikipédia. Notre modèle est évalué sur un test de similarité entre deux couples adjectif-nom puis sur un test de recomposition des vecteurs de contexte du couple adjectif-nom à partir des vecteurs de contexte de ses composants pris séparément. Les résultats de ces deux tâches est ensuite comparé aux modèles déjà existants suivants : l'addition de vecteurs (modèle additif), le modèle additif pondéré avec un coefficient plus fort sur le vecteur du nom, le modèle basique où seul le vecteur contexte du nom est pris en compte, et la multiplication de vecteurs (modèle multiplicatif).",
      "This dissertation presents an evaluation of a neural network model called autoencoder in order to capture the meaning of adjective-noun couples in English. This model works on the representation of words meaning by a vector countaining the index of the words' context lemmas. These indexes are assigned to lemmas according to their frenquency in our Wikipedia-extracted corpus. Our model is evaluated on similarity task between two adjective-noun couples, and then on a task of recomposition of adjective-noun couples vector from their separated components context vectors. These two task results were eventually compared to the following already existing models : the vectors sum (additive model), the weighted additive with a stronger rating on the noun vector, the baseline model where only the nouns vector is taken, and the multiplicative model (multiplication of vectors)."
    ],
    "authFullName_s": [
      "Chloé Cimpello"
    ],
    "halId_s": "dumas-01212786",
    "producedDateY_i": 2015,
    "texte_nettoye": "Ce mémoire présente une évaluation du modèle de réseau de neurones, appelé autoencodeur, qui permet de capturer le sens de couples adjectif-nom en anglais. Ce modèle fonctionne sur la base de la représentation du sens des mots par un vecteur contenant les indices des lemmes constituant le contexte des mots en question. Ces indices sont attribués aux lemmes en fonction de leur fréquence dans notre corpus issu de Wikipédia. Notre modèle est évalué sur un test de similarité entre deux couples adjectif-nom puis sur un test de recomposition des vecteurs de contexte du couple adjectif-nom à partir des vecteurs de contexte de ses composants pris séparément. Les résultats de ces deux tâches est ensuite comparé aux modèles déjà existants suivants : l'addition de vecteurs (modèle additif), le modèle additif pondéré avec un coefficient plus fort sur le vecteur du nom, le modèle basique où seul le vecteur contexte du nom est pris en compte, et la multiplication de vecteurs (modèle multiplicatif)."
  },
  {
    "title_s": [
      "Conceptualisation d’un modèle de données complètement structurées et standardisées appliqué à un dossier patient informatisé"
    ],
    "keyword_s": [
      "Dossiers médicaux",
      "Dossier médical partagé",
      "Bases de données médico-administratives -- Qualité -- Contrôle",
      "Qualité des données",
      "Terminologies",
      "Données structurées",
      "Données médicales",
      "Dossier patient informatisé"
    ],
    "abstract_s": [
      "Le dossier patient informatisé ou DPI est un élément central dans la prise en charge médicale d’un patient. Il regroupe un nombre d’informations concernant le patient qu’un professionnel de santé doit connaître. Aujourd’hui, il existe de nombreux logiciels incluant le DPI mais toutes les données n’y sont pas totalement structurées. En France, le DPI, le Dossier Médical Partagé et l’Espace Numérique de Santé sont encadrés par la législation qui mentionne les éléments indispensables à retrouver dans un dossier patient. Des codes et terminologies médicales existent à des fins de descriptions médicales du patient et de son parcours de soins. Cette thèse donne une proposition des codes et terminologies les plus adéquats en regard de chaque élément d’un DPI. Cette proposition de structuration n’est pas parfaite et est discutable. En effet, la structuration permet une meilleure qualité des données pour les entrepôts de données de santé par exemple. Mais elle n’est pas toujours adaptée à l’activité médicale. Les éditeurs doivent adapter leur logiciel afin de transformer par des techniques de traitement du langage, le texte libre des médecins, en texte structuré pour améliorer la qualité des données médicales.L’apprentissage automatique et les réseaux de neurones sont très efficaces pour améliorer toutes les problématiques évoquées mais ils sont très sensibles à la qualité des données qui reste un défi à relever aujourd’hui."
    ],
    "authFullName_s": [
      "Laura Gosselin"
    ],
    "halId_s": "dumas-03959941",
    "producedDateY_i": 2022,
    "texte_nettoye": "Le dossier patient informatisé ou DPI est un élément central dans la prise en charge médicale d’un patient. Il regroupe un nombre d’informations concernant le patient qu’un professionnel de santé doit connaître. Aujourd’hui, il existe de nombreux logiciels incluant le DPI mais toutes les données n’y sont pas totalement structurées. En France, le DPI, le Dossier Médical Partagé et l’Espace Numérique de Santé sont encadrés par la législation qui mentionne les éléments indispensables à retrouver dans un dossier patient. Des codes et terminologies médicales existent à des fins de descriptions médicales du patient et de son parcours de soins. Cette thèse donne une proposition des codes et terminologies les plus adéquats en regard de chaque élément d’un DPI. Cette proposition de structuration n’est pas parfaite et est discutable. En effet, la structuration permet une meilleure qualité des données pour les entrepôts de données de santé par exemple. Mais elle n’est pas toujours adaptée à l’activité médicale. Les éditeurs doivent adapter leur logiciel afin de transformer par des techniques de traitement du langage, le texte libre des médecins, en texte structuré pour améliorer la qualité des données médicales.L’apprentissage automatique et les réseaux de neurones sont très efficaces pour améliorer toutes les problématiques évoquées mais ils sont très sensibles à la qualité des données qui reste un défi à relever aujourd’hui."
  },
  {
    "title_s": [
      "Application de la fusion de chemins de données à la synthèse d'architectures parallèles"
    ],
    "abstract_s": [
      "Le marché du système embarqué est le théâtre d'une véritable course à tous les niveaux (performance, miniaturisation, consommation électrique). En effet que ce soit pour le multimédia portable (consoles portables, appareils photos et caméras numériques...) ou pour les télécoms (émetteurs-récepteurs et téléphones 3 et bientôt 4G), la très forte concurrence oblige les systèmes à être de plus en plus performants (meilleure résolution, meilleure transmission...) et à avoir une plus grande autonomie, tout en conservant, voir en diminuant, leur taille. La plupart de ces systèmes embarqués ont un point commun : ils doivent exécuter des fonctions lourdes et répétitives dues au traitement d'images ou du signal. Il s'agit généralement de traitements successifs sur des matrices, utilisant des boucles imbriquées, qui si elles sont exécutées sur le processeur générique embarqué, l'alourdissent et le ralentissent. La synthèse de haut-niveau (HLS pour High-Level Synthesis), a pour but d'obtenir à partir d'un algorithme décrit dans un langage de programmation de haut-niveau (C, Matlab...) une description matérielle d'une puce exécutant de manière spécique cet algorithme. Le partage de ressources s'intéresse au sein de la HLS à la réduction de la surface de silicium utilisée par un circuit, et donc de son coût, par la réutilisation de composants matériels. C'est dans le cadre de la HLS que se situe ce stage, dont le but était d'obtenir un outil de conception automatique de circuits dédiés pour des algorithmes contenant des boucles imbriquées, permettant de partager les ressources matérielles au sein de ces circuits pour en diminuer le coût et la taille."
    ],
    "authFullName_s": [
      "Clément Guy"
    ],
    "halId_s": "dumas-00530700",
    "producedDateY_i": 2010,
    "texte_nettoye": "Le marché du système embarqué est le théâtre d'une véritable course à tous les niveaux (performance, miniaturisation, consommation électrique). En effet que ce soit pour le multimédia portable (consoles portables, appareils photos et caméras numériques...) ou pour les télécoms (émetteurs-récepteurs et téléphones 3 et bientôt 4G), la très forte concurrence oblige les systèmes à être de plus en plus performants (meilleure résolution, meilleure transmission...) et à avoir une plus grande autonomie, tout en conservant, voir en diminuant, leur taille. La plupart de ces systèmes embarqués ont un point commun : ils doivent exécuter des fonctions lourdes et répétitives dues au traitement d'images ou du signal. Il s'agit généralement de traitements successifs sur des matrices, utilisant des boucles imbriquées, qui si elles sont exécutées sur le processeur générique embarqué, l'alourdissent et le ralentissent. La synthèse de haut-niveau (HLS pour High-Level Synthesis), a pour but d'obtenir à partir d'un algorithme décrit dans un langage de programmation de haut-niveau (C, Matlab...) une description matérielle d'une puce exécutant de manière spécique cet algorithme. Le partage de ressources s'intéresse au sein de la HLS à la réduction de la surface de silicium utilisée par un circuit, et donc de son coût, par la réutilisation de composants matériels. C'est dans le cadre de la HLS que se situe ce stage, dont le but était d'obtenir un outil de conception automatique de circuits dédiés pour des algorithmes contenant des boucles imbriquées, permettant de partager les ressources matérielles au sein de ces circuits pour en diminuer le coût et la taille."
  },
  {
    "title_s": [
      "Réalisation d’un système Q&A spécialisé dans la qualité des eaux"
    ],
    "keyword_s": [
      "Système Q&A",
      "Qualité des eaux"
    ],
    "abstract_s": [
      "Le BRGM, Service géologique national, est l'établissement public de référence dans les applications des sciences de la Terre pour gérer les ressources et les risques du sol et du sous-sol. Son action est orientée vers la recherche scientifique, l'appui aux politiques publiques et la coopération internationale. Le BRGM développe notamment depuis plusieurs années une expertise reconnue à l’internationale des systèmes d’information dédiés à l’environnement, en particulier sur le domaine de l’eau, du site web portail de référence aux API web de diffusion des données environnementales, en passant par l’indexation de grands volumes de données et la modélisation prédictive. Parmis les outils developés par le BRGM, le portail Hub'eau qui comporte une dizaine d'API Rest chacune spécialisé dans un domaine en relation avec l'eau (qualité des eaux souterraines, rivières, poisson, etc). Pour permettre aux usagers intéressés par les problématiques environnementales de bénéficier des informations que peuvent offrir ces données, nous proposons de concevoir les premières briques d’un service numérique dont la fonction sera de répondre aux questions des utilisateurs, posées en langage naturel, de façon contextualisée et en se basant sur les données réelles du portail Hub'eau. Dans ce rapport nous commençons par une brève description de l'état de l'art en matière de chatbots et de systèmes questions-réponses. Par la suite, nous décrirons l'outil Hub'eau développé par le BRGM, nous aborderons les détails d'une des API du portail en précisant la procédure d'interrogation et le format des données. Nous poursuivrons par la description de la démarche entreprise pour le développement du système décrit ci-dessus. Nous terminerons par la présentation des résultats auxquels nous avons pu aboutir."
    ],
    "authFullName_s": [
      "Maya Touzari"
    ],
    "halId_s": "hal-03523094",
    "producedDateY_i": 2021,
    "texte_nettoye": "Le BRGM, Service géologique national, est l'établissement public de référence dans les applications des sciences de la Terre pour gérer les ressources et les risques du sol et du sous-sol. Son action est orientée vers la recherche scientifique, l'appui aux politiques publiques et la coopération internationale. Le BRGM développe notamment depuis plusieurs années une expertise reconnue à l’internationale des systèmes d’information dédiés à l’environnement, en particulier sur le domaine de l’eau, du site web portail de référence aux API web de diffusion des données environnementales, en passant par l’indexation de grands volumes de données et la modélisation prédictive. Parmis les outils developés par le BRGM, le portail Hub'eau qui comporte une dizaine d'API Rest chacune spécialisé dans un domaine en relation avec l'eau (qualité des eaux souterraines, rivières, poisson, etc). Pour permettre aux usagers intéressés par les problématiques environnementales de bénéficier des informations que peuvent offrir ces données, nous proposons de concevoir les premières briques d’un service numérique dont la fonction sera de répondre aux questions des utilisateurs, posées en langage naturel, de façon contextualisée et en se basant sur les données réelles du portail Hub'eau. Dans ce rapport nous commençons par une brève description de l'état de l'art en matière de chatbots et de systèmes questions-réponses. Par la suite, nous décrirons l'outil Hub'eau développé par le BRGM, nous aborderons les détails d'une des API du portail en précisant la procédure d'interrogation et le format des données. Nous poursuivrons par la description de la démarche entreprise pour le développement du système décrit ci-dessus. Nous terminerons par la présentation des résultats auxquels nous avons pu aboutir."
  },
  {
    "title_s": [
      "Impact sur la survie globale de la dysthyroïdie sous inhibiteur de checkpoint immunitaire"
    ],
    "keyword_s": [
      "Dysthyroïdie",
      "Cancer solide",
      "Inhibiteur de checkpoint immunitaire"
    ],
    "abstract_s": [
      "Contexte : le traitement médical des cancers solides a radicalement changé depuis le développement des inhibiteurs de checkpoint immunitaire (ICI). Cependant, les effets indésirables (EI) immuno-induits sont un défi dans la pratique courante. La dysthyroïdie est l’EI endocrinien le plus fréquent et certaines séries suggèrent que la dysthyroïdie pourrait être associée à l'efficacité des ICI. Ceci nous a conduit à explorer l'association entre la dysthyroïdie induite par les ICI et la survie globale (SG) dans une grande cohorte de patients atteints de tumeurs solides en utilisant l'exploration des données des dossiers patients informatisés (DPI). Patients et méthodes : ConSoRe est un outil d'analyse de données de nouvelle génération utilisant le Traitement Automatique du Langage (TAL) pour rechercher des données agrégées et effectuer une exploration de données avancée. Cet outil a été utilisé pour extraire des DPI les données des patients traités par ICI pour un cancer solide à l'Institut Paoli-Calmettes (Centre anticancéreux de Marseille, France). Les analyses de survie ont été réalisées par la méthode de Kaplan-Meier et comparées à l'aide du test log-rank (package R survminer). Une analyse landmark a été utilisée pour prendre en compte le biais d’immortalité. Dans l'analyse univariée et multivariée, un modèle de Cox à risques proportionnels a été utilisé pour estimer les variables associées à la SG. Le hazard Ratio (HR) a été estimé avec un intervalle de confiance à 95%. Deux modèles ont été réalisés, un modèle de Cox dit dépendant du temps et un modèle de Cox dit indépendant du temps. Résultats : l'extraction des données a permis d'identifier 1 385 patients traités par ICI entre 2011 et 2021. La survenue d’une dysthyroïdie a été observée chez 90 pts (7%). La dysthyroïdie a été associée à une meilleure SG (HR=0,46, 95%CI 0,33-0,65, p<0,001) avec une médiane de SG de 35,3 mois (mo) chez les patients atteints de dysthyroïdie (groupe DT) contre 15,4 mo chez les patients sans dysthyroïdie (groupe NDT). Lorsqu’un landmark à 6 mois était appliqué, l’impact de la dysthyroïdie sur la survie était similaire avec une médiane de SG passant de 25,5 mo (95%CI 22,8-27,8) dans le groupe NDT à 36,7 mo (95%CI 29,4-NR) dans le groupe DT. Dans l’analyse multivariée, la dysthyroïdie était indépendamment associée à une meilleure SG (HR=0,49, 95%CI 0,35-0,69, p=0,001). Après ajustement dans le modèle de Cox dépendant du temps, cette association était toujours significative (aHR=0,64 95%CI 0,45-0,90 p=0,010). Conclusion : la dysthyroïdie survenue chez les patients sous ICI était associées à une meilleure SG même après prise en compte du biais d'immortalité. L'apparition de la dysthyroïdie pourrait aider les oncologues à détecter les patients les plus susceptibles de bénéficier de l'ICI."
    ],
    "authFullName_s": [
      "Mathilde Beaufils"
    ],
    "halId_s": "dumas-03696996",
    "producedDateY_i": 2022,
    "texte_nettoye": "Contexte : le traitement médical des cancers solides a radicalement changé depuis le développement des inhibiteurs de checkpoint immunitaire (ICI). Cependant, les effets indésirables (EI) immuno-induits sont un défi dans la pratique courante. La dysthyroïdie est l’EI endocrinien le plus fréquent et certaines séries suggèrent que la dysthyroïdie pourrait être associée à l'efficacité des ICI. Ceci nous a conduit à explorer l'association entre la dysthyroïdie induite par les ICI et la survie globale (SG) dans une grande cohorte de patients atteints de tumeurs solides en utilisant l'exploration des données des dossiers patients informatisés (DPI). Patients et méthodes : ConSoRe est un outil d'analyse de données de nouvelle génération utilisant le Traitement Automatique du Langage (TAL) pour rechercher des données agrégées et effectuer une exploration de données avancée. Cet outil a été utilisé pour extraire des DPI les données des patients traités par ICI pour un cancer solide à l'Institut Paoli-Calmettes (Centre anticancéreux de Marseille, France). Les analyses de survie ont été réalisées par la méthode de Kaplan-Meier et comparées à l'aide du test log-rank (package R survminer). Une analyse landmark a été utilisée pour prendre en compte le biais d’immortalité. Dans l'analyse univariée et multivariée, un modèle de Cox à risques proportionnels a été utilisé pour estimer les variables associées à la SG. Le hazard Ratio (HR) a été estimé avec un intervalle de confiance à 95%. Deux modèles ont été réalisés, un modèle de Cox dit dépendant du temps et un modèle de Cox dit indépendant du temps. Résultats : l'extraction des données a permis d'identifier 1 385 patients traités par ICI entre 2011 et 2021. La survenue d’une dysthyroïdie a été observée chez 90 pts (7%). La dysthyroïdie a été associée à une meilleure SG (HR=0,46, 95%CI 0,33-0,65, p<0,001) avec une médiane de SG de 35,3 mois (mo) chez les patients atteints de dysthyroïdie (groupe DT) contre 15,4 mo chez les patients sans dysthyroïdie (groupe NDT). Lorsqu’un landmark à 6 mois était appliqué, l’impact de la dysthyroïdie sur la survie était similaire avec une médiane de SG passant de 25,5 mo (95%CI 22,8-27,8) dans le groupe NDT à 36,7 mo (95%CI 29,4-NR) dans le groupe DT. Dans l’analyse multivariée, la dysthyroïdie était indépendamment associée à une meilleure SG (HR=0,49, 95%CI 0,35-0,69, p=0,001). Après ajustement dans le modèle de Cox dépendant du temps, cette association était toujours significative (aHR=0,64 95%CI 0,45-0,90 p=0,010). Conclusion : la dysthyroïdie survenue chez les patients sous ICI était associées à une meilleure SG même après prise en compte du biais d'immortalité. L'apparition de la dysthyroïdie pourrait aider les oncologues à détecter les patients les plus susceptibles de bénéficier de l'ICI."
  },
  {
    "title_s": [
      "Utiliser un entrepôt de données de santé pour reproduire les résultats d’une cohorte vaccinale COVID-19 chez des patients atteints d’hypogammaglobulinémie",
      "Using a health data warehouse to recreate a COVID-19 vaccine cohort in patients with chronic disease"
    ],
    "keyword_s": [
      "Natural language processing",
      "Hypogammaglobulinemia",
      "Vaccination",
      "COVID-19",
      "Data Warehousing",
      "Hypogammaglobulinémie",
      "Vaccination",
      "COVID-19",
      "Traitement du langage naturel",
      "Entreposage de données"
    ],
    "abstract_s": [
      "Contexte : dans les situations où des investigations urgentes sont nécessaires, il est coûteux en temps et en argent de créer et de suivre des cohortes de patients. L’utilisation d’un entrepôt de données de santé (EDS) est envisageable pour faciliter la création de cohortes. Méthodes : à partir des données structurées et non structurées de l’EDS du CHU de Bordeaux, nous avons reconstitué une cohorte vaccinale COVID-19 de patients ayant une hypogammaglobulinémie primitive ou secondaire, vaccinés contre le COVID-19 en 2021 et avec des résultats de sérologie vaccinale dans les 12 premiers mois suivant cette vaccination. Nous avons développé un algorithme de traitement automatique de la langue pour rechercher la vaccination COVID-19 et les résultats de sérologie dans les documents textuels du dossier médical. Nous avons décrit la réponse humorale à la vaccination dans cette population à 1, 6, 12 et 24 mois après la deuxième dose de vaccin à partir des titres d’anticorps anti-Spike. Résultats : la requête initiale a permis d’identifier 2 065 patients éligibles à l’inclusion. Avec les méthodes employées et les données disponibles, nous avons inclus 258 patients dans l’analyse finale. L’âge médian des patients était de 67 ans. Le sex-ratio était équilibré avec 52,7 % d’hommes. Parmi les patients avec un résultat de sérologie, 63,6 % (63/99) étaient répondeurs à M1, 74,4 % (134/180) étaient répondeurs à M6 et 92,4 % (159/172) étaient répondeurs à M12. Conclusion : nous avons créé une cohorte vaccinale à partir d’un EDS. La réponse humorale à la vaccination COVID-19 est diminuée chez les patients ayant une hypogammaglobulinémie.",
      "Background: in situations where urgent investigations are required, it is costly and time consuming to create and monitor cohorts. Using a health data warehouse (HDW) could support the creation of cohorts. Methods : using structured and unstructured data from the HDW of Bordeaux, we reconstituted a COVID-19 vaccine cohort of patients with primary or secondary hypogammaglobulinemia, vaccinated against COVID-19 in 2021 and with serological response results within the first 12 months following the vaccination. We developed a natural language processing algorithm to detect COVID-19 vaccination and serological results in medical records text documents. We described the humoral response to vaccination in this population at 1, 6, 12 and 24 months after the second dose of vaccine based on anti-Spike antibody titres. Results: the initial query identified 2 065 patients eligible for inclusion. With the methods used and the available data, we included 258 patients in the final analysis. The median age of the patients was 67 years and 52.7 % were men. Among patients with a serological result, 63.6% (63/99) were responders at M1, 74.4% (134/180) were responders at M6 and 92.4% (159/172) were responders at M12. Conclusion: we created a vaccination cohort from a HDW. The humoral response to COVID-19 vaccination is reduced in patients with hypogammaglobulinemia."
    ],
    "authFullName_s": [
      "Adam Loffler"
    ],
    "halId_s": "dumas-04814609",
    "producedDateY_i": 2024,
    "texte_nettoye": "Contexte : dans les situations où des investigations urgentes sont nécessaires, il est coûteux en temps et en argent de créer et de suivre des cohortes de patients. L’utilisation d’un entrepôt de données de santé (EDS) est envisageable pour faciliter la création de cohortes. Méthodes : à partir des données structurées et non structurées de l’EDS du CHU de Bordeaux, nous avons reconstitué une cohorte vaccinale COVID-19 de patients ayant une hypogammaglobulinémie primitive ou secondaire, vaccinés contre le COVID-19 en 2021 et avec des résultats de sérologie vaccinale dans les 12 premiers mois suivant cette vaccination. Nous avons développé un algorithme de traitement automatique de la langue pour rechercher la vaccination COVID-19 et les résultats de sérologie dans les documents textuels du dossier médical. Nous avons décrit la réponse humorale à la vaccination dans cette population à 1, 6, 12 et 24 mois après la deuxième dose de vaccin à partir des titres d’anticorps anti-Spike. Résultats : la requête initiale a permis d’identifier 2 065 patients éligibles à l’inclusion. Avec les méthodes employées et les données disponibles, nous avons inclus 258 patients dans l’analyse finale. L’âge médian des patients était de 67 ans. Le sex-ratio était équilibré avec 52,7 % d’hommes. Parmi les patients avec un résultat de sérologie, 63,6 % (63/99) étaient répondeurs à M1, 74,4 % (134/180) étaient répondeurs à M6 et 92,4 % (159/172) étaient répondeurs à M12. Conclusion : nous avons créé une cohorte vaccinale à partir d’un EDS. La réponse humorale à la vaccination COVID-19 est diminuée chez les patients ayant une hypogammaglobulinémie."
  },
  {
    "title_s": [
      "Développement d’un outil de segmentation et de classification semi-automatique de pierres"
    ],
    "keyword_s": [
      "Digital image processing",
      "Segmentation",
      "Edge detection",
      "Stones",
      "Python programmation",
      "Graphical interface",
      "Supervised classification",
      "Interface graphique",
      "Classification supervisée",
      "Programmation Python",
      "Pierres",
      "Détection de contour",
      "Traitement numérique des images"
    ],
    "abstract_s": [
      "Le travail des archéologues pour étudier les étapes de construction et de reconstruction des façades est aujourd’hui fastidieux entre autre parce qu’ils doivent, à la main, dessiner et qualifier les pierres des murs. Afin de résoudre ce problème, nous avons développé, en collaboration avec le CAPRA, un ensemble d’outils. Une première partie, liée à la détection des contours, est de programmer, dans le langage Python, une interface graphique et d’y implémenter notre algorithme de segmentation. La seconde partie, concernant la classification supervisée du résultat automatique, utilise une succession de deux outils intégrés à l’interface d’ArcMap pour générer automatiquement la classification de ces pierres à partir de l’image segmentée. Les outils se voulant semi-automatiques, l’utilisateur peut modifier à chaque fois les résultats produits. Cependant, des limitations sont bien présentes et particulièrement concernant l’algorithme de segmentation qui n’est pas complètement ajusté à la détection des pierres sur des murs dégradés par le temps et la nature, ou dont les joints ne sont pas propres.",
      "Today, the work of archaeologists to study the steps of construction and reconstruction of facades is boring because they have to draw and qualify stones of walls. In order to solve this problem, we developed a set of tools. The first part, related to edge detection, is to program, using the informatic language Python, a graphical user interface and implementing our segmentation algorithm in it. The second part, concerning the supervised classification of our automatic result, uses a succession of two tools integrated in the software ArcMap to generate automatically the classification of these stones from the segmented image. Tools aiming to be semi-automatic, the user can modify every time the produced results. However, limitations are always present and particularly concerning the segmentation algorithm which is not completely adjusted to the detection of stones on degraded walls by weather or nature or whose joints are not clean."
    ],
    "authFullName_s": [
      "Pierre-Alban 6-02-1992 Hugueny"
    ],
    "halId_s": "dumas-01168266",
    "producedDateY_i": 2014,
    "texte_nettoye": "Le travail des archéologues pour étudier les étapes de construction et de reconstruction des façades est aujourd’hui fastidieux entre autre parce qu’ils doivent, à la main, dessiner et qualifier les pierres des murs. Afin de résoudre ce problème, nous avons développé, en collaboration avec le CAPRA, un ensemble d’outils. Une première partie, liée à la détection des contours, est de programmer, dans le langage Python, une interface graphique et d’y implémenter notre algorithme de segmentation. La seconde partie, concernant la classification supervisée du résultat automatique, utilise une succession de deux outils intégrés à l’interface d’ArcMap pour générer automatiquement la classification de ces pierres à partir de l’image segmentée. Les outils se voulant semi-automatiques, l’utilisateur peut modifier à chaque fois les résultats produits. Cependant, des limitations sont bien présentes et particulièrement concernant l’algorithme de segmentation qui n’est pas complètement ajusté à la détection des pierres sur des murs dégradés par le temps et la nature, ou dont les joints ne sont pas propres."
  },
  {
    "title_s": [
      "Apport des entrepôts de données de santé dans l’identification des besoins en soins palliatifs non repérés : une étude de la portée"
    ],
    "keyword_s": [
      "Soins palliatifs",
      "Médecine générale",
      "Entreposage de données",
      "Examen de la portée"
    ],
    "abstract_s": [
      "Introduction : Les soins palliatifs offrent une approche globale centrée sur le patient pour améliorer sa qualité de vie. En première ligne, le médecin généraliste joue un rôle central de repérage, de coordination et de soutien, mais fait face à des difficultés liées à la sous-détection des symptômes et à l’incertitude. Dans ce contexte, les entrepôts de données de santé (EDS) offrent de nouvelles perspectives pour repérer les besoins non identifiés en soins palliatifs. Cette étude vise à dresser un état des lieux de l’usage de ces données pour identifier les besoins en soins palliatifs non repérés. Matériel et méthode : Cette scoping review, conduite selon les recommandations PRISMA-ScR 2019, a inclus des études quantitatives utilisant des données d’entrepôts de santé pour identifier des adultes atteints de maladies potentiellement mortelles. La sélection, l’extraction et l’analyse des données ont été réalisées de manière rigoureuse et indépendante par deux investigateurs, à partir d’une recherche systématique sur plusieurs bases de données effectuée jusqu’en décembre 2024. Résultats : Les 49 études incluses portaient majoritairement sur des patients hospitalisés ou atteints de pathologies oncologiques, avec des effectifs très variables. La majorité utilisait des entrepôts hospitaliers, des données structurées et, plus rarement, des données non structurées ou des résultats rapportés par les patients (PRO). Les objectifs étaient principalement l’identification du risque de décès ou de symptômes, avec des modèles basés sur l’apprentissage automatique, les scores cliniques ou le traitement du langage naturel pour les données non structurées. Discussion : Les modèles se développent principalement en milieu hospitalier, alors que l’ambulatoire et les maladies chroniques restent peu représentés et difficiles à modéliser. Les ePRO améliorent la détection des symptômes et la qualité des échanges, mais leur intégration reste limitée par des contraintes techniques et numériques. Les données non structurées, issues des notes cliniques ou comptes rendus, enrichissent les modèles prédictifs, notamment via le NLP bien que leur qualité varie selon les pratiques et logiciels. Conclusion : Les résultats montrent que les EDS permettent de repérer les besoins palliatifs non identifiés mais des études sur la population ambulatoire, les maladies chroniques et les données non structurées sont nécessaires."
    ],
    "authFullName_s": [
      "Hugo Métaireau"
    ],
    "halId_s": "dumas-05481376",
    "producedDateY_i": 2025,
    "texte_nettoye": "Introduction : Les soins palliatifs offrent une approche globale centrée sur le patient pour améliorer sa qualité de vie. En première ligne, le médecin généraliste joue un rôle central de repérage, de coordination et de soutien, mais fait face à des difficultés liées à la sous-détection des symptômes et à l’incertitude. Dans ce contexte, les entrepôts de données de santé (EDS) offrent de nouvelles perspectives pour repérer les besoins non identifiés en soins palliatifs. Cette étude vise à dresser un état des lieux de l’usage de ces données pour identifier les besoins en soins palliatifs non repérés. Matériel et méthode : Cette scoping review, conduite selon les recommandations PRISMA-ScR 2019, a inclus des études quantitatives utilisant des données d’entrepôts de santé pour identifier des adultes atteints de maladies potentiellement mortelles. La sélection, l’extraction et l’analyse des données ont été réalisées de manière rigoureuse et indépendante par deux investigateurs, à partir d’une recherche systématique sur plusieurs bases de données effectuée jusqu’en décembre 2024. Résultats : Les 49 études incluses portaient majoritairement sur des patients hospitalisés ou atteints de pathologies oncologiques, avec des effectifs très variables. La majorité utilisait des entrepôts hospitaliers, des données structurées et, plus rarement, des données non structurées ou des résultats rapportés par les patients (PRO). Les objectifs étaient principalement l’identification du risque de décès ou de symptômes, avec des modèles basés sur l’apprentissage automatique, les scores cliniques ou le traitement du langage naturel pour les données non structurées. Discussion : Les modèles se développent principalement en milieu hospitalier, alors que l’ambulatoire et les maladies chroniques restent peu représentés et difficiles à modéliser. Les ePRO améliorent la détection des symptômes et la qualité des échanges, mais leur intégration reste limitée par des contraintes techniques et numériques. Les données non structurées, issues des notes cliniques ou comptes rendus, enrichissent les modèles prédictifs, notamment via le NLP bien que leur qualité varie selon les pratiques et logiciels. Conclusion : Les résultats montrent que les EDS permettent de repérer les besoins palliatifs non identifiés mais des études sur la population ambulatoire, les maladies chroniques et les données non structurées sont nécessaires."
  },
  {
    "title_s": [
      "A Study about Explainability and Fairness in Machine Learning and Knowledge Discovery"
    ],
    "keyword_s": [
      "Machine learning",
      "Explainability",
      "Fairness",
      "Antibiotic classification",
      "Machine learning",
      "Explicabilité",
      "Fairness",
      "Classification antibiotique"
    ],
    "abstract_s": [
      "Progress in machine learning, more recently accelerated by deep learning, now makes possible to build high-performance models for recognition or generation tasks. However, these models are complex, making it difficult to justify their predictions. To avoid a \"black box\" effect and for an interpretability reason, the Orpailleur team is interested in model explanations, a research field that is still recent. These explainers manage to spot biased predictions of some models (e.g., decisions mainly based on skin color). The discoveries of the Orpailleur team show that it is possible to make predictions fairer (we generally talk about \"fairness\") thanks to ensemble methods and variable dropping, without altering the models performance. We particularly focus here on the specific case of textual data, as well as on an adaptation to deep learning models which hold a prominent place in natural language processing. Secondly, we will use these explainers on antibiotic classification models in order to determine how such a classifier reaches the conclusion that a molecule has antibiotic properties. This work will lead to the creation of an interactive web interface to highlight the important patterns learned by these models, through visualizations.",
      "Les progrès en machine learning, plus récemment accélérés par le deep learning, permettent désormais de construire des modèles performants dans des tâches de reconnaissance ou de génération. Ces modèles sont cependant complexes, si bien qu'il devient difficile de justifier leurs prédictions. Pour éviter un effet « boîte noire » et dans un soucis d'interprétabilité, l'équipe Orpailleur s'intéresse aux explicateurs de modèles, un domaine de recherche encore récent. Ces explicateurs permettent, entre autres, de mettre en lumière les prédictions biaisées de certains modèles (exemple : décision principalement basée sur la couleur de peau). Les découvertes de l'équipe Orpailleur montrent qu'il est possible de rendre les prédictions plus justes (on parle généralement de « fairness ») grâce à des méthodes ensemblistes et de suppression de variables, sans altérer les performances des modèles. Nous nous intéressons plus particulièrement ici au cas spécifique des données textuelles, ainsi qu'à une adaptation aux modèles de deep learning qui occupent une place de premier choix dans le traitement automatique du langage. Dans un second temps, nous utilisons ces explicateurs sur des modèles de classification d'antibiotiques afin de déterminer comment un tel classificateur parvient à la conclusion qu'une molécule possède des propriétés antibiotiques. Ces travaux conduiront à la création d'une interface web interactive permettant de mettre en valeur les motifs importants appris par ces modèles, au travers de visualisations."
    ],
    "authFullName_s": [
      "Fabien Bernier"
    ],
    "halId_s": "hal-03371070",
    "producedDateY_i": 2021,
    "texte_nettoye": "Progress in machine learning, more recently accelerated by deep learning, now makes possible to build high-performance models for recognition or generation tasks. However, these models are complex, making it difficult to justify their predictions. To avoid a \"black box\" effect and for an interpretability reason, the Orpailleur team is interested in model explanations, a research field that is still recent. These explainers manage to spot biased predictions of some models (e.g., decisions mainly based on skin color). The discoveries of the Orpailleur team show that it is possible to make predictions fairer (we generally talk about \"fairness\") thanks to ensemble methods and variable dropping, without altering the models performance. We particularly focus here on the specific case of textual data, as well as on an adaptation to deep learning models which hold a prominent place in natural language processing. Secondly, we will use these explainers on antibiotic classification models in order to determine how such a classifier reaches the conclusion that a molecule has antibiotic properties. This work will lead to the creation of an interactive web interface to highlight the important patterns learned by these models, through visualizations."
  },
  {
    "title_s": [
      "Intérêt de l'intelligence artificielle pour l'extraction automatisée des patients présentant une douleur thoracique notifiée dans le dossier de régulation médicale du SAMU 33",
      "Interest of artificial intelligence for the automated extraction of patients with chest pain notified in the medical regulation file of the EMS 33"
    ],
    "keyword_s": [
      "Myocardial infarction",
      "Health",
      "EMS",
      "Emergency",
      "Classification",
      "Algorithm",
      "Deep learning",
      "Artificial intelligence",
      "Chest pain",
      "Medical regulation",
      "Santé",
      "SAMU",
      "Urgences",
      "Classification",
      "Algorithme",
      "Apprentissage profond",
      "Infarctus du myocarde",
      "Régulation médicale",
      "Intelligence artificielle",
      "Douleur thoracique"
    ],
    "abstract_s": [
      "Introduction : la douleur thoracique correspond à l’un des motifs fréquents de recours au centre 15. Depuis quelques années, l’intelligence artificielle s’ancre progressivement dans le milieu de la santé dans un but d’améliorer les performances et les prises de décisions des médecins. L’objectif principal de ce travail de thèse était d’évaluer les performances d’un modèle de classification type « transformer » dans l’extraction automatisée des informations contenues dans un dossier de régulation médicale. Méthode : ce travail a été mené sur le CHU de Bordeaux. Un échantillon de l’ensemble des dossiers de 2009 à 2021, a été soumis à un transformer correspondant à un logiciel de traitement automatique du langage. Après un entrainement préalable, la précision du modèle a été obtenue par comparaison de la prédiction faite du modèle versus l’annotation individuelle (gold standard) de 1000 dossiers. Résultat : 2237 dossiers ont été utilisés pour l’entrainement supervisé du modèle. Au décours, la comparaison des prédictions par le modèle sur 1000 nouveaux dossiers versus notre propre analyse personnelle, a permis d’objectiver une sensibilité de 0,90 (95% IC, 0,88-0,91) et une spécificité de 0,98 (95% IC, 0,97-0,98). La précision du modèle était de 0,99 (95% IC, 0,98-0,99) pour le score « autre » et de 0,85 (95% IC, 0,78-0,92) pour le score « douleur thoracique ». L’application du modèle sur l’ensemble des données a permis d’isoler 360 000 douleurs thoraciques (soit 9% des données) avec des indices de confiance très corrects. Discussion : l’étude révèle une nette performance du transformer dans l’extraction des douleurs thoraciques au sein des dossiers de régulation. Ce projet pourrait rendre exploitable la base de données non structurées du SAMU 33 à des fins épidémiologiques. La liberté d’écriture du médecin régulateur représente une des limites de l’étude. La validité du gold standard peut poser problème. Les similitudes syntaxiques, la négation et la saisie des présentations atypiques restent une voie d’apprentissage. L’approche de l’intelligence artificielle dans ce secteur représente une véritable voie d’avenir.",
      "Introduction: chest pain is one of the most frequent reasons for calling the 15 center. In recent years, artificial intelligence has been progressively introduced in the health sector with the aim of improving the performance and decision making of physicians. The main objective of this thesis work was to evaluate the performance of a \"transformer\" type classification model in the automated classification of information contained in a medical regulation file. Method: this study was conducted in the Bordeaux University Hospital. A sample of all the files from 2009 to 2021 was subjected to a transformer corresponding to an automatic language processing software. After prior training, the accuracy of the model was obtained by comparing the prediction made by the model with the individual annotation (gold standard) of 1000 records. Result: 2237 records were used for supervised training of the model. In the course of this, the comparison of the model's predictions on 1000 new records versus our own personal analysis resulted in a sensitivity of 0.90 (95% CI, 0,88-0,91) and a specificity of 0.98 (95% CI, 0,97-0,98). The accuracy was 0.99 (95% CI, 0,98-0,99) for the \"other\" score and 0.85 (95% CI, 0,78-0,92) for the \"chest pain\" score. Applying the model to the entire data set allowed us to isolate 360,000 chest pains (i.e. 9% of the data) with very good confidence indices. Discussion: the study shows a clear performance of the transformer in the extraction of chest pain from the regulation files. This project could make the unstructured database of the EMS 33 usable for epidemiological purposes. The freedom of writing of the regulating doctor represents one of the limits of the study. The validity of the gold standard can pose a problem. Syntactic similarities, negation and the capture of atypical presentations remain a learning curve. The artificial intelligence approach in this area is a real way forward."
    ],
    "authFullName_s": [
      "Guillaume Gnyp"
    ],
    "halId_s": "dumas-03849849",
    "producedDateY_i": 2022,
    "texte_nettoye": "Introduction : la douleur thoracique correspond à l’un des motifs fréquents de recours au centre 15. Depuis quelques années, l’intelligence artificielle s’ancre progressivement dans le milieu de la santé dans un but d’améliorer les performances et les prises de décisions des médecins. L’objectif principal de ce travail de thèse était d’évaluer les performances d’un modèle de classification type « transformer » dans l’extraction automatisée des informations contenues dans un dossier de régulation médicale. Méthode : ce travail a été mené sur le CHU de Bordeaux. Un échantillon de l’ensemble des dossiers de 2009 à 2021, a été soumis à un transformer correspondant à un logiciel de traitement automatique du langage. Après un entrainement préalable, la précision du modèle a été obtenue par comparaison de la prédiction faite du modèle versus l’annotation individuelle (gold standard) de 1000 dossiers. Résultat : 2237 dossiers ont été utilisés pour l’entrainement supervisé du modèle. Au décours, la comparaison des prédictions par le modèle sur 1000 nouveaux dossiers versus notre propre analyse personnelle, a permis d’objectiver une sensibilité de 0,90 (95% IC, 0,88-0,91) et une spécificité de 0,98 (95% IC, 0,97-0,98). La précision du modèle était de 0,99 (95% IC, 0,98-0,99) pour le score « autre » et de 0,85 (95% IC, 0,78-0,92) pour le score « douleur thoracique ». L’application du modèle sur l’ensemble des données a permis d’isoler 360 000 douleurs thoraciques (soit 9% des données) avec des indices de confiance très corrects. Discussion : l’étude révèle une nette performance du transformer dans l’extraction des douleurs thoraciques au sein des dossiers de régulation. Ce projet pourrait rendre exploitable la base de données non structurées du SAMU 33 à des fins épidémiologiques. La liberté d’écriture du médecin régulateur représente une des limites de l’étude. La validité du gold standard peut poser problème. Les similitudes syntaxiques, la négation et la saisie des présentations atypiques restent une voie d’apprentissage. L’approche de l’intelligence artificielle dans ce secteur représente une véritable voie d’avenir."
  }
]