[
  {
    "title_s": [
      "Langages documentaires et nouvelles technologies : l'avenir des langages et leur positionnement au cœur des systèmes d'informations dans le contexte de la presse."
    ],
    "keyword_s": [
      "Langage documentaire",
      "Langage naturel",
      "Accès à l'information",
      "Système de recherche d'information",
      "TAL traitement automatique du langage",
      "Indexation automatique",
      "Surindexation",
      "Moteur de recherche",
      "Thésaurus",
      "Presse"
    ],
    "abstract_s": [
      "L'auteur définit dans un premier temps les fonctions du langage documentaire (normalisation, désambiguïsation, organisation des connaissances et structuration), et le fonctionnement de l'indexation automatique et des différents traitements linguistiques, statistiques et sémantiques. Puis il replace la problématique de la recherche d'information dans le contexte particulier de la presse et décrit les systèmes d'informations de Bayard, du Monde, des Échos et du Nouvel Observateur. Ensuite, en partant des besoins d'informations des journalistes, il tente de définir les rôles et les fonctions des langages documentaires face à l'utilisation des nouvelles technologies en fonction d'une typologie de l'indexation. Dans les systèmes d'informations presse, les outils d'indexation automatique et de recherche en texte intégral peuvent prendre en charge le traitement linguistique et sémantique, la classification automatique, etc. Pourtant le rôle des langages documentaires et la place de l'indexation humaine restent importants. Un usage sélectif de l'indexation manuelle permet une meilleure conceptualisation du sujet et une véritable structuration des documents en fonction des types d'information. L'intégration des langages améliore les performances de la recherche sémantique et permet une meilleure normalisation et désambiguïsation du langage naturel. Les fonctions d'organisation des connaissances, de médiation et d'aide à la recherche des langages documentaires viennent en complémentarité des classifications et de l'hypertexte. Le langage documentaire apparaît en complémentarité de la recherche en texte intégral. Même s'il devient de plus en plus invisible, intégré aux nouvelles technologies, ces fonctions sont préservées et demeurent indispensables."
    ],
    "authFullName_s": [
      "Odile Contat"
    ],
    "halId_s": "mem_00000057",
    "producedDateY_i": 2003,
    "texte_nettoye": "L'auteur définit dans un premier temps les fonctions du langage documentaire (normalisation, désambiguïsation, organisation des connaissances et structuration), et le fonctionnement de l'indexation automatique et des différents traitements linguistiques, statistiques et sémantiques. Puis il replace la problématique de la recherche d'information dans le contexte particulier de la presse et décrit les systèmes d'informations de Bayard, du Monde, des Échos et du Nouvel Observateur. Ensuite, en partant des besoins d'informations des journalistes, il tente de définir les rôles et les fonctions des langages documentaires face à l'utilisation des nouvelles technologies en fonction d'une typologie de l'indexation. Dans les systèmes d'informations presse, les outils d'indexation automatique et de recherche en texte intégral peuvent prendre en charge le traitement linguistique et sémantique, la classification automatique, etc. Pourtant le rôle des langages documentaires et la place de l'indexation humaine restent importants. Un usage sélectif de l'indexation manuelle permet une meilleure conceptualisation du sujet et une véritable structuration des documents en fonction des types d'information. L'intégration des langages améliore les performances de la recherche sémantique et permet une meilleure normalisation et désambiguïsation du langage naturel. Les fonctions d'organisation des connaissances, de médiation et d'aide à la recherche des langages documentaires viennent en complémentarité des classifications et de l'hypertexte. Le langage documentaire apparaît en complémentarité de la recherche en texte intégral. Même s'il devient de plus en plus invisible, intégré aux nouvelles technologies, ces fonctions sont préservées et demeurent indispensables."
  },
  {
    "title_s": [
      "Découverte automatique des textes littéraires qui présentent les caractéristiques statistiques d'un texte de qualité"
    ],
    "keyword_s": [
      "Automatic language processing",
      "Automatic evaluation of texts",
      "Learning",
      "Apprentissage",
      "Classification",
      "Évaluation automatique de textes",
      "Traitement automatique de la langue"
    ],
    "abstract_s": [
      "Le domaine du traitement automatique des langues naturelles a connu des évolutions très rapides ces dernières années, et spécialement les méthodes de statistique textuelle. Elles ont été mises en lumière par plusieurs disciplines : l'étude des textes, la linguistique, l'analyse du discours, la statistique, l'informatique, le traitement des enquêtes. Ce projet de recherche s'inscrit dans le cadre de la problématique de Short Edition qui concerne l'éditeur communautaire de littérature courte. L'objectif est d'assister le travail du comité de lecture en effectuant une première catégorisation des textes. Notre travail implique la conception et la mise en œuvre d'un prototype permettant de repérer les textes qui présentent les caractéristiques d'un texte de qualité et de trouver une méthode de classification en nous fondant sur les principes de la fouille de données permettant de bien classer nos textes.",
      "The field of natural language processing has witnessed very rapid developments in recent years, particularly with respect to methods used for statistical text analysis. These methods have been brought into focus by several disciplines in particular : the study of texts, linguistics, discours analysis, statistics, computer sciences, and survey processing. This research project develops within the framework of an issue that concerns the publishing company Short Editions. It relies on contributions in a field that employs a vast variety of designations (lexical statistics, statistical linguistics, quantitative linguistics, etcetera). Our work involves the creation of a prototype that allows for the identification of texts that present the characteristics of a quality text and to find appropriate methods of classification of these texts based on data and text mining principles."
    ],
    "authFullName_s": [
      "Hamza Maaouia"
    ],
    "halId_s": "dumas-01066867",
    "producedDateY_i": 2014,
    "texte_nettoye": "Le domaine du traitement automatique des langues naturelles a connu des évolutions très rapides ces dernières années, et spécialement les méthodes de statistique textuelle. Elles ont été mises en lumière par plusieurs disciplines : l'étude des textes, la linguistique, l'analyse du discours, la statistique, l'informatique, le traitement des enquêtes. Ce projet de recherche s'inscrit dans le cadre de la problématique de Short Edition qui concerne l'éditeur communautaire de littérature courte. L'objectif est d'assister le travail du comité de lecture en effectuant une première catégorisation des textes. Notre travail implique la conception et la mise en œuvre d'un prototype permettant de repérer les textes qui présentent les caractéristiques d'un texte de qualité et de trouver une méthode de classification en nous fondant sur les principes de la fouille de données permettant de bien classer nos textes."
  },
  {
    "title_s": [
      "Gestion de corpus linguistiques : fusion de la Linguistique et du Traitement Automatique des Langues dans le projet PREFAB"
    ],
    "keyword_s": [
      "Natural language processing",
      "Similarity calculation",
      "Data analysis",
      "Translation",
      "Parallel corpus",
      "Prefabricated sentences",
      "Traitement automatique des langues",
      "Calcul de similarité",
      "Analyse de données",
      "Traduction",
      "Corpus parallèle",
      "Phrases préfabriquées"
    ],
    "abstract_s": [
      "Ce mémoire présente une partie des actions indispensables dans le cadre du projet PREFAB, basé sur l'analyse de corpus bilingues. Il met en lumière le fonctionnement de différents outils existants pour réaliser des analyses puissantes, ainsi que les limites rencontrées et les corrections nécessaires. Il souligne également la fusion prometteuse entre la linguistique et le traitement automatique des langues pour modéliser un phénomène linguistique peu documenté. Le mémoire offre une description globale du projet et une présentation de la suite des opérations en cours. Parmi les nombreuses réalisations, le repérage, l'annotation, l'analyse et l'inventaire des phrases préfabriquées du français ont été initiés avec succès. Le projet PREFAB, entamé en octobre 2022 pour une durée de 48 mois, ouvre de nouvelles perspectives pour la compréhension des phrases préfabriquées et suscite un fort intérêt pour l'avenir de la recherche en linguistique et traitement automatique des langues.",
      "This dissertation presents some of the actions required as part of the PREFAB project, based on the analysis of bilingual corpora. It highlights the workings of various existing tools for performing powerful analyses, as well as the limitations encountered, and corrections needed. It also highlights the promising fusion between linguistics and natural language processing to model a little-documented linguistic phenomenon. The dissertation offers an overall description of the project and a presentation of the work in progress. Among the many achievements, the identification, annotation, analysis, and inventory of French prefabricated sentences have been successfully initiated. The PREFAB project, which began in October 2022 and will run for 48 months, is opening new perspectives for the understanding of prefabricated sentences and is of great interest for future research in linguistics and natural language processing."
    ],
    "authFullName_s": [
      "Florine Hecquet"
    ],
    "halId_s": "dumas-04260553",
    "producedDateY_i": 2023,
    "texte_nettoye": "Ce mémoire présente une partie des actions indispensables dans le cadre du projet PREFAB, basé sur l'analyse de corpus bilingues. Il met en lumière le fonctionnement de différents outils existants pour réaliser des analyses puissantes, ainsi que les limites rencontrées et les corrections nécessaires. Il souligne également la fusion prometteuse entre la linguistique et le traitement automatique des langues pour modéliser un phénomène linguistique peu documenté. Le mémoire offre une description globale du projet et une présentation de la suite des opérations en cours. Parmi les nombreuses réalisations, le repérage, l'annotation, l'analyse et l'inventaire des phrases préfabriquées du français ont été initiés avec succès. Le projet PREFAB, entamé en octobre 2022 pour une durée de 48 mois, ouvre de nouvelles perspectives pour la compréhension des phrases préfabriquées et suscite un fort intérêt pour l'avenir de la recherche en linguistique et traitement automatique des langues."
  },
  {
    "title_s": [
      "Prétraitement de données et création d'un segmenteur de l'arabe pour un système de traduction probabiliste vers le français"
    ],
    "keyword_s": [
      "Statistical Machine Translation",
      "Maxent",
      "Language Models",
      "Preprocessing and Tokenization of data",
      "Translation Models",
      "Maxent",
      "Traitement automatique de la langue arabe",
      "Modèles de traduction",
      "Traduction Automatique Probabiliste",
      "Prétraitement et segmentation de données",
      "Modèle de langage"
    ],
    "abstract_s": [
      "Le domaine du traitement automatique des langues naturelles a connu des évolutions très rapides ces dernières années, et spécialement dans la traduction automatique, c'est pourquoi les demandes en matière de traducteurs automatiques fiables augmentent sans cesse. De ce fait, nous nous sommes intéressés à ce domaine afin de concevoir un traducteur automatique de la langue arabe vers le français, basé sur un modèle probabiliste. Les performances de traduction des systèmes probabilistes dépendent considérablement de la qualité et de la quantité des données d'apprentissage disponibles. Néanmoins la langue arabe compte encore parmi les langues dites \"peu dotées\", c'est pourquoi la plupart des travaux sur cette langue sont basés sur les données libre d'accès qui proviennent d'organisations internationales (ONU, etc.). Nous présentons dans ce travail, une approche d'optimisation des performances d'un système de traduction de l'arabe. Compte tenu du manque de données et d'outils accessibles, nous avons cherché à moindre coût la meilleure combinaison de prétraitements à appliquer sur nos données en arabe pour améliorer la traduction vers le français.",
      "In recent years, Natural Language Processing has rapidly evolved, especially in the domain of Statistical Machine Translation causing the need for reliable automatic translations to skyrocket with no sign of slowing. Due to this increased need, we have taken a special interest in this domain with the goal of creating a translation machine capable of translating Arabic into French, based on statistical models. The performance of Statistical Machine Translation relies heavily on the quality and the quantity of available training data. However, the Arabic language remains one of the languages with the fewest available resources which is why most of the available works in this language are based on open access data from international organizations, such as the U.N. In this work, we will present our approach to optimizing the performance quality of our Arabic translator. Taking into account the lack of data and available resources, we were able to find a low-cost solution to search for the best pre-processing combinations to apply to our Arabic database in order to obtain the highest quality French translation."
    ],
    "authFullName_s": [
      "Sahnoun Ben Taamallah"
    ],
    "halId_s": "dumas-00757706",
    "producedDateY_i": 2012,
    "texte_nettoye": "Le domaine du traitement automatique des langues naturelles a connu des évolutions très rapides ces dernières années, et spécialement dans la traduction automatique, c'est pourquoi les demandes en matière de traducteurs automatiques fiables augmentent sans cesse. De ce fait, nous nous sommes intéressés à ce domaine afin de concevoir un traducteur automatique de la langue arabe vers le français, basé sur un modèle probabiliste. Les performances de traduction des systèmes probabilistes dépendent considérablement de la qualité et de la quantité des données d'apprentissage disponibles. Néanmoins la langue arabe compte encore parmi les langues dites \"peu dotées\", c'est pourquoi la plupart des travaux sur cette langue sont basés sur les données libre d'accès qui proviennent d'organisations internationales (ONU, etc.). Nous présentons dans ce travail, une approche d'optimisation des performances d'un système de traduction de l'arabe. Compte tenu du manque de données et d'outils accessibles, nous avons cherché à moindre coût la meilleure combinaison de prétraitements à appliquer sur nos données en arabe pour améliorer la traduction vers le français."
  },
  {
    "title_s": [
      "Traduction automatique de documents manuscrits et typographiés en arabe par couplage étroit entre systèmes"
    ],
    "keyword_s": [
      "Optical character recognition",
      "Word embedding",
      "Word lattice",
      "Statistical machine translation",
      "Natural language processing",
      "Plongement de mots",
      "Reconnaissance optique de caractères",
      "Graphes de mots treillis de mots",
      "Traduction automatique probabiliste",
      "Traitement automatique des langues naturelles"
    ],
    "abstract_s": [
      "Durant ces dernières années, le domaine du traitement automatique des langues naturelles (TALN) a connu des évolutions rapides, et spécialement la recherche des informations dans les documents numérisés, qui nécessite deux domaines connexes : la reconnaissance optique de caractères et la traduction automatique. Dans ce mémoire de recherche, nous nous sommes intéressés à la traduction automatique des documents numérisés, soit manuscrite ou typographie. En premier lieu, nous avons créé un système de traduction arabe français. En deuxième lieu, nous avons amélioré notre système par la traduction des graphes de mots qui sont construits à partir des sorties OCR bruitées (N-best). Et en dernier lieu, nous avons ajouté un traitement spécifique de mots hors vocabulaire en se basant sur l’approche de plongement de mots (word2vec).",
      "During the last few years, the field of automatic processing of natural languages has seen rapid developments, especially in research on scanned documents, which involves two interrelated fields: Optical character recognition (OCR) and machine translation (MT). In this research paper, we investigate machine translation of scanned documents, whether handwritten or typed. First of all, we created a Arabic-French machine translation system. Next we improved our system by translating a words lattice constructed from noisy OCR outputs (N-best). Finally, we added a specific preprocessing for out-of-vocabulary (OOV) words using word-embeddings (word2vec)."
    ],
    "authFullName_s": [
      "Kamel Bouzidi"
    ],
    "halId_s": "dumas-01494465",
    "producedDateY_i": 2016,
    "texte_nettoye": "Durant ces dernières années, le domaine du traitement automatique des langues naturelles (TALN) a connu des évolutions rapides, et spécialement la recherche des informations dans les documents numérisés, qui nécessite deux domaines connexes : la reconnaissance optique de caractères et la traduction automatique. Dans ce mémoire de recherche, nous nous sommes intéressés à la traduction automatique des documents numérisés, soit manuscrite ou typographie. En premier lieu, nous avons créé un système de traduction arabe français. En deuxième lieu, nous avons amélioré notre système par la traduction des graphes de mots qui sont construits à partir des sorties OCR bruitées (N-best). Et en dernier lieu, nous avons ajouté un traitement spécifique de mots hors vocabulaire en se basant sur l’approche de plongement de mots (word2vec)."
  },
  {
    "title_s": [
      "Constitution d'un corpus de traduction de la parole : augmentation du corpus LibriSpeech"
    ],
    "keyword_s": [
      "Speech translation",
      "Machine translation",
      "Parallel corpus",
      "Alignment",
      "Natural language processing",
      "Traduction automatique de la parole",
      "Traduction automatique neuronale",
      "Corpus parallèle",
      "Alignement",
      "Traitement automatique des langues"
    ],
    "abstract_s": [
      "Il existe des corpus parallèles en grande quantité tels que <i>Europarl</i>, <i>OpenSubtitles, etc</i>. pour les systèmes de traduction automatique. Toutefois, dans le domaine de la traduction automatique de la parole, le nombre de corpus disponibles est très restreint. Dans ce mémoire de recherche, nous nous sommes intéressés à la constitution d’un corpus réel de grande taille (236h) pour les systèmes de traduction automatique de la parole à partir du corpus LibriSpeech. Tout d'abord, nous avons récupéré les livres électroniques français correspondant aux livres audios présents dans LibriSpeech en anglais. Ensuite, après avoir aligné les œuvres en français avec les œuvres en anglais correspondantes, nous avons également aligné les segments de paroles des livres audio avec les livres électroniques en anglais, au niveau de la phrase (<i>utterance</i>). Ceci nous a permis d’obtenir l’alignement des segments de parole avec la traduction du texte original en français. Finalement, nous avons évalué manuellement 200 alignements et avons ajouté des scores de correspondance entre les transcriptions et les traductions pour trier le corpus en fonction de ces scores.",
      "Large quantities of parallel corpora such as <i>Europarl</i>, <i>OpenSubitles</i>, etc. is available for machine translation systems. However, for speech translation systems, number of available corpora is very limited. In this research paper, we investigate building a large scale (236h) non-synthetic corpus for speech translation systems from prepared speech recordings of LibriSpeech project. First, we gathered available French e-books corresponding to the English audio-books from LibriSpeech. Then, after aligning English and French texts at the sentence level, we also aligned speech segments at the sentence level with their respective translations. This allowed us to obtain an alignment at the utterance level aligned with their translations. Lastly, we manually evaluated 200 alignments and added alignment scores between transcriptions and their respective translations to sort the corpus according to those scores."
    ],
    "authFullName_s": [
      "Ali Can Kocabiyikoğlu"
    ],
    "halId_s": "dumas-01712400",
    "producedDateY_i": 2017,
    "texte_nettoye": "Il existe des corpus parallèles en grande quantité tels que <i>Europarl</i>, <i>OpenSubtitles, etc</i>. pour les systèmes de traduction automatique. Toutefois, dans le domaine de la traduction automatique de la parole, le nombre de corpus disponibles est très restreint. Dans ce mémoire de recherche, nous nous sommes intéressés à la constitution d’un corpus réel de grande taille (236h) pour les systèmes de traduction automatique de la parole à partir du corpus LibriSpeech. Tout d'abord, nous avons récupéré les livres électroniques français correspondant aux livres audios présents dans LibriSpeech en anglais. Ensuite, après avoir aligné les œuvres en français avec les œuvres en anglais correspondantes, nous avons également aligné les segments de paroles des livres audio avec les livres électroniques en anglais, au niveau de la phrase (<i>utterance</i>). Ceci nous a permis d’obtenir l’alignement des segments de parole avec la traduction du texte original en français. Finalement, nous avons évalué manuellement 200 alignements et avons ajouté des scores de correspondance entre les transcriptions et les traductions pour trier le corpus en fonction de ces scores."
  },
  {
    "title_s": [
      "Mise en place d’un système d’acquisition semi-automatique d’un corpus de données hétérogènes (images et textes) : application à la problématique de la sécurité alimentaire en Afrique de l'Ouest"
    ],
    "keyword_s": [
      "Food safety",
      "Corpus",
      "Machine learning",
      "Natural language processing",
      "Text mining",
      "Sécurité alimentaire",
      "Fouille de texte",
      "Corpus",
      "Apprentissage automatique",
      "Traitement automatique du langage"
    ],
    "abstract_s": [
      "Des systèmes d’alerte précoce sur la gestion des risques liés à la sécurité alimentaire sont mis en place pour informer les acteurs, afin d’adapter et améliorer le développement de l’agriculture. Ces systèmes analysent principalement des images satellitaires (par exemple des images de champs agricoles) et des données quantitatives (par exemple le prix du marché alimentaire). Cependant, peu de données textuelles sont intégrées dans leur traitement, or elles sont de plus en plus abondantes et accessibles sur Internet. Elles ne décrivent pas parfaitement la situation géographique, mais elles pourraient apporter une information ou une observation complémentaire aux images satellitaires. Ce mémoire propose donc d’acquérir un corpus d’articles de sites d’actualités en lien avec le thème de la sécurité alimentaire au Burkina Faso, d’extraire les informations spatio-temporelles présentes dans les articles, puis de les mettre en relation avec les données présentes dans les systèmes d’alerte précoce de sécurité alimentaire.",
      "Food security early warning systems are set up to inform stakeholders in order to adapt and improve agricultural development. These systems mainly analyze satellite images (e. g. images of agricultural fields) and quantitative data (e. g. food market prices). However, few textual data are integrated into their processing, yet they are increasingly abundant and accessible on the Internet. They do not perfectly describe the geographical situation, but they could provide additional information or observations to satellite images. This thesis therefore proposes to acquire a corpus of articles from newspapers sites related to the theme of food security in Burkina Faso, to extract the spatio-temporal information contained in the corpus, and then to link them to the data contained in food security early warning systems."
    ],
    "authFullName_s": [
      "Camille Schaeffer"
    ],
    "halId_s": "dumas-02302235",
    "producedDateY_i": 2019,
    "texte_nettoye": "Des systèmes d’alerte précoce sur la gestion des risques liés à la sécurité alimentaire sont mis en place pour informer les acteurs, afin d’adapter et améliorer le développement de l’agriculture. Ces systèmes analysent principalement des images satellitaires (par exemple des images de champs agricoles) et des données quantitatives (par exemple le prix du marché alimentaire). Cependant, peu de données textuelles sont intégrées dans leur traitement, or elles sont de plus en plus abondantes et accessibles sur Internet. Elles ne décrivent pas parfaitement la situation géographique, mais elles pourraient apporter une information ou une observation complémentaire aux images satellitaires. Ce mémoire propose donc d’acquérir un corpus d’articles de sites d’actualités en lien avec le thème de la sécurité alimentaire au Burkina Faso, d’extraire les informations spatio-temporelles présentes dans les articles, puis de les mettre en relation avec les données présentes dans les systèmes d’alerte précoce de sécurité alimentaire."
  },
  {
    "title_s": [
      "Intégration des expressions polylexicales dans un système de traduction statistique"
    ],
    "keyword_s": [
      "Multiword expression",
      "Translation models",
      "Statistical machine translation",
      "Specific corpus",
      "Natural language processing",
      "Traitement automatique des langues naturelles",
      "Expressions polylexicales",
      "Corpus spécifiques",
      "Traduction automatique statistique",
      "Modèles de traduction"
    ],
    "abstract_s": [
      "Les expressions polylexicales (EPL) constituent un champ de recherche intéressant dans le domaine de la linguistique computationnelle. Elles peuvent être considérées comme un vrai défi pour les systèmes de traduction automatique statistique (TAS). Dans ce mémoire, nous avons enrichi une ressource textuelle littéraire existante en vue d'extraire des corpus spécifiques pour évaluer le problème des expressions polylexicales en traduction automatique statistique. Par ailleurs, dans le but d'améliorer les performances d'un système de traduction automatique, nous avons testé une stratégie d'intégration des EPL en anglais. Les résultats obtenus au niveau du score BLEU sont encourageants.",
      "Multiword Expressions constitute an interesting field of research in Computational Linguistics. They can be a real challenge for Statistical Machine Translation systems (SMT). In this paper, we have improved an existing literary lexical resource in order to extract a corpus in order to assess the problem of Multiword Expressions in SMT. Moreover, in order to improve the performance of an automatic translation system, we integrated English EPL. As regards the BLEU score, the results were encouraging."
    ],
    "authFullName_s": [
      "Zied Elloumi"
    ],
    "halId_s": "dumas-01063275",
    "producedDateY_i": 2014,
    "texte_nettoye": "Les expressions polylexicales (EPL) constituent un champ de recherche intéressant dans le domaine de la linguistique computationnelle. Elles peuvent être considérées comme un vrai défi pour les systèmes de traduction automatique statistique (TAS). Dans ce mémoire, nous avons enrichi une ressource textuelle littéraire existante en vue d'extraire des corpus spécifiques pour évaluer le problème des expressions polylexicales en traduction automatique statistique. Par ailleurs, dans le but d'améliorer les performances d'un système de traduction automatique, nous avons testé une stratégie d'intégration des EPL en anglais. Les résultats obtenus au niveau du score BLEU sont encourageants."
  },
  {
    "title_s": [
      "Fact-checking dans les médias"
    ],
    "keyword_s": [
      "Artificial intelligence",
      "Fact-checking",
      "Data set",
      "Natural language processing",
      "Jeu de données",
      "Traitement automatique des langues",
      "Vérification des faits",
      "Intelligence artificielle"
    ],
    "abstract_s": [
      "Durant cinq mois j’ai travaillé sur une thématique portant sur l’intelligence artificielle qui consistait à vérifier des faits dans des articles de presse majoritairement. C’est en intégrant l’entreprise Buster.AI, dont le but premier étant de réaliser mon stage de fin d’études, que j’ai pu traiter ce sujet. Tout au long de mon stage, j’ai travaillé avec des bases de données importantes et ai construit des jeux de données qui ont permis de tester, d’entrainer et d’améliorer les modèles de l’entreprise. Ce travail a développé de nouvelles performances au produit de Buster grâce aux résultats obtenus. Ce stage a également approfondi mes compétences et m’en a appris des nouvelles dans le domaine du traitement automatique des langues. J’ai aussi, grâce à cette expérience professionnelle, pu découvrir le monde professionnel et l’organisation d’une entreprise.",
      "During five months I worked on a topic related to artificial intelligence which was about the fact-checking in newspaper mainly. It is by joining the company Buster.AI, whose first goal was to carry out my end of studies internship, that I was able to study this subject. Throughout my internship, I worked with large databases and built many datasets that allowed me to test, train and improve the company's models. This work developed new performances to the Buster product thanks to the new results obtained. This internship has also improved my skills and taught me new ones in the domain of natural language processing. Thanks to this work experience, I was also able to discover the professional world and the organization of a company."
    ],
    "authFullName_s": [
      "Sanda Hachana"
    ],
    "halId_s": "dumas-03643250",
    "producedDateY_i": 2021,
    "texte_nettoye": "Durant cinq mois j’ai travaillé sur une thématique portant sur l’intelligence artificielle qui consistait à vérifier des faits dans des articles de presse majoritairement. C’est en intégrant l’entreprise Buster.AI, dont le but premier étant de réaliser mon stage de fin d’études, que j’ai pu traiter ce sujet. Tout au long de mon stage, j’ai travaillé avec des bases de données importantes et ai construit des jeux de données qui ont permis de tester, d’entrainer et d’améliorer les modèles de l’entreprise. Ce travail a développé de nouvelles performances au produit de Buster grâce aux résultats obtenus. Ce stage a également approfondi mes compétences et m’en a appris des nouvelles dans le domaine du traitement automatique des langues. J’ai aussi, grâce à cette expérience professionnelle, pu découvrir le monde professionnel et l’organisation d’une entreprise."
  },
  {
    "title_s": [
      "Micro-expressions audio-visuelles dans la communication expressive : enjeux pluri-culturels"
    ],
    "keyword_s": [
      "Speech micro-expressions",
      "Social affective attitudes",
      "NLP",
      "Natural Language Processing",
      "Applications strategies",
      "Enjeux applicatifs",
      "Traitement Automatique de la Langue",
      "TAL",
      "Attitudes socio-affectifs",
      "Perception",
      "Synergologie",
      "Micro-expressions de la parole"
    ],
    "abstract_s": [
      "L'évolution des habitudes sociales crée chaque jour de nouveaux besoins dans le domaine du Traitement Automatique de la Langue. Plus particulièrement dans le cas des technologies de la parole, l'un des enjeux majeurs est l'analyse des bruits non lexicalisés, accompagnés de gestes spécifiques, prémisses du langage humain. Ces objets sont désignés sous le terme de micro-expressions de la parole dans le cadre de cette étude. En reconnaissance, ces objets permettent de comprendre les attitudes socio-affectives de l'humain qui sont au cœur des nouveaux enjeux applicatifs actuels. Ainsi, si nous avons dors et déjà des pistes d'investigation sur les aspects acoustiques de ces micro-expressions, l'analyse visuelle en est une toute autre affaire. Il est alors intéressant de voir comment les nouveaux courants à la mode tels que la synergologie peut servir à une recherche scientifique. L'étude en cours vise à comprendre tous ces mécanismes et ces enjeux à travers une expérience de perception des micro-expressions de la parole en contraste français-japonais.",
      "The changing of social habits creates every day new needs in the area of Natural Language Processing. Especially in the case of speech technologies, one of the major challenges is the analysis of non-lexicalized sounds, accompanied by specific gestures, premises of human language. These objects are referred to as speech micro-expressions in this study. In recognition, these objects provide an understanding of the socio-emotional attitudes of humans which are the heart of the new challenges of modern applications. If we already have some ideas to investigate acoustic aspects of these micro-expressions, visual analysis is another matter. It is interesting to see how new movements in fashion as synergologie can be used in a scientific research. The current study aims to understand these mechanisms and issues through an experiment of speech micro-expressions perception in a French-Japanese contrast."
    ],
    "authFullName_s": [
      "Yuko Sasa"
    ],
    "halId_s": "dumas-00709396",
    "producedDateY_i": 2012,
    "texte_nettoye": "L'évolution des habitudes sociales crée chaque jour de nouveaux besoins dans le domaine du Traitement Automatique de la Langue. Plus particulièrement dans le cas des technologies de la parole, l'un des enjeux majeurs est l'analyse des bruits non lexicalisés, accompagnés de gestes spécifiques, prémisses du langage humain. Ces objets sont désignés sous le terme de micro-expressions de la parole dans le cadre de cette étude. En reconnaissance, ces objets permettent de comprendre les attitudes socio-affectives de l'humain qui sont au cœur des nouveaux enjeux applicatifs actuels. Ainsi, si nous avons dors et déjà des pistes d'investigation sur les aspects acoustiques de ces micro-expressions, l'analyse visuelle en est une toute autre affaire. Il est alors intéressant de voir comment les nouveaux courants à la mode tels que la synergologie peut servir à une recherche scientifique. L'étude en cours vise à comprendre tous ces mécanismes et ces enjeux à travers une expérience de perception des micro-expressions de la parole en contraste français-japonais."
  },
  {
    "title_s": [
      "Natural Language Processing for virtual assistants: what contribution synthetic data could bring to intents classification?",
      "Traitement automatique du langage pour les assistants virtuels : quel apport des jeux de données synthétiques pour la détection d'intentions ?"
    ],
    "keyword_s": [
      "Generation",
      "Human factor",
      "Virtual agent",
      "Natural language understanding",
      "Natural language processing",
      "Aeronautic",
      "Airbus",
      "Facteur humain",
      "Génération automatique",
      "Aéronautique",
      "Traitement automatique du langage naturel",
      "Compréhension du langage naturel",
      "Agent Virtuel"
    ],
    "abstract_s": [
      "This document summarizes a 6 months internship that took place within the Human Factor department of the Airbus Company in Toulouse, France. This internship was centred on a virtual agent research thematic. Our initial hypothesis was that a system of automatic intent categorization could benefit from using synthetic “natural-like” data. In order to validate or invalidate this hypothesis we decided, first, to create a methodology that would help us collect natural questions from end-users. Then we used the “natural” data we previously collected along with a synthetic question generator we designed in order to output synthetic questions that feels “natural-like” if compared to the input ones. Lastly, we experimented on the synthetic datasets using various tools in order to put to the test our initial hypothesis. The results we obtained from these tests allowed us to open new perspectives on the natural language understanding part of the virtual agent system.",
      "Ce document synthétise un stage de 6 mois passé dans le service Facteur Humain du constructeur aéronautique Airbus à Toulouse sur une problématique de développement d'assistant virtuel. L'objet de ce stage était de vérifier l'hypothèse selon laquelle un système de catégorisation de requêtes d'utilisateurs pouvait bénéficier de la création de jeux de données synthétiques. Pour vérifier cette hypothèse nous avons dans un premier temps créé une méthodologie de récolte de questions dites \"naturelles\" auprès des utilisateurs finaux de l'agent virtuel. Ensuite nous avons développé un programme permettant de générer des questions synthétiques en se basant sur le jeu de données naturelles préalablement collectées. Pour finir nous avons expérimenté à l'aide de plusieurs outils sur les jeux de données générés. Les résultats obtenus ont permis d'ouvrir de nouvelles pistes de recherches et d'amélioration sur le sujet de la compréhension du langage naturel par le système."
    ],
    "authFullName_s": [
      "Sylvain Daronnat"
    ],
    "halId_s": "dumas-01695385",
    "producedDateY_i": 2017,
    "texte_nettoye": "This document summarizes a 6 months internship that took place within the Human Factor department of the Airbus Company in Toulouse, France. This internship was centred on a virtual agent research thematic. Our initial hypothesis was that a system of automatic intent categorization could benefit from using synthetic “natural-like” data. In order to validate or invalidate this hypothesis we decided, first, to create a methodology that would help us collect natural questions from end-users. Then we used the “natural” data we previously collected along with a synthetic question generator we designed in order to output synthetic questions that feels “natural-like” if compared to the input ones. Lastly, we experimented on the synthetic datasets using various tools in order to put to the test our initial hypothesis. The results we obtained from these tests allowed us to open new perspectives on the natural language understanding part of the virtual agent system."
  },
  {
    "title_s": [
      "Conception et réalisation d'une chaîne de traitement automatique des langues adaptée à des projets littéraires"
    ],
    "keyword_s": [
      "NLP",
      "XML",
      "Part-of-speech tagging",
      "Textometrics",
      "XML",
      "Textométrie",
      "Étiquetage morphosyntaxique",
      "TAL"
    ],
    "abstract_s": [
      "J’ai développé un outil en Python afin de faciliter l’utilisation de plusieurs outils de traitement morphosyntaxique et textométrique sur plusieurs types de fichiers, y compris les fichiers XML. Cet outil fonctionne à l’aide d’une interface Web développée en PHP dont le but est de simplifier autant que possible l’utilisation de l’outil par un utilisateur non formé à l’utilisation d’outils informatiques complexes.",
      "Using Python, I developped a tool whose goal was to simplify the use of part-of-speech tagging and textometrical tools on multiple types of files, including XML files. This tool uses a Web interface written in PHP in order to ensure that even an user who is untrained in the use of complex technological tools can make use of it."
    ],
    "authFullName_s": [
      "Julien Fagot"
    ],
    "halId_s": "dumas-02987314",
    "producedDateY_i": 2020,
    "texte_nettoye": "J’ai développé un outil en Python afin de faciliter l’utilisation de plusieurs outils de traitement morphosyntaxique et textométrique sur plusieurs types de fichiers, y compris les fichiers XML. Cet outil fonctionne à l’aide d’une interface Web développée en PHP dont le but est de simplifier autant que possible l’utilisation de l’outil par un utilisateur non formé à l’utilisation d’outils informatiques complexes."
  },
  {
    "title_s": [
      "Répartition hommes/femmes dans les systèmes d’IA : une étude pilote sur les grands corpus pour la transcription automatique de la parole"
    ],
    "keyword_s": [
      "Machine learning",
      "Automatic speech processing",
      "Corpus",
      "Gender",
      "Apprentissage automatique",
      "Traitement automatique de la parole",
      "Genre"
    ],
    "abstract_s": [
      "Les systèmes d’IA sont développés sur des grands corpus de données et les technologies du traitement automatique de la parole n’échappent pas à cette règle. Mais ces grands corpus de données peuvent contenir des répartitions de genre non-équilibrées qui peuvent conduire au développement d’algorithmes discriminants. Les systèmes d’IA infiltrant de plus en plus notre quotidien, et la voix s’imposant comme la nouvelle interface homme/machine, il devient nécessaire de pouvoir étudier et quantifier l’impact de la répartition homme/femme dans les données d’apprentissage sur les performances des systèmes. Ce mémoire propose donc dans un premier temps d’étudier la répartition des genres dans les grands corpus du français oral, et dans un second temps, d’évaluer l’impact de cette représentation sur les performances d’un système de reconnaissance automatique de la parole.",
      "AI systems are trained on a huge amount of data, and speech processing technologies are no exception to the rule. However corpora may be statistically imbalanced regarding genders and this can lead to discriminative algorithms. With AI becoming ever more present in our everyday life, it seems more than necessary to be aware of the impact of gender representation in training data on the system’s performances. This masters’ thesis proposes to study gender representation in large spoken french corpora and to estimate the impact of this distribution on the performances of an automatic speech recognition system."
    ],
    "authFullName_s": [
      "Mahault Garnerin"
    ],
    "halId_s": "dumas-01835333",
    "producedDateY_i": 2018,
    "texte_nettoye": "Les systèmes d’IA sont développés sur des grands corpus de données et les technologies du traitement automatique de la parole n’échappent pas à cette règle. Mais ces grands corpus de données peuvent contenir des répartitions de genre non-équilibrées qui peuvent conduire au développement d’algorithmes discriminants. Les systèmes d’IA infiltrant de plus en plus notre quotidien, et la voix s’imposant comme la nouvelle interface homme/machine, il devient nécessaire de pouvoir étudier et quantifier l’impact de la répartition homme/femme dans les données d’apprentissage sur les performances des systèmes. Ce mémoire propose donc dans un premier temps d’étudier la répartition des genres dans les grands corpus du français oral, et dans un second temps, d’évaluer l’impact de cette représentation sur les performances d’un système de reconnaissance automatique de la parole."
  },
  {
    "title_s": [
      "SMS et TAL : kL 1Trè* ? (*SMS et TAL : Quel intérêt ?)"
    ],
    "keyword_s": [
      "Traitement automatique",
      "Transcription",
      "Anonymisation",
      "Corpus",
      "Langage",
      "SMS"
    ],
    "abstract_s": [
      "Ce mémoire présente le travail réalisé en préparation d'une collecte de SMS en France métropolitaine : alpes4science. Ce projet vise à constituer un corpus afin de proposer des données nombreuses et diverses comme outils d'études aux chercheurs travaillant sur le SMS, les pratiques qui lui sont associées et le langage SMS qui peut découler de ce mode de communication. Le SMS peut apparaître comme un mode de communication écrit mais aussi oral, par son immédiateté et le langage utilisé. Dans un premier temps, nous proposons un état de l'art concernant le SMS et ses pratiques. Puis nous présentons les applications élaborées à partir de et pour les SMS, ainsi que les corpus réalisés jusqu'alors. Dans une dernière partie, nous expliquons ce qui a été mis en place pour constituer et exploiter le corpus alpes4science."
    ],
    "authFullName_s": [
      "Gaëlle Chabert"
    ],
    "halId_s": "dumas-00561995",
    "producedDateY_i": 2010,
    "texte_nettoye": "Ce mémoire présente le travail réalisé en préparation d'une collecte de SMS en France métropolitaine : alpes4science. Ce projet vise à constituer un corpus afin de proposer des données nombreuses et diverses comme outils d'études aux chercheurs travaillant sur le SMS, les pratiques qui lui sont associées et le langage SMS qui peut découler de ce mode de communication. Le SMS peut apparaître comme un mode de communication écrit mais aussi oral, par son immédiateté et le langage utilisé. Dans un premier temps, nous proposons un état de l'art concernant le SMS et ses pratiques. Puis nous présentons les applications élaborées à partir de et pour les SMS, ainsi que les corpus réalisés jusqu'alors. Dans une dernière partie, nous expliquons ce qui a été mis en place pour constituer et exploiter le corpus alpes4science."
  },
  {
    "title_s": [
      "Analyse des tendances des thématiques de recherche en réanimation : une approche par apprentissage automatique"
    ],
    "keyword_s": [
      "Bibliométrie",
      "Traitement automatique du langage naturel",
      "Apprentissage automatique",
      "Traitement automatique du langage",
      "Réanimation"
    ],
    "abstract_s": [
      "La réanimation est une spécialité médicale récente formalisée dans les années 1950. Elle a la particularité d’être une spécialité multidisciplinaire et sa littérature scientifique reflète cette multidisciplinarité. Cependant, la représentation de chaque spécialité médicale dans cette littérature et leur évolution au cours du temps ne sont pas connues. L’objectif de cette thèse est d’analyser la littérature de réanimation, d’en extraire les thématiques de recherche et d’observer les tendances en utilisant des algorithmes d’apprentissage automatique. Matériel et Méthodes : Les titres et abstracts des articles originaux des principales revues de réanimation depuis leur création jusqu’au 31 décembre 2019 ont été inclus. Après prétraitement, ce corpus a alimenté un algorithme de structural topic modeling pour extraire les thèmes sémantiques sous-jacents. Les thèmes ont été identifiés par un groupe d’experts composé de médecins réanimateurs de différentes formations. L’évolution temporelle a ensuite été analysée et la présence de tendances a été recherchée par des tests de Mann-Kendall. Résultats : 49276 articles issus de 10 revues ont été inclus. Après extraction et identification des thèmes, 124 sujets de recherche ont été sélectionnés. Les thèmes ont été classés en 19 catégories, les plus représentées étant les catégories respiratoire, expérimentale, neurologie et infectiologie. Des dynamiques de tendance à la hausse ont été observées pour la recherche sur le thème respiratoire et des tendances à la baisse pour la réanimation cardio-pulmonaire. Conclusion : Cette étude a passé en revue de manière exhaustive tous les articles des principales revues de réanimation. Elle permet de mieux comprendre le paysage de la recherche en réanimation en analysant l’évolution temporelle des thèmes de recherche dans la littérature."
    ],
    "authFullName_s": [
      "Benjamin Popoff"
    ],
    "halId_s": "dumas-03372812",
    "producedDateY_i": 2021,
    "texte_nettoye": "La réanimation est une spécialité médicale récente formalisée dans les années 1950. Elle a la particularité d’être une spécialité multidisciplinaire et sa littérature scientifique reflète cette multidisciplinarité. Cependant, la représentation de chaque spécialité médicale dans cette littérature et leur évolution au cours du temps ne sont pas connues. L’objectif de cette thèse est d’analyser la littérature de réanimation, d’en extraire les thématiques de recherche et d’observer les tendances en utilisant des algorithmes d’apprentissage automatique. Matériel et Méthodes : Les titres et abstracts des articles originaux des principales revues de réanimation depuis leur création jusqu’au 31 décembre 2019 ont été inclus. Après prétraitement, ce corpus a alimenté un algorithme de structural topic modeling pour extraire les thèmes sémantiques sous-jacents. Les thèmes ont été identifiés par un groupe d’experts composé de médecins réanimateurs de différentes formations. L’évolution temporelle a ensuite été analysée et la présence de tendances a été recherchée par des tests de Mann-Kendall. Résultats : 49276 articles issus de 10 revues ont été inclus. Après extraction et identification des thèmes, 124 sujets de recherche ont été sélectionnés. Les thèmes ont été classés en 19 catégories, les plus représentées étant les catégories respiratoire, expérimentale, neurologie et infectiologie. Des dynamiques de tendance à la hausse ont été observées pour la recherche sur le thème respiratoire et des tendances à la baisse pour la réanimation cardio-pulmonaire. Conclusion : Cette étude a passé en revue de manière exhaustive tous les articles des principales revues de réanimation. Elle permet de mieux comprendre le paysage de la recherche en réanimation en analysant l’évolution temporelle des thèmes de recherche dans la littérature."
  },
  {
    "title_s": [
      "Du présentiel au distanciel : le TAL pour comparer image voulue et image perçue"
    ],
    "keyword_s": [
      "Digital transition",
      "Digital interfaces",
      "Semantic resources",
      "Opinion mining",
      "NLP",
      "Transition numérique",
      "Interfaces numériques",
      "TAL",
      "Ressources sémantiques"
    ],
    "abstract_s": [
      "Le travail que nous présentons se situe dans le cadre d’un projet de recherche en collaboration entre l’Université Grenoble Alpes et l’entreprise COMONGO dont le cœur de métier est l’accompagnement et la gestion d’image des personnes morales et physiques. Notre démarche a consisté dans un premier temps à transposer une pratique en focus group vers une pratique distancielle numérique. Dans un second temps, il s’est agi d’intégrer les outils du Traitement Automatique des Langues, notamment les ressources sémantiques, à cette démarche professionnelle d’entreprise. Cette transformation d’une pratique métier nous a menée à poser trois grandes hypothèses : la transition numérique a un impact sur la qualité des données ; les ressources sémantiques permettent une meilleure appréhension des données textuelles traitées mais s’avèrent insuffisantes après simulation ; une démarche incrémentale d’amélioration des ressources doit être envisagée afin d’obtenir des traitements optimaux. Cette première expérimentation sur données réelles permet de poser les bases d’un projet de recherche et développement à plus long terme au sein de la société COMONGO alliant les domaines de la linguistique de corpus, du Traitement Automatique des Langues et des sciences de l’information et de la communication.",
      "The work that we present is part of a collaborative research project between the University Grenoble Alpes and the company COMONGO which provides image management of legal and physical persons. In a first step, we have transposed a focus group based approach into a distant digital practice. In a second step, we have investigated the integration of NLP tools, in particular semantic resources, into this professional approach. This digital transformation of a practice led us to make three main assumptions : the digital transition has an impact on the quality of the data ; the semantic resources allow a better understanding of the textual data processed but are insufficient after simulation ; an incremental approach to resources improvement should be considered in order to obtain optimal results. This first experimentation with real data allows us to lay the foundations for a longer-term research and development project within the company COMONGO combining the fields of corpus linguistics, Natural Language Processing and Information and Communication Sciences."
    ],
    "authFullName_s": [
      "Pauline Soutrenon"
    ],
    "halId_s": "dumas-01767534",
    "producedDateY_i": 2017,
    "texte_nettoye": "Le travail que nous présentons se situe dans le cadre d’un projet de recherche en collaboration entre l’Université Grenoble Alpes et l’entreprise COMONGO dont le cœur de métier est l’accompagnement et la gestion d’image des personnes morales et physiques. Notre démarche a consisté dans un premier temps à transposer une pratique en focus group vers une pratique distancielle numérique. Dans un second temps, il s’est agi d’intégrer les outils du Traitement Automatique des Langues, notamment les ressources sémantiques, à cette démarche professionnelle d’entreprise. Cette transformation d’une pratique métier nous a menée à poser trois grandes hypothèses : la transition numérique a un impact sur la qualité des données ; les ressources sémantiques permettent une meilleure appréhension des données textuelles traitées mais s’avèrent insuffisantes après simulation ; une démarche incrémentale d’amélioration des ressources doit être envisagée afin d’obtenir des traitements optimaux. Cette première expérimentation sur données réelles permet de poser les bases d’un projet de recherche et développement à plus long terme au sein de la société COMONGO alliant les domaines de la linguistique de corpus, du Traitement Automatique des Langues et des sciences de l’information et de la communication."
  },
  {
    "title_s": [
      "Amélioration des systèmes de reconnaissance de la parole des personnes âgées"
    ],
    "keyword_s": [
      "Ambiant Assisted Living",
      "Automatic Speech Recognition",
      "Elderly",
      "MLLR adaptation",
      "Acoustic model",
      "Language model",
      "Reconnaissance Automatique de la Parole",
      "Personnes âgées",
      "Assistance à la vie autonome",
      "Adaptation MLLR",
      "Modèle acoustique",
      "Modèle de langage"
    ],
    "abstract_s": [
      "La Reconnaissance Automatique de la Parole est une technologie en plein essor dont l'utilisation pour l'aide aux personnes fragiles apparaît comme un domaine novateur et porteur d'espoirs. Ce mémoire a été réalisé dans ce contexte avec pour objectif d'évaluer l'état du système de reconnaissance de la parole destiné à des personnes âgées dans le cadre du projet CIRDO. Nous avons étudié, à partir des travaux précédents sur le sujet, les différences entre la parole dite âgée et non âgée ; les comportements du système, après adaptations, de manière approfondie sur le plan des phonèmes et classes de phonèmes ainsi que le décodage en mots pour de la parole âgée avec pour métrique le WER. Ces recherches nous ont permis de mieux comprendre le fonctionnement du système Sphinx3 et de trouver un paramétrage qui permette d'obtenir des résultats intéressants sur un corpus de voix âgées de parole lue et de parole spontanée, thème sur lequel seuls de très rares travaux existent.",
      "Recognition Automatic Speech is a burgeoning technology whose use for assistance to frail appears as an innovative area and hope. This dissertation was made in this context with the aim of assessing the state of the system for speech recognition to elderly in the CIRDO project. We studied from previous works on the subject, the differences between the non-elderly speaking and elderly speaking ; behavior of the system, after adjustments, thoroughly in terms of phonemes and classes of phonemes ; words decoding of speech for elderly with the WER metric. This research helped us better understand how the system works and finding a Sphinx3 parameterization which allows to obtain interesting results on a corpus of read speech voice aged and spontaneous speech, topic on which only very few studies exist."
    ],
    "authFullName_s": [
      "Juline Le Grand"
    ],
    "halId_s": "dumas-00736504",
    "producedDateY_i": 2012,
    "texte_nettoye": "La Reconnaissance Automatique de la Parole est une technologie en plein essor dont l'utilisation pour l'aide aux personnes fragiles apparaît comme un domaine novateur et porteur d'espoirs. Ce mémoire a été réalisé dans ce contexte avec pour objectif d'évaluer l'état du système de reconnaissance de la parole destiné à des personnes âgées dans le cadre du projet CIRDO. Nous avons étudié, à partir des travaux précédents sur le sujet, les différences entre la parole dite âgée et non âgée ; les comportements du système, après adaptations, de manière approfondie sur le plan des phonèmes et classes de phonèmes ainsi que le décodage en mots pour de la parole âgée avec pour métrique le WER. Ces recherches nous ont permis de mieux comprendre le fonctionnement du système Sphinx3 et de trouver un paramétrage qui permette d'obtenir des résultats intéressants sur un corpus de voix âgées de parole lue et de parole spontanée, thème sur lequel seuls de très rares travaux existent."
  },
  {
    "title_s": [
      "Recherche automatique d'antériorités de brevets par la recherche de revendications et de segments proches potentiellement invalidants"
    ],
    "keyword_s": [
      "Repeated segments",
      "Patents",
      "Claims",
      "Patent analysis",
      "Natural language processing",
      "Lexical",
      "Syntax",
      "Internal structure of document",
      "Similar document",
      "Similarity",
      "Analyse de brevets",
      "Revendications",
      "Brevets",
      "Structure interne de documents",
      "Traitement automatique des langues",
      "Lexique",
      "Syntaxe",
      "Segments répétés",
      "Similarité de chaines",
      "Similarité",
      "Documents proches"
    ],
    "abstract_s": [
      "Le stage de fin d'études effectué à Tecknowmetrix, société de conseils et services en innovation proposant des outils ou des études de veille technologique, a pour but de mettre en place un moyen permettant de valider ou d'invalider des revendications d'un brevet, et ce de façon automatique ou semi-automatique. L'invalidation est ici restreinte au critère de nouveauté des brevets, c'est-à-dire à la recherche de documents proches pouvant invalider le caractère nouveau. Notre but est donc de créer un outil et une méthode permettant de retrouver au sein de grandes quantités de brevets quels sont les segments qui, étant très proches, peuvent s'invalider. La méthodologie utilisée s'éloigne des chemins classiques qui exploitent les statistiques et se veut plus orientée vers la linguistique en faisant intervenir la structure syntaxique interne des brevets, couplées à des méthodes issues du traitement automatique des langues. Nous définissons dans un premier temps ce qu'est un brevet et analysons les revendications du point de vue lexical et syntaxique afin de trouver des régularités que nous essayons de modaliser informatiquement pour mettre en place un outil de recherche de revendications proches sur un corpus de brevets de langue anglaise.",
      "Our internship took place in Tecknowmetrix; a business specialized in innovation, intellectual property and strategic intelligence. Our aim here was to create a tool which allows validating or invalidating some patent claims, in an automatic or semi-automatic way. Our goal was to invent a priority research tool which finds all the relevant and similar patents in order to find segments which are invalidating \"novel criteria\". Our methodology is different from classical ways which are exploiting statistical methods and is more oriented towards linguistic in basing itself on the internal structure of patents. Our work involved in natural language processing field. To do so, we defined what a patent is and we studied claims by lexical and syntax views in order to discover some regularities. These regularities will help us to figure out how producing a data model which can be applied on our tool that we want to develop. Our tool should be able to extract similar segments of similar claims on a corpus of patents written in the English language."
    ],
    "authFullName_s": [
      "Edmée Marazel"
    ],
    "halId_s": "dumas-00631733",
    "producedDateY_i": 2011,
    "texte_nettoye": "Le stage de fin d'études effectué à Tecknowmetrix, société de conseils et services en innovation proposant des outils ou des études de veille technologique, a pour but de mettre en place un moyen permettant de valider ou d'invalider des revendications d'un brevet, et ce de façon automatique ou semi-automatique. L'invalidation est ici restreinte au critère de nouveauté des brevets, c'est-à-dire à la recherche de documents proches pouvant invalider le caractère nouveau. Notre but est donc de créer un outil et une méthode permettant de retrouver au sein de grandes quantités de brevets quels sont les segments qui, étant très proches, peuvent s'invalider. La méthodologie utilisée s'éloigne des chemins classiques qui exploitent les statistiques et se veut plus orientée vers la linguistique en faisant intervenir la structure syntaxique interne des brevets, couplées à des méthodes issues du traitement automatique des langues. Nous définissons dans un premier temps ce qu'est un brevet et analysons les revendications du point de vue lexical et syntaxique afin de trouver des régularités que nous essayons de modaliser informatiquement pour mettre en place un outil de recherche de revendications proches sur un corpus de brevets de langue anglaise."
  },
  {
    "title_s": [
      "Normalisation des messages issus de la communication électronique médiée"
    ],
    "keyword_s": [
      "Annotated corpus",
      "SMS",
      "Tweets",
      "Automatic normalization",
      "NLP",
      "Traitement automatique de la langue TAL",
      "Normalisation automatique",
      "Corpus annoté"
    ],
    "abstract_s": [
      "Le travail dont ce mémoire rend compte consistait à élaborer un outil de normalisation automatique des textes non standard en français, en particulier les tweets et les SMS. Pour cela, nous avons d’abord annoté un corpus de 1000 tweets et 1000 SMS, en fonction de phénomènes morpho-lexicaux et morpho-syntaxiques, que nous avions au préalable identifiés lors de l’élaboration d’une typologie pour l’annotation de textes non standard. À partir de l’observation de ce corpus, nous avons développé un outil de normalisation automatique qui génère pour chaque token non standard un ensemble de candidats en fonction des phénomènes observés le plus fréquemment dans notre corpus de tweets et de SMS. Ensuite, la normalisation du token non standard est sélectionnée parmi l’ensemble de ces candidats, à l’aide d’un système d’attribution de scores prenant également en compte le contexte immédiat du token traité.",
      "The work reported in this paper consisted in the creation of an automatic normalization tool for non-standard texts written in French, in particular tweets and SMS. In order to do so, we first annotated a corpus of 1000 tweets and 1000 SMS, according to morpho-lexical and morpho-syntactic phenomena, which we had previously identified when elaborating a typology for the annotation of non-standard texts. From the observation of this corpus, we have developed an automatic normalization tool that generates for each non-standard token a set of candidates according to the phenomena observed most frequently in our corpus of tweets and SMS. Then, the normalization of the non-standard token is selected from all of these candidates, using a scoring system that also takes into account the immediate context of the processed token."
    ],
    "authFullName_s": [
      "Louise Tarrade"
    ],
    "halId_s": "dumas-01666146",
    "producedDateY_i": 2017,
    "texte_nettoye": "Le travail dont ce mémoire rend compte consistait à élaborer un outil de normalisation automatique des textes non standard en français, en particulier les tweets et les SMS. Pour cela, nous avons d’abord annoté un corpus de 1000 tweets et 1000 SMS, en fonction de phénomènes morpho-lexicaux et morpho-syntaxiques, que nous avions au préalable identifiés lors de l’élaboration d’une typologie pour l’annotation de textes non standard. À partir de l’observation de ce corpus, nous avons développé un outil de normalisation automatique qui génère pour chaque token non standard un ensemble de candidats en fonction des phénomènes observés le plus fréquemment dans notre corpus de tweets et de SMS. Ensuite, la normalisation du token non standard est sélectionnée parmi l’ensemble de ces candidats, à l’aide d’un système d’attribution de scores prenant également en compte le contexte immédiat du token traité."
  },
  {
    "title_s": [
      "Identification automatique des oronymes dans un corpus de récits d’exploration des Alpes"
    ],
    "keyword_s": [
      "Oronym",
      "NER",
      "Machine learning",
      "NLP",
      "BERT",
      "Oronyme",
      "NER",
      "Apprentissage automatique",
      "TAL",
      "BERT"
    ],
    "abstract_s": [
      "Dans cette tâche, nous avons élaboré un jeu de données en utilisant 1221 oronymes pour affiner les modèles avancés basés sur le Transformer tels que BERT. L'objectif principal est d'identifier les oronymes à partir de notre corpus spécialisé contenant 1,4 million de mots de récits d'explorations des Alpes. Nous avons ensuite utilisé ce jeu de données pour évaluer la performance de spaCy, un modèle général. Notre ambition est d'améliorer la précision de la reconnaissance pour les futurs projets de dénomination géophysique. De plus, nous souhaitons contribuer à l'étude et à l'application du traitement automatique de la langue, en particulier dans le domaine de la reconnaissance des entités nommées.",
      "In this task, we created a dataset using 1221 oronyms to fine-tune advanced Transformer-based models such as BERT, with the aim of identifying oronyms from our specialised corpus of 1.4 million words of Alps exploration narratives. The resultant dataset was also employed to evaluate spaCy, a general-purpose model. We strive to enhance the precision of identification for forthcoming projects in geophysical naming, as well as to support the study and application of natural language processing, particularly in the field of named entity recognition."
    ],
    "authFullName_s": [
      "Xiao Ma"
    ],
    "halId_s": "dumas-04260762",
    "producedDateY_i": 2023,
    "texte_nettoye": "Dans cette tâche, nous avons élaboré un jeu de données en utilisant 1221 oronymes pour affiner les modèles avancés basés sur le Transformer tels que BERT. L'objectif principal est d'identifier les oronymes à partir de notre corpus spécialisé contenant 1,4 million de mots de récits d'explorations des Alpes. Nous avons ensuite utilisé ce jeu de données pour évaluer la performance de spaCy, un modèle général. Notre ambition est d'améliorer la précision de la reconnaissance pour les futurs projets de dénomination géophysique. De plus, nous souhaitons contribuer à l'étude et à l'application du traitement automatique de la langue, en particulier dans le domaine de la reconnaissance des entités nommées."
  },
  {
    "title_s": [
      "Extraction d'expressions polylexicales sur corpus arboré"
    ],
    "keyword_s": [
      "Dependency parsing",
      "Treebank",
      "MWE",
      "Multiword expression",
      "Expression polylexicale",
      "Corpus arboré",
      "Grammaire de dépendances"
    ],
    "abstract_s": [
      "Ce document présente une méthode d'extraction d'expressions polylexicales à partir de corpus analysés syntaxiquement. Ces expressions restent un problème central pour le traitement automatique des langues naturelles, et leur extraction et encodage automatiques sont des tâches encore non résolues. L'approche implémentée permet en particulier d'extraire des expressions de plus de deux mots, et une attention particulière a été portée aux constructions récurrentes imbriquées. Une description morphosyntaxique fine des unités extraites est également générée, en termes de relations syntaxiques, ordres des mots possibles, distance entre mots, flexion ou détermination, informations qui nous semblent nécessaires à leur bonne intégration aux lexiques, pour des applications comme l'extraction d'information ou la traduction automatique.",
      "This document describes a method for extracting multiword expressions from syntactically analyzed corpora. These expressions are still a main issue for NLP applications. Their automatic extraction, and appropriate description, remain largely unsolved problems. This approach most notably allows the extraction of expressions composed of more than two words, and focuses on the issue of nested recurrent structures. It also yields a finegrained morphosyntactic description of the extracted units, including syntactic relations, possible word orders, contiguity, inflection or determination, which we think is necessary to a proper encoding of these expressions in lexicons, for applications such as information extraction or machine translation."
    ],
    "authFullName_s": [
      "Julien Corman"
    ],
    "halId_s": "dumas-00704873",
    "producedDateY_i": 2012,
    "texte_nettoye": "Ce document présente une méthode d'extraction d'expressions polylexicales à partir de corpus analysés syntaxiquement. Ces expressions restent un problème central pour le traitement automatique des langues naturelles, et leur extraction et encodage automatiques sont des tâches encore non résolues. L'approche implémentée permet en particulier d'extraire des expressions de plus de deux mots, et une attention particulière a été portée aux constructions récurrentes imbriquées. Une description morphosyntaxique fine des unités extraites est également générée, en termes de relations syntaxiques, ordres des mots possibles, distance entre mots, flexion ou détermination, informations qui nous semblent nécessaires à leur bonne intégration aux lexiques, pour des applications comme l'extraction d'information ou la traduction automatique."
  },
  {
    "title_s": [
      "Apport des informations visuelles dans la perception et la production du phonème non natif /v/ chez les apprenants thaïlandais"
    ],
    "keyword_s": [
      "V/ consonant",
      "Confusion between /v/ et /w",
      "Audiovisual speech presentation",
      "Speech processing",
      "Thai learners",
      "Traitement de la parole",
      "Apprenants thaïlandais",
      "Présentation audiovisuelle de la parole",
      "Confusion entre /v/ et /w",
      "Consonne /v"
    ],
    "abstract_s": [
      "Cette étude s’inscrivant dans le cadre de l’enseignement et de l’apprentissage d’une LE porte sur des difficultés récurrentes dans le traitement de la consonne non native /v/ en position initiale de syllabe chez les apprenants thaïlandais. Il a été démontré que cette consonne en position initiale de syllabe est très souvent confondue avec /w/. Nous avons effectué une étude expérimentale pour vérifier l’apport des informations audiovisuelles de la parole dans le traitement de la consonne /v/ chez les apprenants thaïlandais du français. Deux tâches (perception et production) ont été effectuées en deux modalités (modalités auditive et audiovisuelle). Les résultats montrent que le score de réussite en modalité audiovisuelle est significativement plus élevé qu’en modalité auditive, quel que soit le type de tâche. La confusion entre /v/ et /w/ est moins présente en modalité audiovisuelle qu’en modalité auditive. De plus, le temps de réponse en perception est significativement moins important en modalité audiovisuelle qu’en modalité auditive.",
      "This study within the framework of foreign language teaching and learning focuses on the recurrent difficulty in processing the non-native consonant /v/ in syllable-initial position among Thai learners. It has been shown that this consonant in syllable-initial position is very often confused with /w/. We therefore conducted an experimental study to verify the contribution of audiovisual speech information in the processing of the consonant /v/ in Thai learners of French. Two tasks (perception and production) were conducted, and two modalities (auditory and audiovisual modalities) were set up. The results show that the percentage of success in the audiovisual modality is significantly higher than that in the auditory modality, whatever the type of task. The confusion between /v/ and /w/ is less frequent in the audiovisual modality than in the auditory modality. Also, the response time in the audiovisual modality of the perception test is significantly less than that of the auditory modality."
    ],
    "authFullName_s": [
      "Supansa Tusnyingyong"
    ],
    "halId_s": "dumas-03794537",
    "producedDateY_i": 2022,
    "texte_nettoye": "Cette étude s’inscrivant dans le cadre de l’enseignement et de l’apprentissage d’une LE porte sur des difficultés récurrentes dans le traitement de la consonne non native /v/ en position initiale de syllabe chez les apprenants thaïlandais. Il a été démontré que cette consonne en position initiale de syllabe est très souvent confondue avec /w/. Nous avons effectué une étude expérimentale pour vérifier l’apport des informations audiovisuelles de la parole dans le traitement de la consonne /v/ chez les apprenants thaïlandais du français. Deux tâches (perception et production) ont été effectuées en deux modalités (modalités auditive et audiovisuelle). Les résultats montrent que le score de réussite en modalité audiovisuelle est significativement plus élevé qu’en modalité auditive, quel que soit le type de tâche. La confusion entre /v/ et /w/ est moins présente en modalité audiovisuelle qu’en modalité auditive. De plus, le temps de réponse en perception est significativement moins important en modalité audiovisuelle qu’en modalité auditive."
  },
  {
    "title_s": [
      "Étude des problèmes de prononciation des consonnes fricatives du français par des apprenants thaïlandais et propositions de correction phonétique"
    ],
    "keyword_s": [
      "System of automatic speech recognition Kaldi",
      "Thai phonetic",
      "French",
      "Pronunciation acquisition",
      "Thai learners",
      "Auditory discrimination",
      "Discrimination auditive",
      "Apprenant thaïlandais",
      "Français",
      "Thaï",
      "FLE",
      "Correction phonétique",
      "Système de reconnaissance automatique de parole Kaldi",
      "Appropriation de la prononciation",
      "Perception",
      "Production"
    ],
    "abstract_s": [
      "Cette recherche se situe dans le cadre des études contrastives. Son objectif est d’examiner et de confirmer l'existence des interférences des consonnes fricatives entre le français et le thaï ; ces interférences influencent en effet de façon assez importante la perception et la prononciation des consonnes fricatives du français chez les apprenants thaïlandais. À partir d’une analyse comparative entre les systèmes phonologiques des consonnes fricatives du français et du thaï, nous avons répertorié les principales difficultés de prononciation des consonnes fricatives chez les apprenants thaïlandais. Nous avons évalué ce phénomène à l’aide d’un test d’identification auprès d’un système de reconnaissance automatique de parole (Kaldi). Nous espérons que les analyses phonologiques des deux langues, les résultats de la recherche et les propositions de correction phonétique pour les apprenants thaïlandais pourraient aider les enseignants de français en Thaïlande à appliquer les méthodes adéquates face aux difficultés des Thaïlandais.",
      "This research is mainly related with contrastive studies. The objectives of this study are to examine the interference of Thai sounds in French fricative consonants. In fact, these interferences influence significantly perception and pronunciation of French fricatives among Thai students. From the analysis of French and Thai phonological systems regarding fricative consonants, the researcher could identify main difficulties of pronunciation of French fricatives faced by Thai students. This phenomenon was assessed by using an identification test with automatic speech recognition system (Kaldi). This study, at last, shows the result of the test and proposes possible phonetic corrections for Thai students. We hope that the comparative study of these two languages can help teachers of French as a foreign language (FLE) in Thailand to apply necessary methods in order to help students with their difficulties."
    ],
    "authFullName_s": [
      "Sakson Promkesa"
    ],
    "halId_s": "dumas-01078534",
    "producedDateY_i": 2014,
    "texte_nettoye": "Cette recherche se situe dans le cadre des études contrastives. Son objectif est d’examiner et de confirmer l'existence des interférences des consonnes fricatives entre le français et le thaï ; ces interférences influencent en effet de façon assez importante la perception et la prononciation des consonnes fricatives du français chez les apprenants thaïlandais. À partir d’une analyse comparative entre les systèmes phonologiques des consonnes fricatives du français et du thaï, nous avons répertorié les principales difficultés de prononciation des consonnes fricatives chez les apprenants thaïlandais. Nous avons évalué ce phénomène à l’aide d’un test d’identification auprès d’un système de reconnaissance automatique de parole (Kaldi). Nous espérons que les analyses phonologiques des deux langues, les résultats de la recherche et les propositions de correction phonétique pour les apprenants thaïlandais pourraient aider les enseignants de français en Thaïlande à appliquer les méthodes adéquates face aux difficultés des Thaïlandais."
  },
  {
    "title_s": [
      "Moteurs de recherche et restitution de l'information dans les grandes entreprises : l'exemple du portail Cyberthèque de la Direction des Systèmes d'Information de la Société Générale"
    ],
    "keyword_s": [
      "CLASSIFICATION AUTOMATIQUE",
      "RÉSUMÉ AUTOMATIQUE",
      "CATÉGORISATION AUTOMATIQUE",
      "ÉTIQUETAGE",
      "LEMMATISATION",
      "SEGMENTATION",
      "ANALYSE LINGUISTIQUE",
      "TALN",
      "ONTOLOGIE",
      "TOPIC MAPS",
      "TAXONOMIE",
      "THÉSAURUS",
      "LANGAGE DOCUMENTAIRE",
      "INDEXATION AUTOMATIQUE",
      "VERITY",
      "MOTEUR DE RECHERCHE"
    ],
    "abstract_s": [
      "Après avoir replacé les moteurs de recherche dans le contexte de la recherche d'information et des langages documentaires (notamment le thésaurus), compte tenu des mutations de ces dernières années (taxonomies, ontologies, Topic Maps), la première partie du mémoire se propose de décrire le fonctionnement de ces outils issus de la recherche en traitement automatique du langage (TALN). La définition du TALN, en soulignant les apports de chaque discipline impliquée avec un éclairage particulier sur la linguistique, est suivie d'une typologie des produits présents dans les grandes entreprises. Le fonctionnement des moteurs de recherche est décrit ensuite à travers les opérations effectuées par les moteurs de recherche linguistiques pour traiter la masse d'information textuelle lors de l'indexation. Cette description prend pour exemple un produit particulier : \"K2 Enterprise\" de la société Verity. La seconde partie retrace l'audit effectué afin de mettre en place des améliorations dans un portail d'entreprise dont la recherche est gérée par le moteur de recherche Verity K2 : la Cyberthèque de la Direction des Systèmes d'Information de la branche Banque de Détail de la Société Générale, portail de veille technologique et concurrentielle."
    ],
    "authFullName_s": [
      "Alina Ivanciuc Deniau"
    ],
    "halId_s": "mem_00000013",
    "producedDateY_i": 2003,
    "texte_nettoye": "Après avoir replacé les moteurs de recherche dans le contexte de la recherche d'information et des langages documentaires (notamment le thésaurus), compte tenu des mutations de ces dernières années (taxonomies, ontologies, Topic Maps), la première partie du mémoire se propose de décrire le fonctionnement de ces outils issus de la recherche en traitement automatique du langage (TALN). La définition du TALN, en soulignant les apports de chaque discipline impliquée avec un éclairage particulier sur la linguistique, est suivie d'une typologie des produits présents dans les grandes entreprises. Le fonctionnement des moteurs de recherche est décrit ensuite à travers les opérations effectuées par les moteurs de recherche linguistiques pour traiter la masse d'information textuelle lors de l'indexation. Cette description prend pour exemple un produit particulier : \"K2 Enterprise\" de la société Verity. La seconde partie retrace l'audit effectué afin de mettre en place des améliorations dans un portail d'entreprise dont la recherche est gérée par le moteur de recherche Verity K2 : la Cyberthèque de la Direction des Systèmes d'Information de la branche Banque de Détail de la Société Générale, portail de veille technologique et concurrentielle."
  },
  {
    "title_s": [
      "Conception et développement d'un module de traitement des lemmes comportant des lettres dérivatives"
    ],
    "keyword_s": [
      "Silent letters",
      "Derivational letters",
      "NLP",
      "Spelling",
      "TAL",
      "Lettres dérivatives",
      "Lettres muettes",
      "Orthographe"
    ],
    "abstract_s": [
      "L'une des difficultés les plus fréquentes lors de l'apprentissage du français est l'orthographe des lettres muettes qui sont très présentes à l'écrit. En effet, la majorité des mots en français se terminent par une lettre muette. Ces lettres muettes sont principalement dues à la morphologie dérivationnelle et marquent l'appartenance à une même famille de mots (le t de 'chat' dans 'chaton'). Ce mémoire présente le processus de la conception d'un outil de traitement automatique du langage permettant de détecter automatiquement ces lettres dérivatives. Cet outil permettra ensuite d'étudier les performances des élèves face aux lettres muettes selon leur nature dérivationnelle ou non.",
      "The spelling of silent letters is one of the most common difficulties for French learners. Moreover these silent letters are very common in French. Indeed, most of the words in French end with a silent letter. These silent letters are mainly due to derivational morphology and specify the belonging of the same semantic and morphological family (the t in 'chat' in 'chaton'). This master thesis introduces the process of designing a natural language processing tool to detect these derivational letters. This tool will then be used to study students' skills beside silent letters according to their derivational or non-derivational nature."
    ],
    "authFullName_s": [
      "Cynthia Rakotoarisoa"
    ],
    "halId_s": "dumas-03485502",
    "producedDateY_i": 2021,
    "texte_nettoye": "L'une des difficultés les plus fréquentes lors de l'apprentissage du français est l'orthographe des lettres muettes qui sont très présentes à l'écrit. En effet, la majorité des mots en français se terminent par une lettre muette. Ces lettres muettes sont principalement dues à la morphologie dérivationnelle et marquent l'appartenance à une même famille de mots (le t de 'chat' dans 'chaton'). Ce mémoire présente le processus de la conception d'un outil de traitement automatique du langage permettant de détecter automatiquement ces lettres dérivatives. Cet outil permettra ensuite d'étudier les performances des élèves face aux lettres muettes selon leur nature dérivationnelle ou non."
  },
  {
    "title_s": [
      "Analyse syntaxique automatique d'un corpus du français médiéval"
    ],
    "keyword_s": [
      "Part-of-speech tagging",
      "Phraseological units",
      "Parsing",
      "NLP",
      "Étiquetage morphosyntaxique",
      "Unités phraséologiques",
      "Analyse syntaxique",
      "TAL"
    ],
    "abstract_s": [
      "Pour explorer le rôle de la phraséologie étendue dans la structuration des genres textuels littéraires en français médiéval, les techniques de TAL et la linguistique de corpus prennent une part décisive, car les corpus informatisés et les programmes qui les utilisent permettent d'identifier les unités phraséologiques. Dans cet article, nous présentons un travail de traitement du corpus à l'aide de LGeRM, une plateforme de lemmatisation du français médiéval et de l'analyseur syntaxique Hops, incluant le traitement de lemmatiser, étiqueter et analyser en dépendances syntaxiques. Cet article détaille également comment le corpus traité peut être intégré dans le Lexicoscope, l'outil de fouille textuelle. L'objectif est de pouvoir constituer le corpus sur le Lexicoscope et d'utiliser les fonctionnalités de cet outil pour mener des recherches sur l'exploration des liens entre la phraséologie étendue et les genres textuels en français médiéval.",
      "For explore the role of extended phraseology in the structuring of literary textual genres in medieval French, NLP techniques and corpus linguistics play a decisive role, as computerized corpus and the programs allow the identification of phraseological units. In this article, we present a corpus processing work using LGeRM, a lemmatization platform for medieval French, and the syntactic parser Hops, including lemmatization, tagging and syntactic dependency parsing. This article also details how the processed corpus can be integrated into the Lexicoscope, tool of text mining. The aim is to be able to build the corpus on the Lexicoscope and to use the functionalities of this tool to conduct research on the exploration of the links between extended phraseology and textual genres in medieval French."
    ],
    "authFullName_s": [
      "Jingyu Liu"
    ],
    "halId_s": "dumas-03485357",
    "producedDateY_i": 2021,
    "texte_nettoye": "Pour explorer le rôle de la phraséologie étendue dans la structuration des genres textuels littéraires en français médiéval, les techniques de TAL et la linguistique de corpus prennent une part décisive, car les corpus informatisés et les programmes qui les utilisent permettent d'identifier les unités phraséologiques. Dans cet article, nous présentons un travail de traitement du corpus à l'aide de LGeRM, une plateforme de lemmatisation du français médiéval et de l'analyseur syntaxique Hops, incluant le traitement de lemmatiser, étiqueter et analyser en dépendances syntaxiques. Cet article détaille également comment le corpus traité peut être intégré dans le Lexicoscope, l'outil de fouille textuelle. L'objectif est de pouvoir constituer le corpus sur le Lexicoscope et d'utiliser les fonctionnalités de cet outil pour mener des recherches sur l'exploration des liens entre la phraséologie étendue et les genres textuels en français médiéval."
  },
  {
    "title_s": [
      "Création d’une intelligence artificielle capable de détecter les impacts de qualité de vie liée à la santé à partir de données de vie réelle",
      "Creating an artificial intelligence capable of detecting health-related quality of life impacts from real-life data"
    ],
    "keyword_s": [
      "Intelligence artificielle en médecine",
      "Patients -- Satisfaction -- Évaluation",
      "Réseaux sociaux Internet",
      "Traitement automatique du langage naturel"
    ],
    "abstract_s": [
      "L’objet de ce travail était de réaliser un algorithme d’Intelligence artificielle, ou plus précisément, un modèle de traitement automatisé du langage, capable d’identifier dans des témoignages libres de patients, les verbatims reflétant un impact de qualité de vie. Il existe une synergie entre le patient centrisme et l’essor des données de vie réelle. Ensemble de données médicalement contextualisées, elles reflètent et objectivent la réalité patiente, par définition non captée lors d'essais cliniques. Sur les réseaux sociaux et forums médicaux, les patients forment des communautés en ligne dans un but de soutien, d’information et de partage d’expériences médicales. Ces commentaires publics sont récupérables informatiquement, dans le respect des réglementations pour la protection des données. C’est là que le traitement automatisé du langage entre en jeu. En étant capable d’analyser et de traduire le langage patient en ontologie médicale, il devient possible de gagner en information sur la réalité patiente, telle que décrite directement et sans filtre par eux-mêmes. Ainsi, notre algorithme répond à l’enjeu de compréhension fine de ce qui peut réellement impacter la qualité de vie des patients, en vie réelle.",
      "The purpose of this work was to develop an artificial intelligence algorithm, or more precisely, an automated language processing model, capable of identifying in free patient testimonies, the verbatims reflecting an impact on quality of life. There is a synergy between patient centrism and the rise of real-life data. Medically contextualized data sets reflect and objectify patient reality, which by definition is not captured in clinical trials. On social networks and medical forums, patients form online communities for support, information and sharing of medical experiences. These public comments can be retrieved electronically, in compliance with data protection regulations. This is where automated language processing comes in. By being able to analyze and translate patient language into medical ontology, it becomes possible to gain information about patient reality, as described directly and unfiltered by themselves. Thus, our algorithm meets the challenge of fine understanding of what can really impact the quality of life of patients, in real life."
    ],
    "authFullName_s": [
      "Tom Marty"
    ],
    "halId_s": "dumas-03956122",
    "producedDateY_i": 2022,
    "texte_nettoye": "L’objet de ce travail était de réaliser un algorithme d’Intelligence artificielle, ou plus précisément, un modèle de traitement automatisé du langage, capable d’identifier dans des témoignages libres de patients, les verbatims reflétant un impact de qualité de vie. Il existe une synergie entre le patient centrisme et l’essor des données de vie réelle. Ensemble de données médicalement contextualisées, elles reflètent et objectivent la réalité patiente, par définition non captée lors d'essais cliniques. Sur les réseaux sociaux et forums médicaux, les patients forment des communautés en ligne dans un but de soutien, d’information et de partage d’expériences médicales. Ces commentaires publics sont récupérables informatiquement, dans le respect des réglementations pour la protection des données. C’est là que le traitement automatisé du langage entre en jeu. En étant capable d’analyser et de traduire le langage patient en ontologie médicale, il devient possible de gagner en information sur la réalité patiente, telle que décrite directement et sans filtre par eux-mêmes. Ainsi, notre algorithme répond à l’enjeu de compréhension fine de ce qui peut réellement impacter la qualité de vie des patients, en vie réelle."
  },
  {
    "title_s": [
      "Étude des modèles neuronaux profonds pour la compréhension automatique du langage naturel dans les habitats intelligents"
    ],
    "keyword_s": [
      "Corpus generation",
      "Natural language understanding",
      "Deep learning",
      "Machine learning",
      "Smart home",
      "Génération du corpus",
      "Modèles profonds",
      "Apprentissage automatique",
      "Habitat intelligent",
      "Compréhension automatique de la langue"
    ],
    "abstract_s": [
      "Le but du projet VocADom est de créer un système de compréhension automatique des ordres vocaux qui pourrait être fonctionnel dans un habitat intelligent pour les personnes âgées. L’approche par apprentissage automatique nous semble plus flexible que l’approche par règles, ce dernier ne prenant pas en compte les énoncés qui ne se conforment pas à la grammaire ; et les études ont montré que les personnes âgées sont enclines à s’écarter de la grammaire imposée. Pourtant il y a un manque de données d’apprentissage dans ce domaine. Pour contourner ce problème, une grammaire générant le corpus artificiel pour le domaine de l’habitat intelligent a été créé. Un des problèmes traités par ce mémoire est l’amélioration de la grammaire existante. Après amélioration, le corpus artificiel résultant compte 42195 phrases annotées. Il a été ensuite évalué en entraînant avec lui trois modèles états de l’art - Tri-CRF, att-RNN, RASA - ainsi que Tf-seq2seq, modèle séquence-vers-séquence. Ces modèles ont ensuite été testés sur un petit corpus d’ordres naturels enregistrés dans le cadre du projet VocADom. Cela nous a permis de voir les limites du corpus artificiel. Ces modèles ont ensuite été comparés aux modèles entraînés et testés sur le corpus de données réelles PORTMEDIA qui relève du domaine du tourisme. Cette comparaison nous a permis de savoir que la performance des modèles entraînés sur le corpus artificiel est due aux limitations du corpus artificiel plutôt qu’aux limitations des modèles eux-mêmes. Tous les modèles entraînés et testés sur PORTMEDIA avaient rendu de bons résultats mais les modèles entraînés sur le corpus artificiel et testés sur le corpus naturel VocADom ont été bien moins performants. Ceci montre la difficulté à prendre en charge la diversité lexicale et syntaxique d’un corpus réel. Tous les modèles ont des performances comparables, mais Tf-seq2seq, contrairement à Tri-CRF, att-RNN et RASA, ne requiert pas de données alignées et est plus facile à entraîner.",
      "The goal of the VocADom project is to create a natural language understanding system for processing voice orders in a smart home for the elderly. Machine learning approach seems more flexible than the rule-based approach, which does not take into account orders that do not conform to grammar ; and studies have shown that older people are inclined to deviate from imposed grammar. However there is a lack of learning data in this domain. To remedy the problem, a grammar generating the artificial corpus for the smart home domain was created. One of the goals of this work is to improve this grammar. After the improvement, the resulting artificial corpus has 42195 annotated sentences. It was then evaluated by training three state-of-the-art models on it - Tri-CRF, att-RNN, RASA, as well as Tf-seq2seq, a sequence-to-sequence model. The models were then tested on a small natural existing corpus recorded as part of the VocADom project. This allowed us to see the limitations of the artificial corpus. These models were then compared to the models that were trained and tested on the PORTMEDIA corpus of touristic domain. This comparison allowed us to know that the performance of the models trained on the artificial corpus is due to the limitations of the artificial corpus rather than the limitations of the models themselves. All the models trained and tested on PORTMEDIA gave good results but the models trained on the artificial corpus and tested on the VocADom corpus were much less efficient. This shows the difficulty of accounting for the lexical and syntactic diversity of the real data. All models show comparable performances but Tf-seq2seq, unlike Tri-CRF, att-RNN et RASA, doesn’t require aligned data and is easier to train."
    ],
    "authFullName_s": [
      "Anastasiia Mishakova"
    ],
    "halId_s": "dumas-01846913",
    "producedDateY_i": 2018,
    "texte_nettoye": "Le but du projet VocADom est de créer un système de compréhension automatique des ordres vocaux qui pourrait être fonctionnel dans un habitat intelligent pour les personnes âgées. L’approche par apprentissage automatique nous semble plus flexible que l’approche par règles, ce dernier ne prenant pas en compte les énoncés qui ne se conforment pas à la grammaire ; et les études ont montré que les personnes âgées sont enclines à s’écarter de la grammaire imposée. Pourtant il y a un manque de données d’apprentissage dans ce domaine. Pour contourner ce problème, une grammaire générant le corpus artificiel pour le domaine de l’habitat intelligent a été créé. Un des problèmes traités par ce mémoire est l’amélioration de la grammaire existante. Après amélioration, le corpus artificiel résultant compte 42195 phrases annotées. Il a été ensuite évalué en entraînant avec lui trois modèles états de l’art - Tri-CRF, att-RNN, RASA - ainsi que Tf-seq2seq, modèle séquence-vers-séquence. Ces modèles ont ensuite été testés sur un petit corpus d’ordres naturels enregistrés dans le cadre du projet VocADom. Cela nous a permis de voir les limites du corpus artificiel. Ces modèles ont ensuite été comparés aux modèles entraînés et testés sur le corpus de données réelles PORTMEDIA qui relève du domaine du tourisme. Cette comparaison nous a permis de savoir que la performance des modèles entraînés sur le corpus artificiel est due aux limitations du corpus artificiel plutôt qu’aux limitations des modèles eux-mêmes. Tous les modèles entraînés et testés sur PORTMEDIA avaient rendu de bons résultats mais les modèles entraînés sur le corpus artificiel et testés sur le corpus naturel VocADom ont été bien moins performants. Ceci montre la difficulté à prendre en charge la diversité lexicale et syntaxique d’un corpus réel. Tous les modèles ont des performances comparables, mais Tf-seq2seq, contrairement à Tri-CRF, att-RNN et RASA, ne requiert pas de données alignées et est plus facile à entraîner."
  },
  {
    "title_s": [
      "Développement d'un module de génération de paraphrases pour la data augmentation"
    ],
    "keyword_s": [
      "Semantics",
      "Syntax",
      "Natural langage processing",
      "Paraphrase",
      "Data augmentation",
      "Sémantique",
      "Syntaxe",
      "Data augmentation",
      "Traitement automatique de la langue",
      "Paraphrase"
    ],
    "abstract_s": [
      "Ce document résume 6 mois de travail au sein de l’équipe Recherche et Développement de Linagora Toulouse. Le but de ce projet était de développer un module de paraphrase permettant d’enrichir le corpus d’apprentissage de l’agent conversationnel LinTO. Nous avons commencé par analyser les différentes commandes présentes dans notre corpus initial : les structures syntaxiques récurrentes et les mécanismes de paraphrasage qu’on peut leur appliquer. À partir de ces observations, nous avons créé une grammaire à base de règles pour générer plusieurs paraphrases d’une commande en entrée.",
      "This paper summarizes the work completed during a six-month internship in the Research & Development team of Linagora. The aim of this project is to develop a paraphrasing tool able to expand the training datasets of the smart vocal assistant LinTO. We started by analyzing the existing commands in the original corpus : the recurrent syntactic structures and the paraphrasing mecanisms that can be applied to them. From these observations, we created a rule-based grammar to generate semantically and synctactically correct sentences."
    ],
    "authFullName_s": [
      "Sonia Ratsiandavana"
    ],
    "halId_s": "dumas-02294883",
    "producedDateY_i": 2019,
    "texte_nettoye": "Ce document résume 6 mois de travail au sein de l’équipe Recherche et Développement de Linagora Toulouse. Le but de ce projet était de développer un module de paraphrase permettant d’enrichir le corpus d’apprentissage de l’agent conversationnel LinTO. Nous avons commencé par analyser les différentes commandes présentes dans notre corpus initial : les structures syntaxiques récurrentes et les mécanismes de paraphrasage qu’on peut leur appliquer. À partir de ces observations, nous avons créé une grammaire à base de règles pour générer plusieurs paraphrases d’une commande en entrée."
  },
  {
    "title_s": [
      "<i>DeCorScol</i> : conception d'un outil d'assistance à l'annotation des chaînes de coréférence dans les écrits scolaires"
    ],
    "keyword_s": [
      "Cohésion",
      "Cohérence",
      "Traitement automatique des langues",
      "Coréférence",
      "Écrits scolaires"
    ],
    "abstract_s": [
      "Quelles sont les qualités qui distinguent un texte « bien écrit » d’un texte « mal écrit », « incompréhensible », « peu ou pas cohérent » ? Divers facteurs entrent en jeu lors de la compréhension (et écriture) d’un texte sur plusieurs plans langagiers. Entre les différents éléments qui rendent un texte cohérent et cohésif, les chaînes de coréférence jouent sans doute un rôle de premier plan. Elles permettent aux lecteurs de suivre le cheminement des personnages impliqués dans les actions évoquées par le récit et, aux scripteurs plus avancés, d’attribuer des qualités à leurs personnages, ainsi que de les faire agir dans l’univers fictionnel qu’ils décrivent. Cependant, comment se réalise la coréférence dans les écrits scolaires et comment est-il possible de la décrire de manière à fournir des critères d’évaluation objectifs aux enseignants ? L’un des buts de ce travail est de fournir un outil TAL d’assistance à l’annotation de ce phénomène, de manière à contribuer à cette « cartographie de l’emploi des formes linguistiques qui déterminent la cohérence et la cohésion textuelles » déjà initiée en France (Garcia-Debanc et al., 2021). Nous allons introduire dans le chapitre 1 la notion de coréférence en linguistique et en Traitement Automatique des Langues. Nous décrirons ensuite le domaine des corpus scolaires dans le chapitre 2 puis nous aborderons les approches utilisées pour l’annotation de la coréférence sur des corpus variés dans le chapitre 3. La question de la coréférence et de l’annotation de la continuité référentielle dans les corpus scolaires sera traitée dans le chapitre 4. Depuis les premières recherches en TAL sur l’écriture des apprenants dans les années 80, ce sont des questions didactiques qui pilotent les travaux linguistiques sur les textes d’élèves (Doquet et Ponton, 2021). En nous positionnant dans cette tradition, nous souhaitons ici pouvoir répondre à certains questionnements partagés avec la didactique, en recourant au développement et à l’application d’un outil de Traitement Automatique des Langues. Celui-ci servira d’aide à l’annotation des chaînes de coréférence sur les données du corpus scolaire Scoledit (CE2). Cet outil, nommé <i>DeCorScol</i> (<i>Détection des Coréférences sur les écrits Scolaires</i>), a comme but d’offrir une aide automatisée pour l’annotation des chaînes de coréférences dans le cadre du corpus Scoledit. La méthodologie de travail que nous avons adoptée sur le corpus Scoledit sera présentée dans le chapitre 5. Nous décrirons ensuite l’architecture de l’outil dans le chapitre 6. Cet outil nous a déjà permis d’effectuer des tests d’annotation. Sur la base de ces données, nous avons pu tirer nos premières réflexions que nous présenterons plus en détail dans le chapitre 7. Elles portent à la fois sur les configurations des chaînes de coréférence dans les écrits scolaires de niveau CE2 et sur la manière dont la modélisation linguistique et le TAL nous permettent de cerner cette configuration à savoir : 1. la nature facultative des ressources sémantiques pour la détection des mentions nominales dans des productions de CE2, donc l’observation d’une variété lexicale limitée dans ces types de mentions. 2. le postulat d’une prépondérance de continuité thématique dans la construction des récits par la part des élèves ; 3. la productivité de certaines règles déjà exploitées pour la détection de coréférences dans des textes rédigés par des scripteurs experts (comme les verbes pronominaux) et l’absence d’autre structures syntaxiques plus complexes, également exploitées afin de résoudre la coréférence dans des textes de scripteurs experts, dans les écrits scolaires du niveau objet de notre analyse. L’outil que nous proposons est fonctionnel pour l’étude du développement des notions de textualité et de cohérence textuelle tout au long du parcours d’apprentissage de l’écriture chez l’enfant. Nous prévoyons effectivement que cet outil puisse être davantage utilisé sur les niveaux restants du corpus objet de nos études, de manière à fournir par la suite une cartographie longitudinale des éléments qui composent la notion de coréférence dans les écrits scolaires de ces niveaux."
    ],
    "authFullName_s": [
      "Martina Barletta"
    ],
    "halId_s": "dumas-03826763",
    "producedDateY_i": 2022,
    "texte_nettoye": "Quelles sont les qualités qui distinguent un texte « bien écrit » d’un texte « mal écrit », « incompréhensible », « peu ou pas cohérent » ? Divers facteurs entrent en jeu lors de la compréhension (et écriture) d’un texte sur plusieurs plans langagiers. Entre les différents éléments qui rendent un texte cohérent et cohésif, les chaînes de coréférence jouent sans doute un rôle de premier plan. Elles permettent aux lecteurs de suivre le cheminement des personnages impliqués dans les actions évoquées par le récit et, aux scripteurs plus avancés, d’attribuer des qualités à leurs personnages, ainsi que de les faire agir dans l’univers fictionnel qu’ils décrivent. Cependant, comment se réalise la coréférence dans les écrits scolaires et comment est-il possible de la décrire de manière à fournir des critères d’évaluation objectifs aux enseignants ? L’un des buts de ce travail est de fournir un outil TAL d’assistance à l’annotation de ce phénomène, de manière à contribuer à cette « cartographie de l’emploi des formes linguistiques qui déterminent la cohérence et la cohésion textuelles » déjà initiée en France (Garcia-Debanc et al., 2021). Nous allons introduire dans le chapitre 1 la notion de coréférence en linguistique et en Traitement Automatique des Langues. Nous décrirons ensuite le domaine des corpus scolaires dans le chapitre 2 puis nous aborderons les approches utilisées pour l’annotation de la coréférence sur des corpus variés dans le chapitre 3. La question de la coréférence et de l’annotation de la continuité référentielle dans les corpus scolaires sera traitée dans le chapitre 4. Depuis les premières recherches en TAL sur l’écriture des apprenants dans les années 80, ce sont des questions didactiques qui pilotent les travaux linguistiques sur les textes d’élèves (Doquet et Ponton, 2021). En nous positionnant dans cette tradition, nous souhaitons ici pouvoir répondre à certains questionnements partagés avec la didactique, en recourant au développement et à l’application d’un outil de Traitement Automatique des Langues. Celui-ci servira d’aide à l’annotation des chaînes de coréférence sur les données du corpus scolaire Scoledit (CE2). Cet outil, nommé <i>DeCorScol</i> (<i>Détection des Coréférences sur les écrits Scolaires</i>), a comme but d’offrir une aide automatisée pour l’annotation des chaînes de coréférences dans le cadre du corpus Scoledit. La méthodologie de travail que nous avons adoptée sur le corpus Scoledit sera présentée dans le chapitre 5. Nous décrirons ensuite l’architecture de l’outil dans le chapitre 6. Cet outil nous a déjà permis d’effectuer des tests d’annotation. Sur la base de ces données, nous avons pu tirer nos premières réflexions que nous présenterons plus en détail dans le chapitre 7. Elles portent à la fois sur les configurations des chaînes de coréférence dans les écrits scolaires de niveau CE2 et sur la manière dont la modélisation linguistique et le TAL nous permettent de cerner cette configuration à savoir : 1. la nature facultative des ressources sémantiques pour la détection des mentions nominales dans des productions de CE2, donc l’observation d’une variété lexicale limitée dans ces types de mentions. 2. le postulat d’une prépondérance de continuité thématique dans la construction des récits par la part des élèves ; 3. la productivité de certaines règles déjà exploitées pour la détection de coréférences dans des textes rédigés par des scripteurs experts (comme les verbes pronominaux) et l’absence d’autre structures syntaxiques plus complexes, également exploitées afin de résoudre la coréférence dans des textes de scripteurs experts, dans les écrits scolaires du niveau objet de notre analyse. L’outil que nous proposons est fonctionnel pour l’étude du développement des notions de textualité et de cohérence textuelle tout au long du parcours d’apprentissage de l’écriture chez l’enfant. Nous prévoyons effectivement que cet outil puisse être davantage utilisé sur les niveaux restants du corpus objet de nos études, de manière à fournir par la suite une cartographie longitudinale des éléments qui composent la notion de coréférence dans les écrits scolaires de ces niveaux."
  },
  {
    "title_s": [
      "Outiller la description de la morphologie adjectivale du primaire à l'université"
    ],
    "keyword_s": [
      "Inflectional morphology",
      "Linguistic modeling",
      "NLP Natural Language Processing",
      "Spelling",
      "TAL Traitement Automatique du Langage",
      "Modélisation linguistique",
      "Morphologie flexionnelle",
      "Orthographe"
    ],
    "abstract_s": [
      "En ce qui concerne les compétences orthographiques des élèves, il n’existe pas de réelle description à cause du manque de données. Le projet E-Calm dans lequel s’inscrit ce mémoire cherche donc à rassembler un ensemble de ressources pour chaque niveau scolaire afin de pouvoir se lancer dans des analyses linguistiques à des fins didactiques, et grâce à la mise en place d’outils de TAL. En effet, la quantité de données récoltée ne permet pas les analyses manuelles. Le travail décrit ici est centré sur les aptitudes des apprenants concernant la morphologie adjectivale. Dans un premier temps, ce mémoire propose une modélisation linguistique du comportement morphologique flexionnel de l’adjectif en se basant sur des productions d’élèves et d’étudiants. Dans un second temps, il s’intéresse à la conception d’un outil permettant une réalisation automatique de cette modélisation et enfin à l’analyse des résultats du système présenté.",
      "Concerning french pupils an students’ spelling skills, there is no real description due to a lack of data. The E-Calm project in which this dissertation takes part, therefore, seeks to bring together a set of resources for each school level in order to be able to undertake linguistic analyses for teaching purposes. This is made possible thanks to the implementation of NLP tools; indeed, the amount of data collected does not allow manual analyses. The work described here is focused on the learners' abilities concerning adjectival morphology. Initially, this masters’ thesis proposes a linguistic modeling of the adjective's inflectional morphological behavior based on students’ productions. In a second step, it focuses on the design of a tool allowing an automatic realization of this modeling and finally on the analysis of the results of the presented system."
    ],
    "authFullName_s": [
      "Rachel Gaubil"
    ],
    "halId_s": "dumas-02978282",
    "producedDateY_i": 2020,
    "texte_nettoye": "En ce qui concerne les compétences orthographiques des élèves, il n’existe pas de réelle description à cause du manque de données. Le projet E-Calm dans lequel s’inscrit ce mémoire cherche donc à rassembler un ensemble de ressources pour chaque niveau scolaire afin de pouvoir se lancer dans des analyses linguistiques à des fins didactiques, et grâce à la mise en place d’outils de TAL. En effet, la quantité de données récoltée ne permet pas les analyses manuelles. Le travail décrit ici est centré sur les aptitudes des apprenants concernant la morphologie adjectivale. Dans un premier temps, ce mémoire propose une modélisation linguistique du comportement morphologique flexionnel de l’adjectif en se basant sur des productions d’élèves et d’étudiants. Dans un second temps, il s’intéresse à la conception d’un outil permettant une réalisation automatique de cette modélisation et enfin à l’analyse des résultats du système présenté."
  },
  {
    "title_s": [
      "E-Lexic : Extraction lexicale à partir de textes bilingues alignés"
    ],
    "keyword_s": [
      "Word alignment",
      "Word extractor",
      "Dictionnary",
      "Translation",
      "Dictionnaire",
      "Extracteur de mots",
      "Alignement lexical",
      "Traduction"
    ],
    "abstract_s": [
      "On a vu naître, avec l’émergence de nombreux outils de traitement automatique des langues, de nombreux domaines en linguistique computationnelle. Parmi eux on retrouve la traduction assistée par ordinateur. Dans ce sous-domaine de la traduction, on a là encore observé l’apparition de différents outils numériques comme la mémoire de traduction ou ce qui nous intéressera dans ce mémoire, les aligneurs syntagmatiques et lexicaux. Ce mémoire rend compte d’un travail fourni lors du développement d’un extracteur de mots basé sur l’alignement lexical de textes bilingues. L’objectif étant la création grâce à cette extraction d’un dictionnaire bilingue spécifique au texte traité consultable par un utilisateur.",
      "With the emergence of numerous automatic language processing tools, many fields in computational linguistics have been brought to light. Among them we can find computerassisted translation. In this sub-domain of translation, we have again observed the appearance of various digital tools such as translation memory or, what will interest us in this thesis, syntactic and lexical aligners. This dissertation reports on work done in the development of a word extractor based on the lexical alignment of bilingual texts. The objective being the creation, thanks to this extraction, of a bilingual dictionary. This dictionary will be specific to the text that is being processed and can be consulted by a user."
    ],
    "authFullName_s": [
      "Florent Marié"
    ],
    "halId_s": "dumas-02978581",
    "producedDateY_i": 2020,
    "texte_nettoye": "On a vu naître, avec l’émergence de nombreux outils de traitement automatique des langues, de nombreux domaines en linguistique computationnelle. Parmi eux on retrouve la traduction assistée par ordinateur. Dans ce sous-domaine de la traduction, on a là encore observé l’apparition de différents outils numériques comme la mémoire de traduction ou ce qui nous intéressera dans ce mémoire, les aligneurs syntagmatiques et lexicaux. Ce mémoire rend compte d’un travail fourni lors du développement d’un extracteur de mots basé sur l’alignement lexical de textes bilingues. L’objectif étant la création grâce à cette extraction d’un dictionnaire bilingue spécifique au texte traité consultable par un utilisateur."
  },
  {
    "title_s": [
      "Découverte non supervisée de lexique à partir d'un corpus multimodal pour la documentation des langues en danger"
    ],
    "keyword_s": [
      "Endangered languages",
      "Documentation",
      "UTD",
      "Unsupervised term discovery",
      "Multimodal corpus",
      "NLP",
      "Langues en danger",
      "Lexique",
      "Découverte non supervisée",
      "Corpus multimodal",
      "TAL"
    ],
    "abstract_s": [
      "De nombreuses langues disparaissent tous les ans et ce à un rythme jamais atteint auparavant. Les linguistes de terrain manquent de temps et de moyens afin de pouvoir toutes les documenter et décrire avant qu’elles ne disparaissent à jamais. L’objectif de notre travail est donc de les aider dans leur tâche en facilitant le traitement des données. Nous proposons dans ce mémoire des méthodes d’extraction non supervisées de lexique à partir de corpus multimodaux incluant des signaux de parole et des images. Nous proposons également une méthode issue de la recherche d’information afin d’émettre des hypothèses de signification sur les éléments lexicaux découverts. Ce mémoire présente en premier lieu la constitution d’un corpus multimodal parole-image de grande taille. Ce corpus simulant une langue en danger permet ainsi de tester les approches computationnelles de découverte non supervisée de lexique. Dans une seconde partie, nous appliquons un algorithme de découverte non supervisée de lexique utilisant de l’alignement dynamique temporel segmental (S-DTW) sur un corpus multimodal synthétique de grande taille ainsi que sur un corpus multimodal d’une vraie langue en danger, le Mboshi.",
      "Many languages are on the brink of extinction and many disappear each and every year at a rate never seen before. Field linguists lack the time and the means to document and describe all of them before they die out. The goal of our work is to help them in their task, make it easier and speed up the data processing and annotation tasks. In this dissertation, we propose methods to use an unsupervised term discovery (UTD) system to extract lexicon from multimodal corpora consisting of speech and images. We also propose a method using information retrieval techniques to hypothesise the meaning of the discovered lexical items. In the first place, this dissertation presents the creation of a large multimodal corpus which includes speech and images. This corpus simulating that of an endangered language will allow us evaluate the performances of an unsupervised term discovery system. In the second place, we apply an unsupervised term discovery system based on segmental dynamic time warping (S-DTW) to a large synthetic multimodal corpus and also to the multimodal corpus of a real endangered language called Mboshi, spoken in Congo-Brazzaville."
    ],
    "authFullName_s": [
      "William N Havard"
    ],
    "halId_s": "dumas-01562024",
    "producedDateY_i": 2017,
    "texte_nettoye": "De nombreuses langues disparaissent tous les ans et ce à un rythme jamais atteint auparavant. Les linguistes de terrain manquent de temps et de moyens afin de pouvoir toutes les documenter et décrire avant qu’elles ne disparaissent à jamais. L’objectif de notre travail est donc de les aider dans leur tâche en facilitant le traitement des données. Nous proposons dans ce mémoire des méthodes d’extraction non supervisées de lexique à partir de corpus multimodaux incluant des signaux de parole et des images. Nous proposons également une méthode issue de la recherche d’information afin d’émettre des hypothèses de signification sur les éléments lexicaux découverts. Ce mémoire présente en premier lieu la constitution d’un corpus multimodal parole-image de grande taille. Ce corpus simulant une langue en danger permet ainsi de tester les approches computationnelles de découverte non supervisée de lexique. Dans une seconde partie, nous appliquons un algorithme de découverte non supervisée de lexique utilisant de l’alignement dynamique temporel segmental (S-DTW) sur un corpus multimodal synthétique de grande taille ainsi que sur un corpus multimodal d’une vraie langue en danger, le Mboshi."
  },
  {
    "title_s": [
      "« Qu'est ce qu'il dit ? » : analyse de marqueurs audiovisuels de l'incompréhension"
    ],
    "keyword_s": [
      "Incomprehension",
      "Nonverbal",
      "Back-channel",
      "Non-verbal",
      "Rétroaction",
      "Dialogue"
    ],
    "abstract_s": [
      "Ce mémoire porte sur les signaux verbaux et non verbaux qu'émet le récepteur d'un message lors d'un dialogue face à face lorsque qu'il est placé en situation d'incompréhension. Cette recherche se concentre sur deux aspects : Quels sont parmi les signaux émis, ceux auto-adressés et ceux à but communicatif et est ce que le degré d'incompréhension a une influence sur le type de signal produit. Cette recherche a mené à la constitution d'un corpus audio-visuel annoté avec les comportements de quinze sujets. Des analyses statistiques effectuées sur ces annotations ont permis d'isoler neuf comportements relatifs à l'incompréhension. À partir des comportements faciaux isolés nous avons pu extraire un ensemble de paramètres grâce à un outil de détection de mouvements faciaux pour identifier des paramètres pertinents dans l’optique de développer un système de détection.",
      "This thesis addresses the verbal and nonverbal signals that the listener of a message produces during a face to face dialogue in an incomprehension situation. This research focuses on two aspects : identifying among these behaviors, which that are self-adressed and those with a communicative purpose and does the incomprehension degree influence the type of behavior? For this research we built an audio-visual corpus annotated with the behaviors of fourteen subjects. A statistical analysis of these annotations led to singling out nine behaviors related to incomprehension. From these behaviors, we extracted parameters with a facial behavior analysis toolkit in order to check our results from these low-level descriptors."
    ],
    "authFullName_s": [
      "Éric Le Ferrand"
    ],
    "halId_s": "dumas-01901904",
    "producedDateY_i": 2018,
    "texte_nettoye": "Ce mémoire porte sur les signaux verbaux et non verbaux qu'émet le récepteur d'un message lors d'un dialogue face à face lorsque qu'il est placé en situation d'incompréhension. Cette recherche se concentre sur deux aspects : Quels sont parmi les signaux émis, ceux auto-adressés et ceux à but communicatif et est ce que le degré d'incompréhension a une influence sur le type de signal produit. Cette recherche a mené à la constitution d'un corpus audio-visuel annoté avec les comportements de quinze sujets. Des analyses statistiques effectuées sur ces annotations ont permis d'isoler neuf comportements relatifs à l'incompréhension. À partir des comportements faciaux isolés nous avons pu extraire un ensemble de paramètres grâce à un outil de détection de mouvements faciaux pour identifier des paramètres pertinents dans l’optique de développer un système de détection."
  },
  {
    "title_s": [
      "Évaluation de la pertinence des citations dans les articles scientifiques"
    ],
    "keyword_s": [
      "Scientific paper",
      "Similarity of text",
      "Citation accuracy",
      "Articles scientifiques",
      "Similarité textuelle",
      "Pertinence de citations"
    ],
    "abstract_s": [
      "Les citations jouent un rôle important dans la recherche scientifique. Cependant, des études récentes ont révélé un problème préoccupant : de nombreuses citations inexactes sont présentes dans les articles scientifiques. Ces références erronées peuvent entraîner des interprétations erronées, une déformation des intentions de l’auteur original et potentiellement même des conséquences plus graves. Pour faire face à cette préoccupation, notre article présente deux méthodes conçues pour évaluer l’exactitude des citations en utilisant des techniques de traitement automatique du langage (TAL). La première méthode consiste à mesurer les similitudes textuelles à l’aide de représentations vectorielles. La deuxième méthode implique l’utilisation d’une approche de classification de texte affinée pour trouver les citations fiables et les citations erronées. Ces deux méthodologies adoptent des modèles de langage. Selon nos résultats expérimentaux, il semble que la méthode de classification de texte affinée a démontré la meilleure performance.",
      "Citations play an important role in scientific research. However, recent studies have unveiled a concerning issue : numerous inaccurate citations are present within scientific papers. These erroneous references can result in misinterpretations, distortion of the original author’s intentions, and potentially even more serious consequences. To address this concern, our paper introduces two methods designed to assess citation accuracy using Natural Language Processing (NLP) techniques. The first method involves measuring text similarities through vectorized text representations. The second method entails employing a fine-tuned text classification approach to distinguish between reliable and flawed citations. Both of these methodologies adopt language models. Based on our experimental findings, it appears that the fine-tuned text classification method demonstrated the most optimal performance."
    ],
    "authFullName_s": [
      "Qinyue Liu"
    ],
    "halId_s": "dumas-04260594",
    "producedDateY_i": 2023,
    "texte_nettoye": "Les citations jouent un rôle important dans la recherche scientifique. Cependant, des études récentes ont révélé un problème préoccupant : de nombreuses citations inexactes sont présentes dans les articles scientifiques. Ces"
  },
  {
    "title_s": [
      "Apport du TAL à la constitution et l'exploitation d'un corpus scolaire de cours préparatoire"
    ],
    "keyword_s": [
      "Spell checking",
      "Spelling errors",
      "Learner corpora",
      "Détection d'erreurs",
      "Annotation",
      "Orthographe",
      "Corpus scolaire"
    ],
    "abstract_s": [
      "L'intérêt pour l'étude des corpus scolaires, tout en étant grandissant, se heurte à la taille de ces corpus et donc à la difficulté d'une analyse entièrement manuelle. Utiliser des méthodes empruntées au traitement automatique des langues (TAL) pourrait aider à l'exploitation de ces corpus. Cela représente cependant un défi pour le TAL du fait de l'éloignement de ces corpus à la norme. L'objectif de notre travail est d'adapter certaines techniques du TAL, éprouvées par ailleurs, afin de faciliter la constitution et l'exploitation d'un corpus recueilli en classe de CP. L'enjeu est donc double. Il s'agit à la fois de proposer une première définition d'un outil répondant aux besoins de la recherche en linguistique et en didactique. Mais il s'agit également, pour le TAL, de caractériser et de modéliser un type d'écrit distant de la norme. Nous proposerons dans ce mémoire un premier schéma d'annotation d'erreurs et des pistes pour l'analyse automatique de ce type de corpus.",
      "Whereas interest for learner has corpora increased, this research deals with the size of those corpora. Difficulties exist from manual treatments. Therefore we propose to use NLP (Natural Language Processing) methods to help exploit those corpora. This represents a challenge for NLP due to numerous errors from the age level. Our work aims to adapt some verified methods from NLP to build and exploit a first grade elementary school corpus. Our project has two goals in mind. First we hope to construct a framework which can deal with needs in didactic's and linguistic's research. And secondly we aim to model this particular writing type which is far from standard spelling. In this master's thesis we will present a proposition of annotation schema and suggestions for future research."
    ],
    "authFullName_s": [
      "Claire Wolfarth"
    ],
    "halId_s": "dumas-01167286",
    "producedDateY_i": 2015,
    "texte_nettoye": "L'intérêt pour l'étude des corpus scolaires, tout en étant grandissant, se heurte à la taille de ces corpus et donc à la difficulté d'une analyse entièrement manuelle. Utiliser des méthodes empruntées au traitement automatique des langues (TAL) pourrait aider à l'exploitation de ces corpus. Cela représente cependant un défi pour le TAL du fait de l'éloignement de ces corpus à la norme. L'objectif de notre travail est d'adapter certaines techniques du TAL, éprouvées par ailleurs, afin de faciliter la constitution et l'exploitation d'un corpus recueilli en classe de CP. L'enjeu est donc double. Il s'agit à la fois de proposer une première définition d'un outil répondant aux besoins de la recherche en linguistique et en didactique. Mais il s'agit également, pour le TAL, de caractériser et de modéliser un type d'écrit distant de la norme. Nous proposerons dans ce mémoire un premier schéma d'annotation d'erreurs et des pistes pour l'analyse automatique de ce type de corpus."
  },
  {
    "title_s": [
      "Étude de faisabilité de mise en place d'une indexation semi-automatique avec un thésaurus spécialisé en archéologie"
    ],
    "keyword_s": [
      "Langage naturel",
      "Indexation semi-automatique",
      "TAL traitement automatique du langage",
      "Thésaurus",
      "Articles scientifiques",
      "Archéologie"
    ],
    "abstract_s": [
      "Le réseau de bibliothèques et de laboratoires FRANTIQ a entre autres, comme but de fédérer les documents d'archéologie et de sciences de l'Antiquité (papier ou électronique) dans un catalogue commun indexé afin de les retrouver plus facilement. Chaque document fait l'objet d'une notice et d'une indexation à l'aide du thésaurus spécialisé en archéologie PACTOLS. Or le nombre de documents ne cesse d'accroître et l'indexation manuelle est une tâche très coûteuse en temps/homme. De plus, en archéologie, les informations sont cumulatives, c'est-à-dire que les informations ne deviennent jamais obsolètes mais s'ajoutent au fil du temps et les documents sont des ressources importantes pour le travail des chercheurs et des étudiants. En 2009, pour alléger le travail d'indexation des documentalistes et bibliothécaires et étendre la couverture du catalogue, les premiers essais d'indexation semi-automatique à l'aide du thésaurus PACTOLS ont été réalisés par l'informaticien du réseau FRANTIQ aidé de stagiaires. Le résultat n'étant pas très concluant, notamment à cause d'ambiguïtés dans les mots-clés proposés par l'algorithme, et par manque de temps et de moyens pour améliorer les résultats, ce projet a été temporairement abandonné. Ce mémoire a pour but de reprendre ce projet et d'analyser les méthodes modernes et anciennes de l'indexation automatique et semi-automatique afin d'améliorer les résultats et son taux d'acceptation par les indexeurs en poste. Deux systèmes ont étés sélectionnés afin d'en comparer les résultats : un système à base de règles crée avec python et natural language toolkit (NLTK), et un système d'apprentissage basé sur KEA++."
    ],
    "authFullName_s": [
      "Anita Mazur"
    ],
    "halId_s": "mem_00737359",
    "producedDateY_i": 2012,
    "texte_nettoye": "Le réseau de bibliothèques et de laboratoires FRANTIQ a entre autres, comme but de fédérer les documents d'archéologie et de sciences de l'Antiquité (papier ou électronique) dans un catalogue commun indexé afin de les retrouver plus facilement. Chaque document fait l'objet d'une notice et d'une indexation à l'aide du thésaurus spécialisé en archéologie PACTOLS. Or le nombre de documents ne cesse d'accroître et l'indexation manuelle est une tâche très coûteuse en temps/homme. De plus, en archéologie, les informations sont cumulatives, c'est-à-dire que les informations ne deviennent jamais obsolètes mais s'ajoutent au fil du temps et les documents sont des ressources importantes pour le travail des chercheurs et des étudiants. En 2009, pour alléger le travail d'indexation des documentalistes et bibliothécaires et étendre la couverture du catalogue, les premiers essais d'indexation semi-automatique à l'aide du thésaurus PACTOLS ont été réalisés par l'informaticien du réseau FRANTIQ aidé de stagiaires. Le résultat n'étant pas très concluant, notamment à cause d'ambiguïtés dans les"
  },
  {
    "title_s": [
      "TALN, Text-Mining et ontologie pour la maintenance de panneaux photovoltaïques"
    ],
    "keyword_s": [
      "Text-Mining",
      "Ontology",
      "Maintenance",
      "Solar parks",
      "NLP",
      "Parcs solaires",
      "Fouille de texte",
      "TAL",
      "Maintenance",
      "Ontologies"
    ],
    "abstract_s": [
      "L’abondance de données qui circulent dans le monde, conséquence de l’avènement du numérique, s’accompagne de questionnements quant aux usages qui peuvent être fait de ces données. C’est ce besoin qui a conduit EDF à lancer des projets de recherches afin d’exploiter les données à sa disposition. Parmi ces projets, l’un se penche sur les données textuelles rendant compte des opérations de maintenance effectuées dans les parcs électriques solaires. Le travail réalisé dans ce mémoire a été accompli dans le cadre de ce projet. L’objectif est d’exploiter les données textuelles et d’en extraire des informations afin d’optimiser le processus de maintenance dans les parcs photovoltaïques. La problématique est la suivante : Comment le traitement automatique des langues peut-il permettre d’optimiser le processus de conception, d’exploitation et de maintenance d’installation photovoltaïques ? Pour répondre à cette problématique plusieurs actions ont été menées afin d’adapter une application d’analyse textuelle au domaine solaire. L’application repose sur une ontologie et des règles. Les principales actions décrites dans ce mémoire ont pour but de mettre en forme les données de départ et d’aboutir à l’enrichissement de l’ontologie afin que l’application soit compatible avec le traitement de données textuelles provenant de comptes rendus de maintenance de panneaux solaire.",
      "The very large amount of circulating data, around the world is related to the rise of the digital. Many questions stand out as to the uses that can be made of these data. As a resort EDF aims to launch research projects to make use of the data at his disposal. Among these projects, one looks at textual data. The analysis conducted in this thesis was established as part of this project. The aim is to use textual data and extract information to optimize the maintenance process in photovoltaic parks. The issue is: How can natural language processing optimize the process of designing, operating and maintaining photovoltaic systems? To answer this, several actions have been carried out in order to adapt a textual analysis application to the solar domain. The application lays on an on an ontology and rules. The main actions described in this thesis are intended to format the initial data in order to extend the ontology, so that the application is compatible with the processing of textual data, from maintenance reports of solar panels."
    ],
    "authFullName_s": [
      "Sami Bouhouche"
    ],
    "halId_s": "dumas-02322096",
    "producedDateY_i": 2019,
    "texte_nettoye": "L’abondance de données qui circulent dans le monde, conséquence de l’avènement du numérique, s’accompagne de questionnements quant aux usages qui peuvent être fait de ces données. C’est ce besoin qui a conduit EDF à lancer des projets de recherches afin d’exploiter les données à sa disposition. Parmi ces projets, l’un se penche sur les données textuelles rendant compte des opérations de maintenance effectuées dans les parcs électriques solaires. Le travail réalisé dans ce mémoire a été accompli dans le cadre de ce projet. L’objectif est d’exploiter les données textuelles et d’en extraire des informations afin d’optimiser le processus de maintenance dans les parcs photovoltaïques. La problématique est la suivante : Comment le traitement automatique des langues peut-il permettre d’optimiser le processus de conception, d’exploitation et de maintenance d’installation photovoltaïques ? Pour répondre à cette problématique plusieurs actions ont été menées afin d’adapter une application d’analyse textuelle au domaine solaire. L’application repose sur une ontologie et des règles. Les principales actions décrites dans ce mémoire ont pour but de mettre en forme les données de départ et d’aboutir à l’enrichissement de l’ontologie afin que l’application soit compatible avec le traitement de données textuelles provenant de comptes rendus de maintenance de panneaux solaire."
  },
  {
    "title_s": [
      "Modélisation auto-supervisée de la parole affective spontanée"
    ],
    "keyword_s": [
      "Self-Supervised Representation Learning",
      "Speech Emotion Recognition",
      "Apprentissage des représentations",
      "Représentation auto-supervisée",
      "Apprentissage auto-supervisé",
      "Reconnaissance des émotions de la parole"
    ],
    "abstract_s": [
      "Les modèles auto-supervisés pré-entraîné utilisant des données non étiquetées pour extraire des représentations ont été largement exploré dans le domaine du traitement automatique de la parole. Ce mémoire explore une nouvelle approche consistant à extraire des représentations linguistiques des transcriptions à partir des modèles auto-supervisés pré-entraîné pour la reconnaissance des émotions de la parole spontanée en temps continu. Nous avons examiné une méthode d’alignement pour accorder les représentations linguistiques avec du temps. Les résultats des expérimentations montrent que les représentations auto-supervisées linguistiques peuvent prédire les émotions en dimension arousal et valence aussi bien que les représentations auto-supervisées acoustiques.",
      "Self-supervised pre-trained models using unlabeled data to extract representations have been widely explored in the field of automatic speech processing. This paper explores a new approach of extracting linguistic representations from transcriptions using pre-trained self-supervised models for time-continuous emotion recognition. We examined an alignment method to fit linguistic representations with time. Experimental results show that linguistic self-supervised representations can predict emotions in arousal and valence dimensions and achieve excellent results as acoustic self-supervised representations."
    ],
    "authFullName_s": [
      "Ziyi Tong"
    ],
    "halId_s": "dumas-03516512",
    "producedDateY_i": 2022,
    "texte_nettoye": "Les modèles auto-supervisés pré-entraîné utilisant des données non étiquetées pour extraire des représentations ont été largement exploré dans le domaine du traitement automatique de la parole. Ce mémoire explore une nouvelle approche consistant à extraire des représentations linguistiques des transcriptions à partir des modèles auto-supervisés pré-entraîné pour la reconnaissance des émotions de la parole spontanée en temps continu. Nous avons examiné une méthode d’alignement pour accorder les représentations linguistiques avec du temps. Les résultats des expérimentations montrent que les représentations auto-supervisées linguistiques peuvent prédire les émotions en dimension arousal et valence aussi bien que les représentations auto-supervisées acoustiques."
  },
  {
    "title_s": [
      "Développement phonologique typique français d'enfants multilingues de CP"
    ],
    "keyword_s": [
      "Facteurs extralinguistiques",
      "Facteurs linguistiques",
      "Traitement de la parole",
      "Développement phonologique",
      "Multilinguisme"
    ],
    "abstract_s": [
      "Le projet EULALIES concerne l’évaluation et le dépistage des troubles du développement des sons de la parole. Les objectifs de ce projet sont de créer un outil d’évaluation du développement phonologique en français hexagonal, puis de récolter des données de référence, et enfin de recueillir des données sur les troubles du développement des sons dela parole pour identifier des marqueurs cliniques spécifiques au français. Ce mémoire a été réalisé dans le cadre de ce projet, dans le but de recueillir des données permettant de caractériser la variabilité du développement phonologique typique chez des enfants francophones multilingues de CP. Nous proposons tout d’abord un ancrage théorique autour des études déjà menées sur le développement phonologique multilingue. Grâce à ces études, nous avons ensuite mis en évidence plusieurs facteurs qui ont une influence sur le développement phonologique multilingue. Puis, nous détaillons la méthodologie utilisée dans le cadre du projet EULALIES, avant de présenter et discuter les résultats analysés en fonction de l’âge d’acquisition des langues, des langues parlées et du statut socioéconomique."
    ],
    "authFullName_s": [
      "Clarisse Puissant"
    ],
    "halId_s": "dumas-02949296",
    "producedDateY_i": 2020,
    "texte_nettoye": "Le projet EULALIES concerne l’évaluation et le dépistage des troubles du développement des sons de la parole. Les objectifs de ce projet sont de créer un outil d’évaluation du développement phonologique en français hexagonal, puis de récolter des données de référence, et enfin de recueillir des données sur les troubles du développement des sons dela parole pour identifier des marqueurs cliniques spécifiques au français. Ce mémoire a été réalisé dans le cadre de ce projet, dans le but de recueillir des données permettant de caractériser la variabilité du développement phonologique typique chez des enfants francophones multilingues de CP. Nous proposons tout d’abord un ancrage théorique autour des études déjà menées sur le développement phonologique multilingue. Grâce à ces études, nous avons ensuite mis en évidence plusieurs facteurs qui ont une influence sur le développement phonologique multilingue. Puis, nous détaillons la méthodologie utilisée dans le cadre du projet EULALIES, avant de présenter et discuter les résultats analysés en fonction de l’âge d’acquisition des langues, des langues parlées et du statut socioéconomique."
  },
  {
    "title_s": [
      "L'escape game comme outil d'expérience : un essai dans le domaine de la robotique sociale"
    ],
    "keyword_s": [
      "Rob’air",
      "Escape game",
      "Robotics",
      "Social navigation",
      "Navigation sociale",
      "Rob’air",
      "Escape game",
      "Robotique"
    ],
    "abstract_s": [
      "Nous avons conçu un escape game qui permettra d’observer les possibles changements de distance sociale entre des personnes utilisant un robot de téléprésence, rob’air, pour communiquer. Deux joueurs et/ou joueuses auront pour but de récupérer des objets et documents tout en résolvant des énigmes dans un espace de jeu délimité : l’appartement intelligent et le plateau d’expérimentation de DOMUS, ainsi qu’une fausse salle de contrôle. L’escape game a été testé dans les anciens locaux de DOMUS et est prêt à être installé à la MACI, où DOMUS doit s’installer en septembre 2019. Les autres missions de notre stage sont aussi décrites dans ce rapport, à savoir : notre participation à la réalisation d’une expérience sur l’effet Lombard et des recherches sur l’enregistrement de sons émis par des animaux et leur perception par les humains.",
      "We created an escape game to test the potential changes of social distance between interlocutors using the telepresence robot Rob’Air to communicate. Two players will have to gather objects and papers while solving riddles. The game will take place in the DOMUS apartment, the DOMUS experimentation platform, and a false control room. The game was tested in the CTL building and is ready to be set up at MACI, where DOMUS will settle in September 2019. We also described in this document the two other missions we had during our internship: our involvement in an experimentation on the Lombard effect and our research on recording sounds emitted by animals and their perception by humans."
    ],
    "authFullName_s": [
      "Émeline Le Goff",
      "Zoé Giorgis"
    ],
    "halId_s": "dumas-02302102",
    "producedDateY_i": 2019,
    "texte_nettoye": "Nous avons conçu un escape game qui permettra d’observer les possibles changements de distance sociale entre des personnes utilisant un robot de téléprésence, rob’air, pour communiquer. Deux joueurs et/ou joueuses auront pour but de récupérer des objets et documents tout en résolvant des énigmes dans un espace de jeu délimité : l’appartement intelligent et le plateau d’expérimentation de DOMUS, ainsi qu’une fausse salle de contrôle. L’escape game a été testé dans les anciens locaux de DOMUS et est prêt à être installé à la MACI, où DOMUS doit s’installer en septembre 2019. Les autres missions de notre stage sont aussi décrites dans ce rapport, à savoir : notre participation à la réalisation d’une expérience sur l’effet Lombard et des recherches sur l’enregistrement de sons émis par des animaux et leur perception par les humains."
  },
  {
    "title_s": [
      "Extraction d'informations en contexte de Petites et Moyennes Entreprises"
    ],
    "keyword_s": [
      "Internship report",
      "Tools selection",
      "Information extraction",
      "Extractions d'informations",
      "Perl",
      "Rapport de stage",
      "Sélection d'outils"
    ],
    "abstract_s": [
      "Le Traitement Automatique du Langage (Naturel) est une discipline jeune qui a vu le jour dans les années 60. Elle est jeune également dans sa représentation en entreprise. L'ensemble de ces caractères définissent sans doute des schémas d'intervention au sein de ces sociétés dont l'expérience au sein de la société N5 peut être un reflet. Ces services sont pour partie prévisibles. Ce sont tout d'abord toutes les missions à visée informative qui permettent à l'entreprise de réaliser son positionnement logiciel voire matériel, au regard de ses besoins. Ces missions d'information sont en effet immanquablement suivies d'une phase de sélection d'outils. Les étapes à suivre voient la mise en place et l'expérimentation des dits outils ou au contraire la commande de missions plus légères, telle la réalisation de missions d'extraction. Ce sont des missions légères au point de vue des ressources engagées, puisqu'elles ne requièrent pas d'outils spécialement coûteux. Toute la spécificité de cette phase réside dans la particularité des besoins à satisfaire. Bien que là encore, des motifs récurrents s'observent. Les outils de base du linguiste n'étant pas toujours les mêmes, le vocabulaire du langage ?",
      "Automatic (Natural) Language Processing is a new discipline, born in the 60's. the discipline is new too in companies' practice. One can presume how ANLP can work for businesses. What happened in N5 is somehow significant of what kinds of interventions are required by small and medium-sized enterprises. These actions are quite predictable. First they are informative missions that define these companies' software and material environment. Selecting tools and trying them is often a natural end in this stage but are not always achieved. Next missions command extraction services. These last activities are financially light: they don't require a lot of money, no more expensive materials. The enterprise's needs made this stage specificity. Although, even here, there are some regular patterns because the linguist's bricks are still the same lowest level: (human) vocabulary."
    ],
    "authFullName_s": [
      "Mylène Joséphine"
    ],
    "halId_s": "dumas-00865992",
    "producedDateY_i": 2012,
    "texte_nettoye": "Le Traitement Automatique du Langage (Naturel) est une discipline jeune qui a vu le jour dans les années 60. Elle est jeune également dans sa représentation en entreprise. L'ensemble de ces caractères définissent sans doute des schémas d'intervention au sein de ces sociétés dont l'expérience au sein de la société N5 peut être un reflet. Ces services sont pour partie prévisibles. Ce sont tout d'abord toutes les missions à visée informative qui permettent à l'entreprise de réaliser son positionnement logiciel voire matériel, au regard de ses besoins. Ces missions d'information sont en effet immanquablement suivies d'une phase de sélection d'outils. Les étapes à suivre voient la mise en place et l'expérimentation des dits outils ou au contraire la commande de missions plus légères, telle la réalisation de missions d'extraction. Ce sont des missions légères au point de vue des ressources engagées, puisqu'elles ne requièrent pas d'outils spécialement coûteux. Toute la spécificité de cette phase réside dans la particularité des besoins à satisfaire. Bien que là encore, des motifs récurrents s'observent. Les outils de base du linguiste n'étant pas toujours les mêmes, le vocabulaire du langage ?"
  },
  {
    "title_s": [
      "Normalisation automatique de variables issues de bases de données en agroécologie"
    ],
    "keyword_s": [
      "Distance de Levenshtein",
      "Développement de systèmes agricoles durables",
      "Données agricoles",
      "Corpus",
      "Canonisation",
      "Cosinus",
      "Similarité",
      "BERT",
      "TF-IDF",
      "Ontologies",
      "Traitement automatique du langage naturel",
      "Correspondance",
      "Agroécologie"
    ],
    "abstract_s": [
      "Ce rapport de stage présente une étude réalisée au sein de l’UMR TETIS, située à la Maison De la Télédétection sur le campus Agropolis de Montpellier, en collaboration avec l’UR AIDA. Le stage s’est focalisé sur l’importance de la correspondance des variables sources et candidates en agroécologie. L’objectif principal de ce stage était de résoudre la problématique liée à l’hétérogénéité des variables utilisées par les chercheurs en agroécologie. Cependant, chaque chercheur a sa propre méthode de nomination et de description des variables sources, ce qui rend la correspondance complexe et sujette à des erreurs. Pour aborder cette problématique, différentes méthodes de représentation des données textuelles ont été explorées, telles que TF-IDF [1] et des approches basées sur des modèles de langues tels que BERT-base (section 3.3.2), BERT-large (section 3.3.2), RoBERTa (section 3.3.4) et XLNet (section 3.3.3), pour la vectorisation des noms et des descriptions des variables. Des mesures de similarité, telles que la distance de Levenshtein [2] et le cosinus [3], ont été appliquées pour évaluer la proximité entre les variables. Les résultats obtenus ont démontré des améliorations significatives par rapport aux approches précédentes [5]. Cependant, certaines limites ont été identifiées, notamment le nombre limité de variables en anglais, la formulation non canonique des variables, les descriptions courtes et l’absence de prise en compte des ontologies associées. Des recommandations ont été formulées pour surmonter ces limites, telles que la traduction des variables dans la même langue que les ontologies, la canonisation des variables non canoniques, l’extension du corpus avec des Données multilingues et hétérogènes, et l’utilisation de Méthodes de plongement de mots et de mesure de similarité. Ce rapport met en évidence l’importance de la correspondance des variables en agroécologie. Les résultats obtenus offrent de nouvelles perspectives pour une meilleure utilisation et compréhension des données agricoles."
    ],
    "authFullName_s": [
      "Oussama Mechhour"
    ],
    "halId_s": "hal-05326124",
    "producedDateY_i": 2023,
    "texte_nettoye": "Ce rapport de stage présente une étude réalisée au sein de l’UMR TETIS, située à la Maison De la Télédétection sur le campus Agropolis de Montpellier, en collaboration avec l’UR AIDA. Le stage s’est focalisé sur l’importance de la correspondance des variables sources et candidates en agroécologie. L’objectif principal de ce stage était de résoudre la problématique liée à l’hétérogénéité des variables utilisées par les chercheurs en agroécologie. Cependant, chaque chercheur a sa propre méthode de nomination et de description des variables sources, ce qui rend la correspondance complexe et sujette à des erreurs. Pour aborder cette problématique, différentes méthodes de représentation des données textuelles ont été explorées, telles que TF-IDF [1] et des approches basées sur des modèles de langues tels que BERT-base (section 3.3.2), BERT-large (section 3.3.2), RoBERTa (section 3.3.4) et XLNet (section 3.3.3), pour la vectorisation des noms et des descriptions des variables. Des mesures de similarité, telles que la distance de Levenshtein [2] et le cosinus [3], ont été appliquées pour évaluer la proximité entre les variables. Les résultats obtenus ont démontré des améliorations significatives par rapport aux approches précédentes [5]. Cependant, certaines limites ont été identifiées, notamment le nombre limité de variables en anglais, la formulation non canonique des variables, les descriptions courtes et l’absence de prise en compte des ontologies associées. Des recommandations ont été formulées pour surmonter ces limites, telles que la traduction des variables dans la même langue que les ontologies, la canonisation des variables non canoniques, l’extension du corpus avec des Données multilingues et hétérogènes, et l’utilisation de Méthodes de plongement de mots et de mesure de similarité. Ce rapport met en évidence l’importance de la correspondance des variables en agroécologie. Les résultats obtenus offrent de nouvelles perspectives pour une meilleure utilisation et compréhension des données agricoles."
  },
  {
    "title_s": [
      "Constitution et évaluation d'un jeu de données linguistiques en français pour l'analyse des fonctions lexicales encodées dans les modèles neuronaux de type FlauBERT"
    ],
    "keyword_s": [
      "FlauBERT",
      "Polysemy",
      "Contextual vectors",
      "Word embeddings",
      "Vecteurs contextuels",
      "Plongement de mots",
      "FlauBERT",
      "Polysémie"
    ],
    "abstract_s": [
      "Chaque langue est constituée de mots qui lui sont propres. Dans la plupart des cas, ceux-ci sont polysémiques - ils possèdent plusieurs sens. La modélisation de la polysémie en Traitement Automatique de la Langue est une tâche difficile lorsqu’il s’agit de vecteurs de mots ; les systèmes de plongements de mots traditionnels ont certaines difficultés à traiter la polysémie. À l’aide de FlauBERT, qui est un nouveau modèle de langue développé en 2019, nous verrons qu’il est maintenant plus facile de traiter de la polysémie, notamment grâce à des vecteurs de mots contextualisés. Le contexte entier d’une phrase est pris en compte par FlauBERT afin de représenter chaque mot sous forme de vecteur. Après une brève analyse des différents domaines en jeu, je présenterai dans ce mémoire les différentes expérimentations que j’ai effectuées à l’aide des vecteurs de mots du système FlauBERT.",
      "Each language is made up of its own words. In most cases, these are polysemic, they have several meanings. Modeling polysemy in Automatic Language Processing is a difficult task when it comes to word vectors and traditional word embeddings systems have some difficulties in dealing with polysemy. Using FlauBERT, which is a new language model developed in 2019, we will see that it is now easier to deal with polysemy, especially with contextualized word vectors. The entire context of a sentence is taken into account by FlauBERT in order to represent each word as a vector. After a brief analysis of the different domains involved, I will present in this paper the different experiments I have performed using FlauBERT word vectors."
    ],
    "authFullName_s": [
      "Vincent Bellue"
    ],
    "halId_s": "dumas-02978401",
    "producedDateY_i": 2020,
    "texte_nettoye": "Chaque langue est constituée de mots qui lui sont propres. Dans la plupart des cas, ceux-ci sont polysémiques - ils possèdent plusieurs sens. La modélisation de la polysémie en Traitement Automatique de la Langue est une tâche difficile lorsqu’il s’agit de vecteurs de mots ; les systèmes de plongements de mots traditionnels ont certaines difficultés à traiter la polysémie. À l’aide de FlauBERT, qui est un nouveau modèle de langue développé en 2019, nous verrons qu’il est maintenant plus facile de traiter de la polysémie, notamment grâce à des vecteurs de mots contextualisés. Le contexte entier d’une phrase est pris en compte par FlauBERT afin de représenter chaque mot sous forme de vecteur. Après une brève analyse des différents domaines en jeu, je présenterai dans ce mémoire les différentes expérimentations que j’ai effectuées à l’aide des vecteurs de mots du système FlauBERT."
  },
  {
    "title_s": [
      "Modélisation d'une ontologie de domaine et des outils d'extraction de l'information associés pour l'anglais et le français"
    ],
    "keyword_s": [
      "Ontology",
      "Named entities",
      "Text mining",
      "Information extraction",
      "Natural Language Processing NLP",
      "Entités nommées",
      "Ontologie",
      "Fouille de textes",
      "Extraction d'information",
      "Traitement Automatique de la Langue TAL"
    ],
    "abstract_s": [
      "Aujourd'hui, l'abondance des sources d'information publiques (sites internet, presse, radio, télévision, etc.) a fait émergé le besoin de \" fouiller \" cette masse de documents afin d'en extraire des connaissances pertinentes dans un but donné. L'équipe IPCC, au sein d'EADS Defence & Security, est chargée de l'innovation en matière de traitement de l'information. Ce mémoire présente une ontologie de domaine et les outils d'extraction de l'information associés pour l'anglais et le français. Après une brève analyse des outils et techniques existants en modélisation d'ontologie et extraction d'information, nous présentons les différents travaux réalisés durant notre stage. Nous avons modélisé, grâce au logiciel Protégé, une petite ontologie de domaine au format OWL, dédiée au renseignement militaire. Afin de repérer dans un texte les différents éléments d'intérêt, nous avons développé, grâce à l'environnement GATE, un outil d'extraction d'entités nommées, évènements et relations. Nous détaillons ici la méthode choisie, les étapes de réalisation ainsi que l'évaluation quantitative et qualitative des résultats obtenus.",
      "Nowadays, the increasing of information sources (websites, newspapers, radio, TV, etc.) has led to \"dig\" these documents in order to extract relevant knowledge considering a set purpose. The IPCC team at EADS Defence & Security is responsible for invention in media mining. This thesis introduces a domain ontology and the associated tools for information extraction in English and French texts. After a brief analysis of existing tools and techniques in ontology development and information extraction, we present the work done during our training course. First, we used the Protégé tool to create a small OWL domain ontology dedicated to military intelligence. In order to recognize the elements of interest, we have built, through GATE architecture, a system to extract named entities, events and relations. We present here our methodology, the different stages of implementation as well as a quantitative and qualitative evaluation of our results."
    ],
    "authFullName_s": [
      "Laurie Serrano"
    ],
    "halId_s": "dumas-00569002",
    "producedDateY_i": 2010,
    "texte_nettoye": "Aujourd'hui, l'abondance des sources d'information publiques (sites internet, presse, radio, télévision, etc.) a fait émergé le besoin de \" fouiller \" cette masse de documents afin d'en extraire des connaissances pertinentes dans un but donné. L'équipe IPCC, au sein d'EADS Defence & Security, est chargée de l'innovation en matière de traitement de l'information. Ce mémoire présente une ontologie de domaine et les outils d'extraction de l'information associés pour l'anglais et le français. Après une brève analyse des outils et techniques existants en modélisation d'ontologie et extraction d'information, nous présentons les différents travaux réalisés durant notre stage. Nous avons modélisé, grâce au logiciel Protégé, une petite ontologie de domaine au format OWL, dédiée au renseignement militaire. Afin de repérer dans un texte les différents éléments d'intérêt, nous avons développé, grâce à l'environnement GATE, un outil d'extraction d'entités nommées, évènements et relations. Nous détaillons ici la méthode choisie, les étapes de réalisation ainsi que l'évaluation quantitative et qualitative des résultats obtenus."
  },
  {
    "title_s": [
      "Extraction d'évènements au sein d'une plateforme de veille"
    ],
    "keyword_s": [
      "Web Intelligence",
      "Market Intelligence",
      "Event Extraction",
      "Text Mining",
      "Information Extraction",
      "Veille sur internet",
      "Market Intelligence",
      "Fouille de textes",
      "Extraction d’information",
      "Extraction d’évènements"
    ],
    "abstract_s": [
      "L’extraction d’évènements est une application du Traitement Automatique des Langues (TAL), et plus précisément une application de fouille de textes, qui consiste à extraire de manière structurée des informations sur des évènements présents de manière non-structurée dans des textes. Lors de ce stage, nous avons travaillé à l’élaboration d’une maquette d’un outil d’extraction d’évènements qui repose sur une méthode de reconnaissance de motifs. Nous avons principalement travaillé avec le corpus ACE (Automatic Content Extraction) issu de la campagne d’évaluation du même nom. Ce stage s’inscrit dans le cadre de l’enrichissement continue des fonctionnalités d’AMI Enterprise Intelligence, la solution de veille stratégique développée et maintenue dans le centre R&D de la société Bertin IT à Montpellier. Les principales contributions de ce mémoire sont : — état de l’art sur l’extraction d’évènements ; — étude de plusieurs outils d’extraction d’évènements disponibles en OpenSource ; — développement d’une maquette d’un outil d’extraction d’évènements.",
      "Event extraction is an application of Natural Language Processing (NLP), more precisely of text mining, consisting in extracting structured information about events present in texts in an unstructured way. During the course of this internship, we worked on the development of the model version of an event extraction tool. This tool is based on a pattern-matching method. We mainly worked with the ACE (Automatic Content Extraction) corpus, from the evaluation conference of the same name. This internship and the developement of this model are part of enhancing AMI Enterprise Intelligence (AMI EI), the business intelligence solution developed and maintained by Bertin IT within its research center in Montpellier. The main contributions of this project are : — state of the art on event extraction ; — review of several Open Source tools ; — development of a model version of an event extraction tool."
    ],
    "authFullName_s": [
      "Capucine Antoine"
    ],
    "halId_s": "dumas-03245021",
    "producedDateY_i": 2020,
    "texte_nettoye": "L’extraction d’évènements est une application du Traitement Automatique des Langues (TAL), et plus précisément une application de fouille de textes, qui consiste à extraire de manière structurée des informations sur des évènements présents de manière non-structurée dans des textes. Lors de ce stage, nous avons travaillé à l’élaboration d’une maquette d’un outil d’extraction d’évènements qui repose sur une méthode de reconnaissance de motifs. Nous avons principalement travaillé avec le corpus ACE (Automatic Content Extraction) issu de la campagne d’évaluation du même nom. Ce stage s’inscrit dans le cadre de l’enrichissement continue des fonctionnalités d’AMI Enterprise Intelligence, la solution de veille stratégique développée et maintenue dans le centre R&D de la société Bertin IT à Montpellier. Les principales contributions de ce mémoire sont : — état de l’art sur l’extraction d’évènements ; — étude de plusieurs outils d’extraction d’évènements disponibles en OpenSource ; — développement d’une maquette d’un outil d’extraction d’évènements."
  },
  {
    "title_s": [
      "L'analyse des adjectifs axiologiques dans les ouvrages touristiques sur la Thaïlande"
    ],
    "keyword_s": [
      "Axiological adjective",
      "Semantic",
      "Tourism discourse",
      "Adjectif axiologique",
      "Corpus",
      "Discours touristiques",
      "Sémantique"
    ],
    "abstract_s": [
      "S’appuyant sur le corpus de 432546 mots, des adjectifs axiologiques dans les discours touristiques servent de support de notre travail fondé sur l’analyse sémantique. Nous nous interrogeons sur la difficulté à catégoriser et à identifier la valeur axiologique ainsi qu’à la polarité des adjectifs axiologiques. La désambiguïsation et l’annotation sont réalisées ainsi en reposant sur la théorie taxonomique de l’Appraisal, à l’aide de quelques outils du traitement automatique des langues, pour parvenir à atteindre l’objectif de cette étude. En réalité, l’objectif final de ce travail s’oriente vers une élaboration de la ressource lexicale des adjectifs axiologiques dans le contexte touristique. Un autre objectif porte sur une analyse des stéréotypes de pensée. Se basant sur la culture et la tradition de chaque communauté, certains adjectifs sont jugés différemment.",
      "The current study based on a 432,546 words corpus provides a semantic analysis of axiological adjectives in tourism discourse. Here, we consider one of the difficulties in categorising and identifying an axiological value and a polarity of axiological adjectives. To achieve the objective of this study, disambiguation and annotation are accomplished relying on the taxonomical theory of Appraisal, by use of tools for Automatic Treatment of Language. In fact, the final expectation of this research is to expand on a lexical resource of axiological adjectives in the context of tourism. Another objective is reached on the analysis of the stereotypic thinking. Certain adjectives are judged differently depending on the culture and tradition of each community."
    ],
    "authFullName_s": [
      "Jitwongnan Jarukan"
    ],
    "halId_s": "dumas-01084118",
    "producedDateY_i": 2014,
    "texte_nettoye": "S’appuyant sur le corpus de 432546 mots, des adjectifs axiologiques dans les discours touristiques servent de support de notre travail fondé sur l’analyse sémantique. Nous nous interrogeons sur la difficulté à catégoriser et à identifier la valeur axiologique ainsi qu’à la polarité des adjectifs axiologiques. La désambiguïsation et l’annotation sont réalisées ainsi en reposant sur la théorie taxonomique de l’Appraisal, à l’aide de quelques outils du traitement automatique des langues, pour parvenir à atteindre l’objectif de cette étude. En réalité, l’objectif final de ce travail s’oriente vers une élaboration de la ressource lexicale des adjectifs axiologiques dans le contexte touristique. Un autre objectif porte sur une analyse des stéréotypes de pensée. Se basant sur la culture et la tradition de chaque communauté, certains adjectifs sont jugés différemment."
  },
  {
    "title_s": [
      "L’intelligence artificielle au service des géomètres-experts : optimiser le contrôle et la rédaction documentaire en copropriété"
    ],
    "keyword_s": [
      "Data structuring",
      "Land Surveying",
      "Master's thesis",
      "Normative phrasing",
      "Natural language processing",
      "Fine-tuning",
      "Artificial intelligence",
      "AI",
      "Co-ownership",
      "Descriptive Statement of Division",
      "DSD",
      "Surveyor-Expert",
      "Automated drafting",
      "Property document",
      "Language model",
      "Supervised learning",
      "Corpus-based optimization",
      "Text generation",
      "EDD",
      "Master Foncier",
      "ESGT",
      "Mémoire de Fin d’Études",
      "Formulation normative",
      "Structuration des données",
      "Traitement automatique du langage Naturel",
      "Génération de texte",
      "Optimisation-fine",
      "Optimisation sur corpus",
      "Apprentissage supervisé",
      "Modèle de langage",
      "Document foncier",
      "Automatisation de la rédaction",
      "Géomètre-Expert",
      "État Descriptif de Division",
      "Copropriété",
      "IA",
      "Intelligence Artificielle"
    ],
    "abstract_s": [
      "L’intégration de l’intelligence artificielle dans le domaine de la copropriété ouvre la voie à une amélioration significative des processus de rédaction et de vérification documentaire. L’État Descriptif de Division (EDD), document structurant par excellence, se prête particulièrement bien à ce type d’approche. En mobilisant des modèles de langage optimisés sur des exemples structurés, il devient possible de générer et contrôler des textes normés, cohérents et fidèles aux attentes professionnelles. Cette démarche permet non seulement de réduire les temps de traitement, mais aussi de sécuriser juridiquement la production grâce à une restitution systématisée des formulations attendues. Elle marque ainsi l’émergence d’une nouvelle manière de produire les documents techniques, plus rapide, plus fiable, et mieux adaptée aux exigences du terrain. À travers cette étude, l’IA apparaît comme un outil opérationnel au service des géomètres-experts, capable de renforcer la qualité, la constance et l’efficience dans la gestion des dossiers de copropriété.",
      "The integration of artificial intelligence into the field of co-ownership paves the way for a significant improvement in the processes of drafting and verifying technical documents. The Descriptive Statement of Division (DSD), a cornerstone document, is particularly well suited to this type of approach. By leveraging language models optimized on structured examples, it becomes possible to generate and validate standardized texts that are coherent and aligned with professional expectations. This method not only reduces processing time but also enhances legal reliability through the systematic reproduction of expected legal phrasing. It thus signals the emergence of a new way of producing technical documents—faster, more reliable, and better suited to on-site requirements. Through this study, AI emerges as a practical tool for land surveyors, capable of strengthening quality, consistency, and efficiency in the management of co-ownership cases."
    ],
    "authFullName_s": [
      "Maxime Mayet-Foulgoc"
    ],
    "halId_s": "dumas-05327446",
    "producedDateY_i": 2025,
    "texte_nettoye": "L’intégration de l’intelligence artificielle dans le domaine de la copropriété ouvre la voie à une amélioration significative des processus de rédaction et de vérification documentaire. L’État Descriptif de Division (EDD), document structurant par excellence, se prête particulièrement bien à ce type d’approche. En mobilisant des modèles de langage optimisés sur des exemples structurés, il devient possible de générer et contrôler des textes normés, cohérents et fidèles aux attentes professionnelles. Cette démarche permet non seulement de réduire les temps de traitement, mais aussi de sécuriser juridiquement la production grâce à une restitution systématisée des formulations attendues. Elle marque ainsi l’émergence d’une nouvelle manière de produire les documents techniques, plus rapide, plus fiable, et mieux adaptée aux exigences du terrain. À travers cette étude, l’IA apparaît comme un outil opérationnel au service des géomètres-experts, capable de renforcer la qualité, la constance et l’efficience dans la gestion des dossiers de copropriété."
  },
  {
    "title_s": [
      "Identification des patients dans un Entrepôt de Données de Santé Hospitalier : impact de la détection de contexte sur l’extraction des concepts médicaux",
      "Patient identification in a Hospital Health Data Warehouse: impact of context detection on medical concept extraction"
    ],
    "keyword_s": [
      "Hospital health data warehouse",
      "Secondary use of data",
      "Phenotyping",
      "Context detection",
      "Natural language processing",
      "Utilisation secondaire des données",
      "Identification de patient",
      "Détection de contexte",
      "Traitement du langage naturel",
      "Entrepôt de données de santé hospitalier"
    ],
    "abstract_s": [
      "Les entrepôts de données de santé hospitaliers (EDSH) permettent une exploitation secondaire des données médicales à des fins de recherche clinique et épidémiologique. Cependant, l’analyse des données non structurées demeure une tâche complexe, nécessitant l’application de techniques avancées de traitement automatique du langage naturel (TAL), notamment pour la détection de contexte. Cette thèse évalue l’impact de la prise en compte du contexte sur l’identification des concepts médicaux au sein de l’EDSH du CHU de Bordeaux. Deux bibliothèques, EDS-NLP et MedSpaCy, ont été comparées quant à leur capacité à détecter la négation, l’hypothèse, ainsi que les notions d’historique et de non-patient. L’évaluation des performances révèle que EDS-NLP présente un meilleur rappel, tandis que MedSpaCy affiche une meilleure précision, bien que les performances globales restent limitées. Une validation en conditions réelles a été réalisée sur la cohorte ArthroVIH, montrant une réduction du bruit dans l’identification des patients de 3 à 7 %, avec une précision avoisinant 80 %. Toutefois, ces résultats ne permettent pas encore d’automatiser intégralement la sélection des patients dans un EDSH pour une étude clinique. Une utilisation prudente des algorithmes à base de règles est recommandée.",
      "Hospital health data warehouses (EDSH) enable the secondary use of medical data for clinical and epidemiological research. However, analyzing unstructured data remains a complex task, requiring advanced natural language processing (NLP) techniques, particularly for context detection. This thesis assesses the impact of context detection on the identification of medical concepts within the EDSH of Bordeaux University Hospital. Two libraries, EDS-NLP and MedSpaCy, were compared for their ability to detect negation, hypothesis, as well as historical and non-patient contexts. Performance evaluation shows that EDS-NLP achieves higher recall, while MedSpaCy provides greater precision, though overall performance remains modest. A real-world validation was conducted on the ArthroVIH cohort, demonstrating a 3 to 7% reduction in noise in patient identification, with an accuracy of approximately 80%. However, these results do not yet allow for the full automation of patient selection in an EDSH for clinical research. It is recommended to adopt a cautious approach when using rule-based algorithms."
    ],
    "authFullName_s": [
      "Matisse Decilap"
    ],
    "halId_s": "dumas-05064981",
    "producedDateY_i": 2025,
    "texte_nettoye": "Les entrepôts de données de santé hospitaliers (EDSH) permettent une exploitation secondaire des données médicales à des fins de recherche clinique et épidémiologique. Cependant, l’analyse des données non structurées demeure une tâche complexe, nécessitant l’application de techniques avancées de traitement automatique du langage naturel (TAL), notamment pour la détection de contexte. Cette thèse évalue l’impact de la prise en compte du contexte sur l’identification des concepts médicaux au sein de l’EDSH du CHU de Bordeaux. Deux bibliothèques, EDS-NLP et MedSpaCy, ont été comparées quant à leur capacité à détecter la négation, l’hypothèse, ainsi que les notions d’historique et de non-patient. L’évaluation des performances révèle que EDS-NLP présente un meilleur rappel, tandis que MedSpaCy affiche une meilleure précision, bien que les performances globales restent limitées. Une validation en conditions réelles a été réalisée sur la cohorte ArthroVIH, montrant une réduction du bruit dans l’identification des patients de 3 à 7 %, avec une précision avoisinant 80 %. Toutefois, ces résultats ne permettent pas encore d’automatiser intégralement la sélection des patients dans un EDSH pour une étude clinique. Une utilisation prudente des algorithmes à base de règles est recommandée."
  },
  {
    "title_s": [
      "Dynamique de la glu socio-relationnelle en Interaction Humain-Robot : ébauche méthodologique du calcul des coûts langagiers"
    ],
    "keyword_s": [
      "Social robotics",
      "Human-robot interaction",
      "Socio-affective glue",
      "Elderly",
      "Distance weighting",
      "Robotique sociale",
      "Interaction humain-robot",
      "Glu socio-affective",
      "Personnes âgées",
      "Distance d’édition"
    ],
    "abstract_s": [
      "L’interaction humain-robot pose le problème de l’attachement que l’interaction vocale créée dynamiquement entre l’humain et le robot. La notion de glu socio-affective déplace le problème de l’interaction sur le lien social résultant de l’interaction. Nous nous intéressons dans cette étude à la description et la modélisation de la nature et l’évolution de la glu socio-relationnelle et aux variations langagières pluri-dimensionnelles qui concourent à la glu dans les interactions. Notre travail s’appuie sur le corpus Elderly Emox Expression, recueilli en magicien d’Oz, qui s’inscrit dans le contexte des personnes âgées socialement isolées. Les conséquences de l’isolement sont un désentrainement à la dynamique de l’interaction que l’interaction avec le robot Emox pourrait aider à réentrainer. Plus précisément, cette étude permet de déterminer les paramètres langagiers, dans EEE, participant à la création de la glu socio-affective et de tester quels coûts d’édition peuvent être attribués à ces paramètres dans le but de produire un coût global qui serait une mesure de la dynamique de la glu socio-relationnelle.",
      "The human-robot interaction raises the problem of the attachment that the vocal interaction creates dynamically between the human and the robot. The concept of socio-emotional glue focuses on the problem of the interaction on the social link resultant from the interaction. We are interested in the description and the establishment of a model of the nature and the evolution of the socio-affective glue and of the multidimensional language variations enable to create the glue in the elderly interactions. Our work is based on the Elderly Emox Expression corpus (EEE corpus) which has been collected in the Wizard of Oz context and which joins in the context of social isolated elderly. The consequence of the social isolation is the loss of the practice in the dynamics of the interaction and we think that the robot Emox could help to train the elderly interaction skills. Indeed, this study allows determining the language parameters in the EEE corpus which are part of the process of creation of the socio-emotional glue, moreover to try to relate a weight to these parameters with the aim of giving a global weight which is the measure of the dynamics of the socio-relational glue."
    ],
    "authFullName_s": [
      "Liliya Tsvetanova"
    ],
    "halId_s": "dumas-01178695",
    "producedDateY_i": 2015,
    "texte_nettoye": "L’interaction humain-robot pose le problème de l’attachement que l’interaction vocale créée dynamiquement entre l’humain et le robot. La notion de glu socio-affective déplace le problème de l’interaction sur le lien social résultant de l’interaction. Nous nous intéressons dans cette étude à la description et la modélisation de la nature et l’évolution de la glu socio-relationnelle et aux variations langagières pluri-dimensionnelles qui concourent à la glu dans les interactions. Notre travail s’appuie sur le corpus Elderly Emox Expression, recueilli en magicien d’Oz, qui s’inscrit dans le contexte des personnes âgées socialement isolées. Les conséquences de l’isolement sont un désentrainement à la dynamique de l’interaction que l’interaction avec le robot Emox pourrait aider à réentrainer. Plus précisément, cette étude permet de déterminer les paramètres langagiers, dans EEE, participant à la création de la glu socio-affective et de tester quels coûts d’édition peuvent être attribués à ces paramètres dans le but de produire un coût global qui serait une mesure de la dynamique de la glu socio-relationnelle."
  },
  {
    "title_s": [
      "Étude et caractérisation des disfluences scripturales dans les manuscrits de Stendhal"
    ],
    "keyword_s": [
      "Literature",
      "Enunciation",
      "Scriptural disflencies",
      "Digital humanities",
      "Manuscripts",
      "Criticism",
      "Literary",
      "Computer sciences",
      "Linguistics",
      "Disfluences scripturales",
      "Énonciation",
      "Linguistique",
      "Informatique",
      "Littérature",
      "Génétique de texte",
      "Manuscrits",
      "Stendhal",
      "Humanités numériques"
    ],
    "abstract_s": [
      "L'objet du projet de recherche que nous présentons dans ce mémoire porte sur des phénomènes de l'écrit constatés sur les brouillons d'auteurs qui sont analogues aux disfluences de l'oral. Nous donnons le terme <i>disfluences scripturales</i> pour décrire ces phénomènes que nous étudions et caractérisons au sein du corpus des manuscrits de Stendhal. Notre étude porte spécifiquement sur le corpus <i>Journaux</i> de cette collection d'écrits. Nous nous appuyons sur les principes linguistiques et utilisons les méthodes informatiques pour étudier ce corpus. Notre approche, d'un point de vue linguistique, s'appuie sur l'apport de la linguistique de l'énonciation à l'étude des disfluences scripturales. Cette analyse en corpus se fonde sur une démarche de traitement automatique des langues - au sens de l'outil et la modélisation informatique comme micro-macro-scope sur un corpus de matériau langagier numérisé. Nous proposons une caractérisation et une discussion sur la notion de disfluences scripturales, à l'aune des analyses plus présentes dans la littérature scientifiques des disfluences de l'oral.",
      "The research project that we present in this memoir concerns phenomena observed in written language and that we specifically observe in the rough drafts of an author's works. This language phenomena is analogous to what are known as oral disfluencies. We attribute the term <i>scriptural disfluencies</i> to the objects of our study, which we characterise within the corpus of Stendhal's manuscripts. Our study focuses specifically on the corpus <i>Journaux</i> (<i>Journals</i>) of this collection of writing. We rely on linguistic principles and use computational methods for this study. Our approach from the linguistic perspective acknowledges the contribution of the theory of enunciation to the study of scriptural disfluencies. This corpus analysis is also founded in an approach for natural language processing - specifically in the adaptation of an instrument of observation designed for application on a corpus of digitised manuscripts. We propose a linguistic characterisation of the phenomenon along with a discussion around the notion of scriptural disfluencies in light of existing analyses of oral disfluencies in scientific literature."
    ],
    "authFullName_s": [
      "Anne Vikhrova"
    ],
    "halId_s": "dumas-01007699",
    "producedDateY_i": 2014,
    "texte_nettoye": "L'objet du projet de recherche que nous présentons dans ce mémoire porte sur des phénomènes de l'écrit constatés sur les brouillons d'auteurs qui sont analogues aux disfluences de l'oral. Nous donnons le terme <i>disfluences scripturales</i> pour décrire ces phénomènes que nous étudions et caractérisons au sein du corpus des manuscrits de Stendhal. Notre étude porte spécifiquement sur le corpus <i>Journaux</i> de cette collection d'écrits. Nous nous appuyons sur les principes linguistiques et utilisons les méthodes informatiques pour étudier ce corpus. Notre approche, d'un point de vue linguistique, s'appuie sur l'apport de la linguistique de l'énonciation à l'étude des disfluences scripturales. Cette analyse en corpus se fonde sur une démarche de traitement automatique des langues - au sens de l'outil et la modélisation informatique comme micro-macro-scope sur un corpus de matériau langagier numérisé. Nous proposons une caractérisation et une discussion sur la notion de disfluences scripturales, à l'aune des analyses plus présentes dans la littérature scientifiques des disfluences de l'oral."
  },
  {
    "title_s": [
      "Étude onomasiologique et motivationnelle de quelques désignations de phénomènes atmosphériques dans l'État du Minas Gerais (Brésil)"
    ],
    "keyword_s": [
      "Phénomènes atmosphériques",
      "Géolinguistique",
      "Motivation lexicale",
      "Fenômenos atmosféricos",
      "Geolingüística",
      "Motivação lexical"
    ],
    "abstract_s": [
      "Ce mémoire a pour objectif d’explorer la motivation des termes désignant la rosée, le brouillard, la bruine et la grêle dans les parlers du Minas Gerais, au Brésil. Dans un premier temps, nous nous concentrons sur les aspects théoriques autour du signe linguistique et de la motivation lexicale, ainsi que sur leur traitement à travers les atlas linguistiques. Nous proposons ensuite un aperçu de la formation de l’aire linguistique avant de nous concentrer sur l’Esboço de um atlas linguistico de Minas Gerais (EALMG) qui constitue l’outil principal de notre analyse. Enfin, nous examinons les termes utilisés pour désigner ces phénomènes atmosphériques, en tenant compte des traditions et des croyances non seulement au Minas Gerais, mais aussi aux régions historiquement ou linguistiquement liées à cette aire. Cette étude aboutit à une proposition de classification des motivations sémantiques et à la cartographie de leur répartition géographique.",
      "Este trabalho tem como objetivo explorar a motivação dos termos que designam o orvalho, a neblina, a garoa e a granizo nos falares de Minas Gerais, Brasil. Inicialmente, nos concentramos nos aspectos teóricos relacionados ao signo linguístico e à motivação lexical, bem como no tratamento desses conceitos a partir dos atlas linguísticos. Em seguida, apresentamos uma visão geral da área linguística, antes de focarmos no Esboço de um Atlas Linguístico de Minas Gerais (EALMG), que constitui a principal ferramenta da nossa análise. Por fim, examinamos os termos usados para designar esses fenômenos atmosféricos, levando em consideração as tradições e crenças associadas não apenas de Minas Gerais, mas também das regiões historicamente ou linguisticamente relacionadas a essa área. Este estudo resulta em uma proposta de classificação das motivações semânticas desses termos, acompanhada de uma cartografia que ilustra sua distribuição geográfica."
    ],
    "authFullName_s": [
      "Renata dos Santos"
    ],
    "halId_s": "dumas-05425686",
    "producedDateY_i": 2025,
    "texte_nettoye": "Ce mémoire a pour objectif d’explorer la motivation des termes désignant la rosée, le brouillard, la bruine et la grêle dans les parlers du Minas Gerais, au Brésil. Dans un premier temps, nous nous concentrons sur les aspects théoriques autour du signe linguistique et de la motivation lexicale, ainsi que sur leur traitement à travers les atlas linguistiques. Nous proposons ensuite un aperçu de la formation de l’aire linguistique avant de nous concentrer sur l’Esboço de um atlas linguistico de Minas Gerais (EALMG) qui constitue l’outil principal de notre analyse. Enfin, nous examinons les termes utilisés pour désigner ces phénomènes atmosphériques, en tenant compte des traditions et des croyances non seulement au Minas Gerais, mais aussi aux régions historiquement ou linguistiquement liées à cette aire. Cette étude aboutit à une proposition de classification des motivations sémantiques et à la cartographie de leur répartition géographique."
  },
  {
    "title_s": [
      "Detection externalisée de vulnerabilités pour la plateforme Android à l'aide du langage OVAL"
    ],
    "keyword_s": [
      "Vulnérabilités",
      "Analyses",
      "OVAL",
      "Android"
    ],
    "abstract_s": [
      "Nous proposons dans ce rapport une approche novatrice pour analyser les systèmes Android et détecter leurs vulnérabilités de façon légère. Cette approche regroupe les principales composantes du processus d'analyse sous forme de service externalisé que les clients mobiles peuvent ensuite exploiter à l'aide d'un agent minimal. Le langage OVAL est utilisé comme support pour la description et l'analyse de vulnérabilités. En configurant la fréquence des analyses et le pourcentage de vulnérabilités à traiter au cours de chacune d'entre elles, l'approche proposée permet de limiter l'allocation de ressources côté client et de transférer les différents traitements sur des serveurs distants. La stratégie employée consiste à partager et distribuer les analyses à travers le temps pour réduire significativement l'activité sur les systèmes mobiles, tout en assurant un traitement de la totalité des vulnérabilités connues dans un laps de temps fini. De cette méthodologie résulte un processus d'analyse orienté cloud plus léger et plus rapide, pouvant limiter de façon significative la consommation de ressources et d'énergie côté client."
    ],
    "authFullName_s": [
      "Gaëtan Hurel"
    ],
    "halId_s": "hal-00875179",
    "producedDateY_i": 2013,
    "texte_nettoye": "Nous proposons dans ce rapport une approche novatrice pour analyser les systèmes Android et détecter leurs vulnérabilités de façon légère. Cette approche regroupe les principales composantes du processus d'analyse sous forme de service externalisé que les clients mobiles peuvent ensuite exploiter à l'aide d'un agent minimal. Le langage OVAL est utilisé comme support pour la description et l'analyse de vulnérabilités. En configurant la fréquence des analyses et le pourcentage de vulnérabilités à traiter au cours de chacune d'entre elles, l'approche proposée permet de limiter l'allocation de ressources côté client et de transférer les différents traitements sur des serveurs distants. La stratégie employée consiste à partager et distribuer les analyses à travers le temps pour réduire significativement l'activité sur les systèmes mobiles, tout en assurant un traitement de la totalité des vulnérabilités connues dans un laps de temps fini. De cette méthodologie résulte un processus d'analyse orienté cloud plus léger et plus rapide, pouvant limiter de façon significative la consommation de ressources et d'énergie côté client."
  },
  {
    "title_s": [
      "Vers une analyse génétique de textes assistée par l'informatique et le TAL : contextes et pistes exploratoires"
    ],
    "keyword_s": [
      "Manuscripts",
      "Philologists",
      "Textual genetic",
      "NLP",
      "Manuscrits",
      "Généticiens",
      "Génétique de textes",
      "TAL"
    ],
    "abstract_s": [
      "L'apparition de corpus numériques de manuscrits littéraires a enrichi notre patrimoine d'une donnée langagière analysable et traitable automatiquement. La transcription du contenu de ces manuscrits dans un format numérique textuel permet de parcourir en quelques secondes des milliers de mots. L'étude des différentes versions d'une œuvre littéraire donne lieu à des enquêtes fastidieuses de la part des chercheurs en littérature. Se posent alors de nouvelles questions de méthodologie de travail : comment exploiter au mieux l'outil informatique pour assister le chercheur, quelles sont les nouvelles études envisageables grâce aux progrès ? Après un aperçu de l'analyse génétique et d'outils de traitement automatique des langues existants dans le domaine, nous présentons la modélisation sur les manuscrits de Stendhal de trois fonctionnalités à trois niveaux de granularité qui assisteraient les chercheurs en littérature dans leur analyse sur les bibliothèques d'auteurs, le théâtre ou le code-switching.",
      "The rise of digital corpora of literary manuscripts has added a substantial amount of linguistic data to our heritage that can be analyzed and processed automatically. The transcription of the content of these manuscripts in digital text format allows thousands of words to be examined in a few seconds. Studying different versions of a literary work requires time-consuming examination by researchers in the field of literature. New methodological issues thus arise: how can information technology tools be best put to use to help researchers? What new studies are made possible by this progress? After an overview of textual genetic analysis and of the existing tools for natural language processing in this domain, we will outline the modelling, on Stendhal's manuscripts, of three functions that can aid literary researchers in their analysis of author's libraries, theatrical writing and code-switching."
    ],
    "authFullName_s": [
      "Claire Lemaire"
    ],
    "halId_s": "dumas-00516484",
    "producedDateY_i": 2010,
    "texte_nettoye": "L'apparition de corpus numériques de manuscrits littéraires a enrichi notre patrimoine d'une donnée langagière analysable et traitable automatiquement. La transcription du contenu de ces manuscrits dans un format numérique textuel permet de parcourir en quelques secondes des milliers de mots. L'étude des différentes versions d'une œuvre littéraire donne lieu à des enquêtes fastidieuses de la part des chercheurs en littérature. Se posent alors de nouvelles questions de méthodologie de travail : comment exploiter au mieux l'outil informatique pour assister le chercheur, quelles sont les nouvelles études envisageables grâce aux progrès ? Après un aperçu de l'analyse génétique et d'outils de traitement automatique des langues existants dans le domaine, nous présentons la modélisation sur les manuscrits de Stendhal de trois fonctionnalités à trois niveaux de granularité qui assisteraient les chercheurs en littérature dans leur analyse sur les bibliothèques d'auteurs, le théâtre ou le code-switching."
  },
  {
    "title_s": [
      "Parsing Punctuation and Coordination Extragrammatically"
    ],
    "keyword_s": [
      "Interaction grammar",
      "Punctuation",
      "Parsing",
      "Coordination"
    ],
    "abstract_s": [
      "Coordination is a syntactic construction that is extremely frequent in natural language and yet very difficult to analyse: it is highly ambiguous, as different types of constituents and non-constituents can be coordinated in different contexts, and it cannot be easily modelled using the same formal tools used to represent the \"basic\", coordination-less part of natural languages. As for punctuation, often neglected as an object of study by linguists because it is peculiar to written language, it can combine with or substitute conjunctions to play a coordinative role, or play an adjunctive role. Some researchers propose that coordination is not a grammatical phenomenon that is a matter of linguistic competence, but rather a performance issue that should be analysed directly among the syntactic structures. We suggest to use this idea to the benefit of natural language processing by defining an algorithm that deals with coordination and punctuation using graph transformations applied directly to the output of a parser based on a model of \"basic\" language. The syntactic structure of a sentence as proposed by our system takes the shape of a directed acyclic graph in which the constituent sharing phenomenon at the roots of coordination appears sharply. We detail an algorithm working within the framework of interaction grammars (but suggest ways to adapt it to other formalisms, namely tree-adjoining grammars, phrase structure grammars, and dependency syntax) which is able to parse many types of coordinative and adjunctive constructions.",
      "La coordination est une construction syntaxique extrêmement fréquente dans les langues naturelles et néanmoins très difficile à analyser : elle est hautement ambiguë, de nombreux types de constituants ou non-constituants pouvant être coordonnés dans différents contextes, et se prête difficilement à une modélisation à l'aide des outils formels employés pour représenter la portion \" basique \" des langues naturelles, c'est-à-dire dépourvue de coordination. Quant à la ponctuation, objet d'étude souvent occulté par la linguistique car étant propre au langage écrit, elle peut se combiner ou se substituer aux conjonctions pour jouer un rôle coordinatif, ou encore jouer un rôle adjonctif. Certains chercheurs avancent que la coordination ne constitue pas un phénomène grammatical relevant de la compétence linguistique, mais plutôt un fait de performance qui devrait s'analyser au sein même des structures syntaxiques. Nous nous proposons de reprendre cette idée au compte du traitement automatique des langues en définissant un algorithme de traitement de la coordination et de la ponctuation qui opère un processus de transformation de graphes sur la sortie d'un analyseur basé sur un modèle de langage \" basique \". La structure syntaxique d'une phrase telle que proposée par notre système prend la forme d'un graphe acyclique orienté, dans lequel apparaît clairement le phénomène de partage de constituants qui définit la nature de la coordination. Nous détaillons un algorithme dans le cadre des grammaires d'interaction, mais pouvant être étendu à d'autres formalismes (nommément les grammaires d'arbres adjoints, les grammaires de structures de phrase et la syntaxe de dépendance), qui permet d'analyser de nombreux types de constructions coordinatives et adjonctives. Nous comparons celui-ci à une approche classique de modélisation de la coordination dans le formalisme des grammaires d'interaction développée par Le Roux et Perrier (2006) puis à une approche similaire à la nôtre développée dans le cadre des grammaires d'arbres adjoints par Joshi et Schabes (1991)."
    ],
    "authFullName_s": [
      "Valmi Dufour-Lussier"
    ],
    "halId_s": "inria-00634736",
    "producedDateY_i": 2010,
    "texte_nettoye": "Coordination is a syntactic construction that is extremely frequent in natural language and yet very difficult to analyse: it is highly ambiguous, as different types of constituents and non-constituents can be coordinated in different contexts, and it cannot be easily modelled using the same formal tools used to represent the \"basic\", coordination-less part of natural languages. As for punctuation, often neglected as an object of study by linguists because it is peculiar to written language, it can combine with or substitute conjunctions to play a coordinative role, or play an adjunctive role. Some researchers propose that coordination is not a grammatical phenomenon that is a matter of linguistic competence, but rather a performance issue that should be analysed directly among the syntactic structures. We suggest to use this idea to the benefit of natural language processing by defining an algorithm that deals with coordination and punctuation using graph transformations applied directly to the output of a parser based on a model of \"basic\" language. The syntactic structure of a sentence as proposed by our system takes the shape of a directed acyclic graph in which the constituent sharing phenomenon at the roots of coordination appears sharply. We detail an algorithm working within the framework of interaction grammars (but suggest ways to adapt it to other formalisms, namely tree-adjoining grammars, phrase structure grammars, and dependency syntax) which is able to parse many types of coordinative and adjunctive constructions."
  },
  {
    "title_s": [
      "Système de gestion lexicale des ressources termino-ontologiques"
    ],
    "keyword_s": [
      "Data Warehousing",
      "Natural Language Processings",
      "Information Science",
      "Medical Informatics",
      "Science de l’informatique médicale",
      "Sciences de l’information",
      "Traitement automatique du langage naturel",
      "Entrepôt de données"
    ],
    "abstract_s": [
      "L’utilisation de services se basant sur un modèle de gestion lexicale permettrait d’apporter une aide aux processus de recherche d’informations dans les documents en texte libre. Ce document est la synthèse d’un travail réalisé au sein de l’unité IAM du CHU de Bordeaux, ayant pour but l’implémentation d’une structure se basant sur un modèle de gestion lexicale afin que des services et méthodes puissent y être développés. Le choix du modèle de gestion lexicale s’est porte sur Ontolex-Lemon, qui répondait le mieux à nos attentes. L’évaluation de cette structure a été faite en y intégrant quatre ressources termino-ontologiques, et des services et m´méthodes comme la détection de termes synonymes et polysémiques y ont été intègres.",
      "The use of services based on a lexical management model would help in the process of searching for information in free text documents. This document is the synthesis of a work carried out within the IAM unit of the CHU of Bordeaux, with the aim of implementing a structure based on a lexical management model so that services and methods can be developed there. The choice of the lexical management model was Ontolex-Lemon, which best met our expectations. The evaluation of this structure was done by integrating four termino-ontological resources, and services and methods such as synonym and polysemous terms detection were integrated."
    ],
    "authFullName_s": [
      "Guillaume Verdy"
    ],
    "halId_s": "dumas-03844324",
    "producedDateY_i": 2022,
    "texte_nettoye": "L’utilisation de services se basant sur un modèle de gestion lexicale permettrait d’apporter une aide aux processus de recherche d’informations dans les documents en texte libre. Ce document est la synthèse d’un travail réalisé au sein de l’unité IAM du CHU de Bordeaux, ayant pour but l’implémentation d’une structure se basant sur un modèle de gestion lexicale afin que des services et méthodes puissent y être développés. Le choix du modèle de gestion lexicale s’est porte sur Ontolex-Lemon, qui répondait le mieux à nos attentes. L’évaluation de cette structure a été faite en y intégrant quatre ressources termino-ontologiques, et des services et m´méthodes comme la détection de termes synonymes et polysémiques y ont été intègres."
  },
  {
    "title_s": [
      "Utilisation de la télédétection pour la détection et le suivi des algues brunes sur la réserve naturelle des terres australes françaises"
    ],
    "abstract_s": [
      "Cette étude vise à étudier la faisabilité de la détection des algues brunes, et principalement de la macroalgue Macrocystis pyrifera (Linnaeus) C. Agardh 1820, au sein du territoire des TAAF (Terres Australes et Antarctiques Françaises), dans le but d’extraire une donnée exhaustive quant à l’étendue des bancs visibles en surface dans un premier temps, et dans un second temps, de tester un algorithme de traitement automatisé. Notre objectif est de produire une cartographie fine de la couverture de Macrocystis pyrifera (MP) sur l’archipel de Kerguelen. Etant donné la proximité de MP avec Durvilleae Antarctica (DA) (du point de vue de leur signature spectrale), les résultats intègrent aussi cette dernière. Les données d’Observation de la Terre utilisée sont les images satellitaires libres et gratuites Sentinel-2 (A et B). Elles ont été téléchargées sur le portail d’accès de Copernicus. La chaîne de traitement mise en place est entièrement basée sur le langage R au sein de l’interface Rstudio (Version 1.3.1093). Ainsi, pour la totalité des traitements, de la requête des produits disponibles et du téléchargement jusqu’au produit final, la chaîne développée est entièrement automatique, reproductible et open source. Les téléchargements et prétraitements sont effectués grâce à une librairie (ensemble de fonctions R) complète, sen2r, créée par Luigi Ranghetti (L. Ranghetti et al., 2020). Les résultats de détection des algues brunes à partir de la chaîne sont proches de l’exhaustivité avec un Kappa global de 0.993. Des produits finaux clairs et utilisables par un utilisateur n’ayant pas forcément été initié à la télédétection seront favorisés. Plusieurs produits seront créés dans ce but, tel qu’une couche shapefile représentant la présence moyenne d’algue brune annuelle et globale (c.à.d. sur toute l’étendue temporelle disponible) ; ainsi que des couches rasters un peu plus complexes représentant des synthèses temporelles annuelles et globales sur la densité et la fréquence de présence de MP. Cette étude rentre dans le cadre d’un rapport final de stage de fin de Master 2. Les résultats de ce stage sont à destination des TAAF et de leurs partenaires."
    ],
    "authFullName_s": [
      "Alexis Pré"
    ],
    "halId_s": "hal-04198366",
    "producedDateY_i": 2020,
    "texte_nettoye": "Cette étude vise à étudier la faisabilité de la détection des algues brunes, et principalement de la macroalgue Macrocystis pyrifera (Linnaeus) C. Agardh 1820, au sein du territoire des TAAF (Terres Australes et Antarctiques Françaises), dans le but d’extraire une donnée exhaustive quant à l’étendue des bancs visibles en surface dans un premier temps, et dans un second temps, de tester un algorithme de traitement automatisé. Notre objectif est de produire une cartographie fine de la couverture de Macrocystis pyrifera (MP) sur l’archipel de Kerguelen. Etant donné la proximité de MP avec Durvilleae Antarctica (DA) (du point de vue de leur signature spectrale), les résultats intègrent aussi cette dernière. Les données d’Observation de la Terre utilisée sont les images satellitaires libres et gratuites Sentinel-2 (A et B). Elles ont été téléchargées sur le portail d’accès de Copernicus. La chaîne de traitement mise en place est entièrement basée sur le langage R au sein de l’interface Rstudio (Version 1.3.1093). Ainsi, pour la totalité des traitements, de la requête des produits disponibles et du téléchargement jusqu’au produit final, la chaîne développée est entièrement automatique, reproductible et open source. Les téléchargements et prétraitements sont effectués grâce à une librairie (ensemble de fonctions R) complète, sen2r, créée par Luigi Ranghetti (L. Ranghetti et al., 2020). Les résultats de détection des algues brunes à partir de la chaîne sont proches de l’exhaustivité avec un Kappa global de 0.993. Des produits finaux clairs et utilisables par un utilisateur n’ayant pas forcément été initié à la télédétection seront favorisés. Plusieurs produits seront créés dans ce but, tel qu’une couche shapefile représentant la présence moyenne d’algue brune annuelle et globale (c.à.d. sur toute l’étendue temporelle disponible) ; ainsi que des couches rasters un peu plus complexes représentant des synthèses temporelles annuelles et globales sur la densité et la fréquence de présence de MP. Cette étude rentre dans le cadre d’un rapport final de stage de fin de Master 2. Les résultats de ce stage sont à destination des TAAF et de leurs partenaires."
  },
  {
    "title_s": [
      "Détection automatique des infections du site opératoire"
    ],
    "keyword_s": [
      "Supervised machine learning",
      "Natural language processing",
      "Data warehousing",
      "Surgical wound infection",
      "Entreposage de données",
      "Traitement du langage naturel",
      "Apprentissage machine supervisé",
      "Infection de plaie opératoire"
    ],
    "abstract_s": [
      "Introduction - L'amélioration de la surveillance et de la prévention des infections du site opératoire (ISO) fait partie du programme national de lutte contre les infections nosocomiales. Notre objectif était de mettre en place un outil de détection automatique par apprentissage supervisé afin de remplacer le système de surveillance actuel. Méthode - Deux approches ont été menées pour détecter les ISO suite à une chirurgie du rachis et une neurochirurgie correspondant respectivement à 2133 et 2303 interventions. La première approche utilise les multiples sources d’information disponibles dans l’entrepôt de données du CHU de Bordeaux. La seconde approche utilise uniquement le texte libre. Pour chaque approche, nous avons comparé la précision de deux algorithmes, à savoir la régression logistique et les forêts aléatoires, avec un rappel fixé à 100%. Résultats - Le modèle final utilisant toutes les données a obtenu les meilleures performances pour la chirurgie du rachis avec une précision de 94%. Le modèle utilisant des données de texte libre a obtenu des résultats corrects et était meilleur pour la neurochirurgie. Discussion - L'utilisation du texte libre présente l'avantage d'être transposable à d'autres établissements de santé et facilement applicable à diverses spécialités chirurgicales avec des performances stables. Les performances de nos algorithmes doivent être évaluées sur un jeu de données test.",
      "Introduction Improving monitoring and prevention of surgical site infections (SSI) is part of the national nosocomial infection control program. Our goal was to implement an automated detection tool with a supervised machine learning to replace the current manual system. Method Two approaches were conducted to detect SSI following spine surgery and neurosurgery corresponding to 2133 and 2303 procedures respectively. The first approach uses the multiple sources of information available in the data warehouse of Bordeaux University Hospital. The second approach uses only free text. For each approach, we compared the precision of two algorithms namely logistic regression and random forests algorithms for a fixed recall value of 100%. Results The final model using all the data achieved the best performance for spine surgery with a precision of 94%. The model using free text data obtained correct results and was better for neurosurgery. Discussion The use of free text has the advantage of being replicable to other health institutions and applicable to various surgical specialties with stable performance. The performance of our algorithms needs to be evaluated on a test dataset."
    ],
    "authFullName_s": [
      "Marine Quéroué"
    ],
    "halId_s": "dumas-02420229",
    "producedDateY_i": 2019,
    "texte_nettoye": "Introduction - L'amélioration de la surveillance et de la prévention des infections du site opératoire (ISO) fait partie du programme national de lutte contre les infections nosocomiales. Notre objectif était de mettre en place un outil de détection automatique par apprentissage supervisé afin de remplacer le système de surveillance actuel. Méthode - Deux approches ont été menées pour détecter les ISO suite à une chirurgie du rachis et une neurochirurgie correspondant respectivement à 2133 et 2303 interventions. La première approche utilise les multiples sources d’information disponibles dans l’entrepôt de données du CHU de Bordeaux. La seconde approche utilise uniquement le texte libre. Pour chaque approche, nous avons comparé la précision de deux algorithmes, à savoir la régression logistique et les forêts aléatoires, avec un rappel fixé à 100%. Résultats - Le modèle final utilisant toutes les données a obtenu les meilleures performances pour la chirurgie du rachis avec une précision de 94%. Le modèle utilisant des données de texte libre a obtenu des résultats corrects et était meilleur pour la neurochirurgie. Discussion - L'utilisation du texte libre présente l'avantage d'être transposable à d'autres établissements de santé et facilement applicable à diverses spécialités chirurgicales avec des performances stables. Les performances de nos algorithmes doivent être évaluées sur un jeu de données test."
  },
  {
    "title_s": [
      "Caractérisation de la nasalité en contexte de parole : séparation du signal oral et nasal pour la recherche des corrélats de la nasalité dans le signal oral. Application au français et au mandarin"
    ],
    "keyword_s": [
      "Naxi language",
      "Mandarin",
      "French",
      "Na dialectology",
      "Articulatory",
      "Acoustico-perceptive cues",
      "Nasalization",
      "Nasality",
      "Acoustics",
      "Phonology",
      "Phonetics",
      "Dialectologie Na",
      "Phonétique",
      "Phonologie",
      "Acoustique",
      "Nasalité",
      "Nasalisation",
      "Corrélats acoustico-perceptifs",
      "Articulatoire",
      "Langue naxi",
      "Français"
    ],
    "abstract_s": [
      "Toute langue se construit par le jeu des oppositions phonologiques qui permettent de distinguer les phonèmes. Définie comme la caractéristique d’un phonème réalisé avec ouverture du port vélopharyngé, la nasalité donne lieu à des paires minimales dans la langue. La nasalité phonologique est présente massivement dans les langues du monde. 22 % des langues ont au moins une voyelle nasale et 96,5 % ont au moins une consonne nasale (données UPSID). En considérant la capacité de désychronisation du velum par rapport aux articulateurs buccaux, la nasalité est placée à un niveau supérieur de hiérarchie dans la théorie de Clements (1985), et par sa place dans l’inventaire phonologique du français, elle constitue un objet qui passionne la recherche. La complexité articulatori-acoustique du phénomène a souvent rendu difficiles les analyses couplées, d’où l’idée de séparer acoustiquement le signal oral et nasal. Partant ensuite de ce pattern appelé œil des nasales, très visible dans la partie orale d’une voyelle nasale, un procédé de calcul des dérivées de formants au cours du temps a été mis au point sur le signal séparé, de sorte que soient mesurées les variations conjointes des formants liées à l’apparition de l’œil des nasales. Le pincement ascendant des formants haute fréquence et le phénomène de densification basse fréquence est exploité en adossant des critères à la variation des formants, pourvu que les mesures soient précises et le traitement statistique de la variabilité pris en compte. La séparation acoustique oral – nasal permet également d’observer en phonologie de laboratoire les phénomènes d’anticipation et de persévérance du trait nasal, à l’image des mesures de durées de nasalisation différentes sur les codas nasales du mandarin de Taïwan. Le bénéfice attendu au niveau de la linguistique est de disposer d’un outil permettant de mieux décrire et comprendre les phénomènes d’émergence de la nasalité dans les langues du monde et d’en améliorer sa catégorisation.",
      "Any language is built based on the phonological oppositions that make phonemes distinct from one another. Nasality is defined as the opening of the velopharyngeal port, and the nasality feature leads to minimal pairs in language. Phonologically present in 22 % of UPSID as a vowel, and 96 % as a consonant, this feature is massively represented in the world’s languages. Considering that the velum can move in desynchronization with the oral tract articulators, nasality pertains to a higher level of hierarchy in Clements’ (1985) theory. Due to its particular status in the french phonological inventory, it represents a fascinating object for research. The articulatori-acoustic complexity of the phenomenon often made coupled analyses more difficult, hence the idea of separating acoustically the oral and nasal signal. A calculation method has been developed based on the pattern called « eye of the nasals », quite recognizable in the oral part of a nasalized vowel. It determines the formants’ order 1 differentiate value as a function of time, based solely on the oral part of a nasal vowel and from there criteria of joint temporal evolution of the formants are defined, as a characteristic of the appearance of the « eye of the nasals », and therefore nasality. The high frequency pinching of the formants and the low frequency densification phenomena are what causes the appearance of the « eye of the nasals ». In addition, the oral – nasal components separation allows us to observe phenomena of nasal anticipation and perseverance in Taiwan Mandarin Chinese. The benefits expected of this laboratory phonology study are to provide linguistics research with a tool that can help better describe and comprehend phenomena related to the emergence of nasality in the world’s languages."
    ],
    "authFullName_s": [
      "Maxime Fily"
    ],
    "halId_s": "dumas-01847016",
    "producedDateY_i": 2018,
    "texte_nettoye": "Toute langue se construit par le jeu des oppositions phonologiques qui permettent de distinguer les phonèmes. Définie comme la caractéristique d’un phonème réalisé avec ouverture du port vélopharyngé, la nasalité donne lieu à des paires minimales dans la langue. La nasalité phonologique est présente massivement dans les langues du monde. 22 % des langues ont au moins une voyelle nasale et 96,5 % ont au moins une consonne nasale (données UPSID). En considérant la capacité de désychronisation du velum par rapport aux articulateurs buccaux, la nasalité est placée à un niveau supérieur de hiérarchie dans la théorie de Clements (1985), et par sa place dans l’inventaire phonologique du français, elle constitue un objet qui passionne la recherche. La complexité articulatori-acoustique du phénomène a souvent rendu difficiles les analyses couplées, d’où l’idée de séparer acoustiquement le signal oral et nasal. Partant ensuite de ce pattern appelé œil des nasales, très visible dans la partie orale d’une voyelle nasale, un procédé de calcul des dérivées de formants au cours du temps a été mis au point sur le signal séparé, de sorte que soient mesurées les variations conjointes des formants liées à l’apparition de l’œil des nasales. Le pincement ascendant des formants haute fréquence et le phénomène de densification basse fréquence est exploité en adossant des critères à la variation des formants, pourvu que les mesures soient précises et le traitement statistique de la variabilité pris en compte. La séparation acoustique oral – nasal permet également d’observer en phonologie de laboratoire les phénomènes d’anticipation et de persévérance du trait nasal, à l’image des mesures de durées de nasalisation différentes sur les codas nasales du mandarin de Taïwan. Le bénéfice attendu au niveau de la linguistique est de disposer d’un outil permettant de mieux décrire et comprendre les phénomènes d’émergence de la nasalité dans les langues du monde et d’en améliorer sa catégorisation."
  },
  {
    "title_s": [
      "Deux romans, une intertextualité, deux approches : lire Meursault, contre-enquête de Kamel Daoud à la croisée des regards : OEil humain, oeil de la machine.",
      "Two novels, an intertextuality, two approaches: reading Kamel Daoud's Meursault, contre-enquête at the crossroads of human and machine perspectives."
    ],
    "keyword_s": [
      "Hyperbase",
      "Logometry",
      "Quantitative intertextuality",
      "Literary rewriting",
      "Réécriture littéraire",
      "Intertextualité quantitative",
      "Logométrie",
      "Hyperbase"
    ],
    "abstract_s": [
      "Cette recherche explore les territoires de l'intertextualité à travers la réécriture de L'Étranger de Camus par Kamel Daoud dans son roman Meursault, contre-enquête. À la croisée des approches littéraires et logométriques, cette étude conjugue l'analyse littéraire traditionnelle aux potentialités du traitement automatique via le logiciel Hyperbase. L’hybridité méthodologique invite à repenser la notion même d'intertextualité en mettant au jour sa pluralité conceptuelle et les différentes facettes des liens unissant l’hypertexte à son hypotexte. L'enjeu est d'importance : comment l'exploration automatisée du langage peut-elle appréhender un phénomène intimement lié à la mémoire du lecteur ? Ainsi, nous suivrons un parcours d'objectivation par la mesure, tout en exposant les apports de ce choix méthodologique et ses limites dans le cas étudié.",
      "This research explores the territories of intertextuality through Kamel Daoud's rewriting of Camus's L'Étranger in his novel Meursault, contre-enquête. At the crossroads of literary and logometric approaches, this study combines traditional literary analysis with the potential of automatic processing using Hyperbase software. This methodological hybridity invites us to rethink the very notion of intertextuality, bringing to light its conceptual plurality and the different facets of the links uniting hypertext to its hypotext. The stakes are high: how can automated language exploration apprehend a phenomenon intimately linked to the reader's memory? We'll follow a path of objectivation through measurement, while outlining the contributions of this methodological choice and its limitations in the case under study."
    ],
    "authFullName_s": [
      "Marwa Boukra"
    ],
    "halId_s": "hal-04923307",
    "producedDateY_i": 2024,
    "texte_nettoye": "Cette recherche explore les territoires de l'intertextualité à travers la réécriture de L'Étranger de Camus par Kamel Daoud dans son roman Meursault, contre-enquête. À la croisée des approches littéraires et logométriques, cette étude conjugue l'analyse littéraire traditionnelle aux potentialités du traitement automatique via le logiciel Hyperbase. L’hybridité méthodologique invite à repenser la notion même d'intertextualité en mettant au jour sa pluralité conceptuelle et les différentes facettes des liens unissant l’hypertexte à son hypotexte. L'enjeu est d'importance : comment l'exploration automatisée du langage peut-elle appréhender un phénomène intimement lié à la mémoire du lecteur ? Ainsi, nous suivrons un parcours d'objectivation par la mesure, tout en exposant les apports de ce choix méthodologique et ses limites dans le cas étudié."
  },
  {
    "title_s": [
      "Une exploration de l'architecture des réseaux de neurones pour la modélisation de la compositionnalité sémantique"
    ],
    "keyword_s": [
      "Linguistics",
      "Computer linguistics",
      "Compositionality",
      "Computer sciences",
      "Neural networks",
      "Machine learning",
      "Vectors",
      "Semantics",
      "Linguistique",
      "Sémantique",
      "Compositionnalité",
      "Vecteurs",
      "Réseaux de neurones",
      "Apprentissage automatique",
      "Informatique",
      "Traitement automatique des langues"
    ],
    "abstract_s": [
      "Ce mémoire présente une évaluation du modèle de réseau de neurones, appelé autoencodeur, qui permet de capturer le sens de couples adjectif-nom en anglais. Ce modèle fonctionne sur la base de la représentation du sens des mots par un vecteur contenant les indices des lemmes constituant le contexte des mots en question. Ces indices sont attribués aux lemmes en fonction de leur fréquence dans notre corpus issu de Wikipédia. Notre modèle est évalué sur un test de similarité entre deux couples adjectif-nom puis sur un test de recomposition des vecteurs de contexte du couple adjectif-nom à partir des vecteurs de contexte de ses composants pris séparément. Les résultats de ces deux tâches est ensuite comparé aux modèles déjà existants suivants : l'addition de vecteurs (modèle additif), le modèle additif pondéré avec un coefficient plus fort sur le vecteur du nom, le modèle basique où seul le vecteur contexte du nom est pris en compte, et la multiplication de vecteurs (modèle multiplicatif).",
      "This dissertation presents an evaluation of a neural network model called autoencoder in order to capture the meaning of adjective-noun couples in English. This model works on the representation of words meaning by a vector countaining the index of the words' context lemmas. These indexes are assigned to lemmas according to their frenquency in our Wikipedia-extracted corpus. Our model is evaluated on similarity task between two adjective-noun couples, and then on a task of recomposition of adjective-noun couples vector from their separated components context vectors. These two task results were eventually compared to the following already existing models : the vectors sum (additive model), the weighted additive with a stronger rating on the noun vector, the baseline model where only the nouns vector is taken, and the multiplicative model (multiplication of vectors)."
    ],
    "authFullName_s": [
      "Chloé Cimpello"
    ],
    "halId_s": "dumas-01212786",
    "producedDateY_i": 2015,
    "texte_nettoye": "Ce mémoire présente une évaluation du modèle de réseau de neurones, appelé autoencodeur, qui permet de capturer le sens de couples adjectif-nom en anglais. Ce modèle fonctionne sur la base de la représentation du sens des mots par un vecteur contenant les indices des lemmes constituant le contexte des mots en question. Ces indices sont attribués aux lemmes en fonction de leur fréquence dans notre corpus issu de Wikipédia. Notre modèle est évalué sur un test de similarité entre deux couples adjectif-nom puis sur un test de recomposition des vecteurs de contexte du couple adjectif-nom à partir des vecteurs de contexte de ses composants pris séparément. Les résultats de ces deux tâches est ensuite comparé aux modèles déjà existants suivants : l'addition de vecteurs (modèle additif), le modèle additif pondéré avec un coefficient plus fort sur le vecteur du nom, le modèle basique où seul le vecteur contexte du nom est pris en compte, et la multiplication de vecteurs (modèle multiplicatif)."
  },
  {
    "title_s": [
      "Conceptualisation d’un modèle de données complètement structurées et standardisées appliqué à un dossier patient informatisé"
    ],
    "keyword_s": [
      "Dossiers médicaux",
      "Dossier médical partagé",
      "Bases de données médico-administratives -- Qualité -- Contrôle",
      "Qualité des données",
      "Terminologies",
      "Données structurées",
      "Données médicales",
      "Dossier patient informatisé"
    ],
    "abstract_s": [
      "Le dossier patient informatisé ou DPI est un élément central dans la prise en charge médicale d’un patient. Il regroupe un nombre d’informations concernant le patient qu’un professionnel de santé doit connaître. Aujourd’hui, il existe de nombreux logiciels incluant le DPI mais toutes les données n’y sont pas totalement structurées. En France, le DPI, le Dossier Médical Partagé et l’Espace Numérique de Santé sont encadrés par la législation qui mentionne les éléments indispensables à retrouver dans un dossier patient. Des codes et terminologies médicales existent à des fins de descriptions médicales du patient et de son parcours de soins. Cette thèse donne une proposition des codes et terminologies les plus adéquats en regard de chaque élément d’un DPI. Cette proposition de structuration n’est pas parfaite et est discutable. En effet, la structuration permet une meilleure qualité des données pour les entrepôts de données de santé par exemple. Mais elle n’est pas toujours adaptée à l’activité médicale. Les éditeurs doivent adapter leur logiciel afin de transformer par des techniques de traitement du langage, le texte libre des médecins, en texte structuré pour améliorer la qualité des données médicales.L’apprentissage automatique et les réseaux de neurones sont très efficaces pour améliorer toutes les problématiques évoquées mais ils sont très sensibles à la qualité des données qui reste un défi à relever aujourd’hui."
    ],
    "authFullName_s": [
      "Laura Gosselin"
    ],
    "halId_s": "dumas-03959941",
    "producedDateY_i": 2022,
    "texte_nettoye": "Le dossier patient informatisé ou DPI est un élément central dans la prise en charge médicale d’un patient. Il regroupe un nombre d’informations concernant le patient qu’un professionnel de santé doit connaître. Aujourd’hui, il existe de nombreux logiciels incluant le DPI mais toutes les données n’y sont pas totalement structurées. En France, le DPI, le Dossier Médical Partagé et l’Espace Numérique de Santé sont encadrés par la législation qui mentionne les éléments indispensables à retrouver dans un dossier patient. Des codes et terminologies médicales existent à des fins de descriptions médicales du patient et de son parcours de soins. Cette thèse donne une proposition des codes et terminologies les plus adéquats en regard de chaque élément d’un DPI. Cette proposition de structuration n’est pas parfaite et est discutable. En effet, la structuration permet une meilleure qualité des données pour les entrepôts de données de santé par exemple. Mais elle n’est pas toujours adaptée à l’activité médicale. Les éditeurs doivent adapter leur logiciel afin de transformer par des techniques de traitement du langage, le texte libre des médecins, en texte structuré pour améliorer la qualité des données médicales.L’apprentissage automatique et les réseaux de neurones sont très efficaces pour améliorer toutes les problématiques évoquées mais ils sont très sensibles à la qualité des données qui reste un défi à relever aujourd’hui."
  },
  {
    "title_s": [
      "Application de la fusion de chemins de données à la synthèse d'architectures parallèles"
    ],
    "abstract_s": [
      "Le marché du système embarqué est le théâtre d'une véritable course à tous les niveaux (performance, miniaturisation, consommation électrique). En effet que ce soit pour le multimédia portable (consoles portables, appareils photos et caméras numériques...) ou pour les télécoms (émetteurs-récepteurs et téléphones 3 et bientôt 4G), la très forte concurrence oblige les systèmes à être de plus en plus performants (meilleure résolution, meilleure transmission...) et à avoir une plus grande autonomie, tout en conservant, voir en diminuant, leur taille. La plupart de ces systèmes embarqués ont un point commun : ils doivent exécuter des fonctions lourdes et répétitives dues au traitement d'images ou du signal. Il s'agit généralement de traitements successifs sur des matrices, utilisant des boucles imbriquées, qui si elles sont exécutées sur le processeur générique embarqué, l'alourdissent et le ralentissent. La synthèse de haut-niveau (HLS pour High-Level Synthesis), a pour but d'obtenir à partir d'un algorithme décrit dans un langage de programmation de haut-niveau (C, Matlab...) une description matérielle d'une puce exécutant de manière spécique cet algorithme. Le partage de ressources s'intéresse au sein de la HLS à la réduction de la surface de silicium utilisée par un circuit, et donc de son coût, par la réutilisation de composants matériels. C'est dans le cadre de la HLS que se situe ce stage, dont le but était d'obtenir un outil de conception automatique de circuits dédiés pour des algorithmes contenant des boucles imbriquées, permettant de partager les ressources matérielles au sein de ces circuits pour en diminuer le coût et la taille."
    ],
    "authFullName_s": [
      "Clément Guy"
    ],
    "halId_s": "dumas-00530700",
    "producedDateY_i": 2010,
    "texte_nettoye": "Le marché du système embarqué est le théâtre d'une véritable course à tous les niveaux (performance, miniaturisation, consommation électrique). En effet que ce soit pour le multimédia portable (consoles portables, appareils photos et caméras numériques...) ou pour les télécoms (émetteurs-récepteurs et téléphones 3 et bientôt 4G), la très forte concurrence oblige les systèmes à être de plus en plus performants (meilleure résolution, meilleure transmission...) et à avoir une plus grande autonomie, tout en conservant, voir en diminuant, leur taille. La plupart de ces systèmes embarqués ont un point commun : ils doivent exécuter des fonctions lourdes et répétitives dues au traitement d'images ou du signal. Il s'agit généralement de traitements successifs sur des matrices, utilisant des boucles imbriquées, qui si elles sont exécutées sur le processeur générique embarqué, l'alourdissent et le ralentissent. La synthèse de haut-niveau (HLS pour High-Level Synthesis), a pour but d'obtenir à partir d'un algorithme décrit dans un langage de programmation de haut-niveau (C, Matlab...) une description matérielle d'une puce exécutant de manière spécique cet algorithme. Le partage de ressources s'intéresse au sein de la HLS à la réduction de la surface de silicium utilisée par un circuit, et donc de son coût, par la réutilisation de composants matériels. C'est dans le cadre de la HLS que se situe ce stage, dont le but était d'obtenir un outil de conception automatique de circuits dédiés pour des algorithmes contenant des boucles imbriquées, permettant de partager les ressources matérielles au sein de ces circuits pour en diminuer le coût et la taille."
  },
  {
    "title_s": [
      "Réalisation d’un système Q&A spécialisé dans la qualité des eaux"
    ],
    "keyword_s": [
      "Système Q&A",
      "Qualité des eaux"
    ],
    "abstract_s": [
      "Le BRGM, Service géologique national, est l'établissement public de référence dans les applications des sciences de la Terre pour gérer les ressources et les risques du sol et du sous-sol. Son action est orientée vers la recherche scientifique, l'appui aux politiques publiques et la coopération internationale. Le BRGM développe notamment depuis plusieurs années une expertise reconnue à l’internationale des systèmes d’information dédiés à l’environnement, en particulier sur le domaine de l’eau, du site web portail de référence aux API web de diffusion des données environnementales, en passant par l’indexation de grands volumes de données et la modélisation prédictive. Parmis les outils developés par le BRGM, le portail Hub'eau qui comporte une dizaine d'API Rest chacune spécialisé dans un domaine en relation avec l'eau (qualité des eaux souterraines, rivières, poisson, etc). Pour permettre aux usagers intéressés par les problématiques environnementales de bénéficier des informations que peuvent offrir ces données, nous proposons de concevoir les premières briques d’un service numérique dont la fonction sera de répondre aux questions des utilisateurs, posées en langage naturel, de façon contextualisée et en se basant sur les données réelles du portail Hub'eau. Dans ce rapport nous commençons par une brève description de l'état de l'art en matière de chatbots et de systèmes questions-réponses. Par la suite, nous décrirons l'outil Hub'eau développé par le BRGM, nous aborderons les détails d'une des API du portail en précisant la procédure d'interrogation et le format des données. Nous poursuivrons par la description de la démarche entreprise pour le développement du système décrit ci-dessus. Nous terminerons par la présentation des résultats auxquels nous avons pu aboutir."
    ],
    "authFullName_s": [
      "Maya Touzari"
    ],
    "halId_s": "hal-03523094",
    "producedDateY_i": 2021,
    "texte_nettoye": "Le BRGM, Service géologique national, est l'établissement public de référence dans les applications des sciences de la Terre pour gérer les ressources et les risques du sol et du sous-sol. Son action est orientée vers la recherche scientifique, l'appui aux politiques publiques et la coopération internationale. Le BRGM développe notamment depuis plusieurs années une expertise reconnue à l’internationale des systèmes d’information dédiés à l’environnement, en particulier sur le domaine de l’eau, du site web portail de référence aux API web de diffusion des données environnementales, en passant par l’indexation de grands volumes de données et la modélisation prédictive. Parmis les outils developés par le BRGM, le portail Hub'eau qui comporte une dizaine d'API Rest chacune spécialisé dans un domaine en relation avec l'eau (qualité des eaux souterraines, rivières, poisson, etc). Pour permettre aux usagers intéressés par les problématiques environnementales de bénéficier des informations que peuvent offrir ces données, nous proposons de concevoir les premières briques d’un service numérique dont la fonction sera de répondre aux questions des utilisateurs, posées en langage naturel, de façon contextualisée et en se basant sur les données réelles du portail Hub'eau. Dans ce rapport nous commençons par une brève description de l'état de l'art en matière de chatbots et de systèmes questions-réponses. Par la suite, nous décrirons l'outil Hub'eau développé par le BRGM, nous aborderons les détails d'une des API du portail en précisant la procédure d'interrogation et le format des données. Nous poursuivrons par la description de la démarche entreprise pour le développement du système décrit ci-dessus. Nous terminerons par la présentation des résultats auxquels nous avons pu aboutir."
  },
  {
    "title_s": [
      "Impact sur la survie globale de la dysthyroïdie sous inhibiteur de checkpoint immunitaire"
    ],
    "keyword_s": [
      "Dysthyroïdie",
      "Cancer solide",
      "Inhibiteur de checkpoint immunitaire"
    ],
    "abstract_s": [
      "Contexte : le traitement médical des cancers solides a radicalement changé depuis le développement des inhibiteurs de checkpoint immunitaire (ICI). Cependant, les effets indésirables (EI) immuno-induits sont un défi dans la pratique courante. La dysthyroïdie est l’EI endocrinien le plus fréquent et certaines séries suggèrent que la dysthyroïdie pourrait être associée à l'efficacité des ICI. Ceci nous a conduit à explorer l'association entre la dysthyroïdie induite par les ICI et la survie globale (SG) dans une grande cohorte de patients atteints de tumeurs solides en utilisant l'exploration des données des dossiers patients informatisés (DPI). Patients et méthodes : ConSoRe est un outil d'analyse de données de nouvelle génération utilisant le Traitement Automatique du Langage (TAL) pour rechercher des données agrégées et effectuer une exploration de données avancée. Cet outil a été utilisé pour extraire des DPI les données des patients traités par ICI pour un cancer solide à l'Institut Paoli-Calmettes (Centre anticancéreux de Marseille, France). Les analyses de survie ont été réalisées par la méthode de Kaplan-Meier et comparées à l'aide du test log-rank (package R survminer). Une analyse landmark a été utilisée pour prendre en compte le biais d’immortalité. Dans l'analyse univariée et multivariée, un modèle de Cox à risques proportionnels a été utilisé pour estimer les variables associées à la SG. Le hazard Ratio (HR) a été estimé avec un intervalle de confiance à 95%. Deux modèles ont été réalisés, un modèle de Cox dit dépendant du temps et un modèle de Cox dit indépendant du temps. Résultats : l'extraction des données a permis d'identifier 1 385 patients traités par ICI entre 2011 et 2021. La survenue d’une dysthyroïdie a été observée chez 90 pts (7%). La dysthyroïdie a été associée à une meilleure SG (HR=0,46, 95%CI 0,33-0,65, p<0,001) avec une médiane de SG de 35,3 mois (mo) chez les patients atteints de dysthyroïdie (groupe DT) contre 15,4 mo chez les patients sans dysthyroïdie (groupe NDT). Lorsqu’un landmark à 6 mois était appliqué, l’impact de la dysthyroïdie sur la survie était similaire avec une médiane de SG passant de 25,5 mo (95%CI 22,8-27,8) dans le groupe NDT à 36,7 mo (95%CI 29,4-NR) dans le groupe DT. Dans l’analyse multivariée, la dysthyroïdie était indépendamment associée à une meilleure SG (HR=0,49, 95%CI 0,35-0,69, p=0,001). Après ajustement dans le modèle de Cox dépendant du temps, cette association était toujours significative (aHR=0,64 95%CI 0,45-0,90 p=0,010). Conclusion : la dysthyroïdie survenue chez les patients sous ICI était associées à une meilleure SG même après prise en compte du biais d'immortalité. L'apparition de la dysthyroïdie pourrait aider les oncologues à détecter les patients les plus susceptibles de bénéficier de l'ICI."
    ],
    "authFullName_s": [
      "Mathilde Beaufils"
    ],
    "halId_s": "dumas-03696996",
    "producedDateY_i": 2022,
    "texte_nettoye": "Contexte : le traitement médical des cancers solides a radicalement changé depuis le développement des inhibiteurs de checkpoint immunitaire (ICI). Cependant, les effets indésirables (EI) immuno-induits sont un défi dans la pratique courante. La dysthyroïdie est l’EI endocrinien le plus fréquent et certaines séries suggèrent que la dysthyroïdie pourrait être associée à l'efficacité des ICI. Ceci nous a conduit à explorer l'association entre la dysthyroïdie induite par les ICI et la survie globale (SG) dans une grande cohorte de patients atteints de tumeurs solides en utilisant l'exploration des données des dossiers patients informatisés (DPI). Patients et méthodes : ConSoRe est un outil d'analyse de données de nouvelle génération utilisant le Traitement Automatique du Langage (TAL) pour rechercher des données agrégées et effectuer une exploration de données avancée. Cet outil a été utilisé pour extraire des DPI les données des patients traités par ICI pour un cancer solide à l'Institut Paoli-Calmettes (Centre anticancéreux de Marseille, France). Les analyses de survie ont été réalisées par la méthode de Kaplan-Meier et comparées à l'aide du test log-rank (package R survminer). Une analyse landmark a été utilisée pour prendre en compte le biais d’immortalité. Dans l'analyse univariée et multivariée, un modèle de Cox à risques proportionnels a été utilisé pour estimer les variables associées à la SG. Le hazard Ratio (HR) a été estimé avec un intervalle de confiance à 95%. Deux modèles ont été réalisés, un modèle de Cox dit dépendant du temps et un modèle de Cox dit indépendant du temps. Résultats : l'extraction des données a permis d'identifier 1 385 patients traités par ICI entre 2011 et 2021. La survenue d’une dysthyroïdie a été observée chez 90 pts (7%). La dysthyroïdie a été associée à une meilleure SG (HR=0,46, 95%CI 0,33-0,65, p<0,001) avec une médiane de SG de 35,3 mois (mo) chez les patients atteints de dysthyroïdie (groupe DT) contre 15,4 mo chez les patients sans dysthyroïdie (groupe NDT). Lorsqu’un landmark à 6 mois était appliqué, l’impact de la dysthyroïdie sur la survie était similaire avec une médiane de SG passant de 25,5 mo (95%CI 22,8-27,8) dans le groupe NDT à 36,7 mo (95%CI 29,4-NR) dans le groupe DT. Dans l’analyse multivariée, la dysthyroïdie était indépendamment associée à une meilleure SG (HR=0,49, 95%CI 0,35-0,69, p=0,001). Après ajustement dans le modèle de Cox dépendant du temps, cette association était toujours significative (aHR=0,64 95%CI 0,45-0,90 p=0,010). Conclusion : la dysthyroïdie survenue chez les patients sous ICI était associées à une meilleure SG même après prise en compte du biais d'immortalité. L'apparition de la dysthyroïdie pourrait aider les oncologues à détecter les patients les plus susceptibles de bénéficier de l'ICI."
  },
  {
    "title_s": [
      "Utiliser un entrepôt de données de santé pour reproduire les résultats d’une cohorte vaccinale COVID-19 chez des patients atteints d’hypogammaglobulinémie",
      "Using a health data warehouse to recreate a COVID-19 vaccine cohort in patients with chronic disease"
    ],
    "keyword_s": [
      "Natural language processing",
      "Hypogammaglobulinemia",
      "Vaccination",
      "COVID-19",
      "Data Warehousing",
      "Hypogammaglobulinémie",
      "Vaccination",
      "COVID-19",
      "Traitement du langage naturel",
      "Entreposage de données"
    ],
    "abstract_s": [
      "Contexte : dans les situations où des investigations urgentes sont nécessaires, il est coûteux en temps et en argent de créer et de suivre des cohortes de patients. L’utilisation d’un entrepôt de données de santé (EDS) est envisageable pour faciliter la création de cohortes. Méthodes : à partir des données structurées et non structurées de l’EDS du CHU de Bordeaux, nous avons reconstitué une cohorte vaccinale COVID-19 de patients ayant une hypogammaglobulinémie primitive ou secondaire, vaccinés contre le COVID-19 en 2021 et avec des résultats de sérologie vaccinale dans les 12 premiers mois suivant cette vaccination. Nous avons développé un algorithme de traitement automatique de la langue pour rechercher la vaccination COVID-19 et les résultats de sérologie dans les documents textuels du dossier médical. Nous avons décrit la réponse humorale à la vaccination dans cette population à 1, 6, 12 et 24 mois après la deuxième dose de vaccin à partir des titres d’anticorps anti-Spike. Résultats : la requête initiale a permis d’identifier 2 065 patients éligibles à l’inclusion. Avec les méthodes employées et les données disponibles, nous avons inclus 258 patients dans l’analyse finale. L’âge médian des patients était de 67 ans. Le sex-ratio était équilibré avec 52,7 % d’hommes. Parmi les patients avec un résultat de sérologie, 63,6 % (63/99) étaient répondeurs à M1, 74,4 % (134/180) étaient répondeurs à M6 et 92,4 % (159/172) étaient répondeurs à M12. Conclusion : nous avons créé une cohorte vaccinale à partir d’un EDS. La réponse humorale à la vaccination COVID-19 est diminuée chez les patients ayant une hypogammaglobulinémie.",
      "Background: in situations where urgent investigations are required, it is costly and time consuming to create and monitor cohorts. Using a health data warehouse (HDW) could support the creation of cohorts. Methods : using structured and unstructured data from the HDW of Bordeaux, we reconstituted a COVID-19 vaccine cohort of patients with primary or secondary hypogammaglobulinemia, vaccinated against COVID-19 in 2021 and with serological response results within the first 12 months following the vaccination. We developed a natural language processing algorithm to detect COVID-19 vaccination and serological results in medical records text documents. We described the humoral response to vaccination in this population at 1, 6, 12 and 24 months after the second dose of vaccine based on anti-Spike antibody titres. Results: the initial query identified 2 065 patients eligible for inclusion. With the methods used and the available data, we included 258 patients in the final analysis. The median age of the patients was 67 years and 52.7 % were men. Among patients with a serological result, 63.6% (63/99) were responders at M1, 74.4% (134/180) were responders at M6 and 92.4% (159/172) were responders at M12. Conclusion: we created a vaccination cohort from a HDW. The humoral response to COVID-19 vaccination is reduced in patients with hypogammaglobulinemia."
    ],
    "authFullName_s": [
      "Adam Loffler"
    ],
    "halId_s": "dumas-04814609",
    "producedDateY_i": 2024,
    "texte_nettoye": "Contexte : dans les situations où des investigations urgentes sont nécessaires, il est coûteux en temps et en argent de créer et de suivre des cohortes de patients. L’utilisation d’un entrepôt de données de santé (EDS) est envisageable pour faciliter la création de cohortes. Méthodes : à partir des données structurées et non structurées de l’EDS du CHU de Bordeaux, nous avons reconstitué une cohorte vaccinale COVID-19 de patients ayant une hypogammaglobulinémie primitive ou secondaire, vaccinés contre le COVID-19 en 2021 et avec des résultats de sérologie vaccinale dans les 12 premiers mois suivant cette vaccination. Nous avons développé un algorithme de traitement automatique de la langue pour rechercher la vaccination COVID-19 et les résultats de sérologie dans les documents textuels du dossier médical. Nous avons décrit la réponse humorale à la vaccination dans cette population à 1, 6, 12 et 24 mois après la deuxième dose de vaccin à partir des titres d’anticorps anti-Spike. Résultats : la requête initiale a permis d’identifier 2 065 patients éligibles à l’inclusion. Avec les méthodes employées et les données disponibles, nous avons inclus 258 patients dans l’analyse finale. L’âge médian des patients était de 67 ans. Le sex-ratio était équilibré avec 52,7 % d’hommes. Parmi les patients avec un résultat de sérologie, 63,6 % (63/99) étaient répondeurs à M1, 74,4 % (134/180) étaient répondeurs à M6 et 92,4 % (159/172) étaient répondeurs à M12. Conclusion : nous avons créé une cohorte vaccinale à partir d’un EDS. La réponse humorale à la vaccination COVID-19 est diminuée chez les patients ayant une hypogammaglobulinémie."
  },
  {
    "title_s": [
      "Développement d’un outil de segmentation et de classification semi-automatique de pierres"
    ],
    "keyword_s": [
      "Digital image processing",
      "Segmentation",
      "Edge detection",
      "Stones",
      "Python programmation",
      "Graphical interface",
      "Supervised classification",
      "Interface graphique",
      "Classification supervisée",
      "Programmation Python",
      "Pierres",
      "Détection de contour",
      "Traitement numérique des images"
    ],
    "abstract_s": [
      "Le travail des archéologues pour étudier les étapes de construction et de reconstruction des façades est aujourd’hui fastidieux entre autre parce qu’ils doivent, à la main, dessiner et qualifier les pierres des murs. Afin de résoudre ce problème, nous avons développé, en collaboration avec le CAPRA, un ensemble d’outils. Une première partie, liée à la détection des contours, est de programmer, dans le langage Python, une interface graphique et d’y implémenter notre algorithme de segmentation. La seconde partie, concernant la classification supervisée du résultat automatique, utilise une succession de deux outils intégrés à l’interface d’ArcMap pour générer automatiquement la classification de ces pierres à partir de l’image segmentée. Les outils se voulant semi-automatiques, l’utilisateur peut modifier à chaque fois les résultats produits. Cependant, des limitations sont bien présentes et particulièrement concernant l’algorithme de segmentation qui n’est pas complètement ajusté à la détection des pierres sur des murs dégradés par le temps et la nature, ou dont les joints ne sont pas propres.",
      "Today, the work of archaeologists to study the steps of construction and reconstruction of facades is boring because they have to draw and qualify stones of walls. In order to solve this problem, we developed a set of tools. The first part, related to edge detection, is to program, using the informatic language Python, a graphical user interface and implementing our segmentation algorithm in it. The second part, concerning the supervised classification of our automatic result, uses a succession of two tools integrated in the software ArcMap to generate automatically the classification of these stones from the segmented image. Tools aiming to be semi-automatic, the user can modify every time the produced results. However, limitations are always present and particularly concerning the segmentation algorithm which is not completely adjusted to the detection of stones on degraded walls by weather or nature or whose joints are not clean."
    ],
    "authFullName_s": [
      "Pierre-Alban 6-02-1992 Hugueny"
    ],
    "halId_s": "dumas-01168266",
    "producedDateY_i": 2014,
    "texte_nettoye": "Le travail des archéologues pour étudier les étapes de construction et de reconstruction des façades est aujourd’hui fastidieux entre autre parce qu’ils doivent, à la main, dessiner et qualifier les pierres des murs. Afin de résoudre ce problème, nous avons développé, en collaboration avec le CAPRA, un ensemble d’outils. Une première partie, liée à la détection des contours, est de programmer, dans le langage Python, une interface graphique et d’y implémenter notre algorithme de segmentation. La seconde partie, concernant la classification supervisée du résultat automatique, utilise une succession de deux outils intégrés à l’interface d’ArcMap pour générer automatiquement la classification de ces pierres à partir de l’image segmentée. Les outils se voulant semi-automatiques, l’utilisateur peut modifier à chaque fois les résultats produits. Cependant, des limitations sont bien présentes et particulièrement concernant l’algorithme de segmentation qui n’est pas complètement ajusté à la détection des pierres sur des murs dégradés par le temps et la nature, ou dont les joints ne sont pas propres."
  },
  {
    "title_s": [
      "Apport des entrepôts de données de santé dans l’identification des besoins en soins palliatifs non repérés : une étude de la portée"
    ],
    "keyword_s": [
      "Soins palliatifs",
      "Médecine générale",
      "Entreposage de données",
      "Examen de la portée"
    ],
    "abstract_s": [
      "Introduction : Les soins palliatifs offrent une approche globale centrée sur le patient pour améliorer sa qualité de vie. En première ligne, le médecin généraliste joue un rôle central de repérage, de coordination et de soutien, mais fait face à des difficultés liées à la sous-détection des symptômes et à l’incertitude. Dans ce contexte, les entrepôts de données de santé (EDS) offrent de nouvelles perspectives pour repérer les besoins non identifiés en soins palliatifs. Cette étude vise à dresser un état des lieux de l’usage de ces données pour identifier les besoins en soins palliatifs non repérés. Matériel et méthode : Cette scoping review, conduite selon les recommandations PRISMA-ScR 2019, a inclus des études quantitatives utilisant des données d’entrepôts de santé pour identifier des adultes atteints de maladies potentiellement mortelles. La sélection, l’extraction et l’analyse des données ont été réalisées de manière rigoureuse et indépendante par deux investigateurs, à partir d’une recherche systématique sur plusieurs bases de données effectuée jusqu’en décembre 2024. Résultats : Les 49 études incluses portaient majoritairement sur des patients hospitalisés ou atteints de pathologies oncologiques, avec des effectifs très variables. La majorité utilisait des entrepôts hospitaliers, des données structurées et, plus rarement, des données non structurées ou des résultats rapportés par les patients (PRO). Les objectifs étaient principalement l’identification du risque de décès ou de symptômes, avec des modèles basés sur l’apprentissage automatique, les scores cliniques ou le traitement du langage naturel pour les données non structurées. Discussion : Les modèles se développent principalement en milieu hospitalier, alors que l’ambulatoire et les maladies chroniques restent peu représentés et difficiles à modéliser. Les ePRO améliorent la détection des symptômes et la qualité des échanges, mais leur intégration reste limitée par des contraintes techniques et numériques. Les données non structurées, issues des notes cliniques ou comptes rendus, enrichissent les modèles prédictifs, notamment via le NLP bien que leur qualité varie selon les pratiques et logiciels. Conclusion : Les résultats montrent que les EDS permettent de repérer les besoins palliatifs non identifiés mais des études sur la population ambulatoire, les maladies chroniques et les données non structurées sont nécessaires."
    ],
    "authFullName_s": [
      "Hugo Métaireau"
    ],
    "halId_s": "dumas-05481376",
    "producedDateY_i": 2025,
    "texte_nettoye": "Introduction : Les soins palliatifs offrent une approche globale centrée sur le patient pour améliorer sa qualité de vie. En première ligne, le médecin généraliste joue un rôle central de repérage, de coordination et de soutien, mais fait face à des difficultés liées à la sous-détection des symptômes et à l’incertitude. Dans ce contexte, les entrepôts de données de santé (EDS) offrent de nouvelles perspectives pour repérer les besoins non identifiés en soins palliatifs. Cette étude vise à dresser un état des lieux de l’usage de ces données pour identifier les besoins en soins palliatifs non repérés. Matériel et méthode : Cette scoping review, conduite selon les recommandations PRISMA-ScR 2019, a inclus des études quantitatives utilisant des données d’entrepôts de santé pour identifier des adultes atteints de maladies potentiellement mortelles. La sélection, l’extraction et l’analyse des données ont été réalisées de manière rigoureuse et indépendante par deux investigateurs, à partir d’une recherche systématique sur plusieurs bases de données effectuée jusqu’en décembre 2024. Résultats : Les 49 études incluses portaient majoritairement sur des patients hospitalisés ou atteints de pathologies oncologiques, avec des effectifs très variables. La majorité utilisait des entrepôts hospitaliers, des données structurées et, plus rarement, des données non structurées ou des résultats rapportés par les patients (PRO). Les objectifs étaient principalement l’identification du risque de décès ou de symptômes, avec des modèles basés sur l’apprentissage automatique, les scores cliniques ou le traitement du langage naturel pour les données non structurées. Discussion : Les modèles se développent principalement en milieu hospitalier, alors que l’ambulatoire et les maladies chroniques restent peu représentés et difficiles à modéliser. Les ePRO améliorent la détection des symptômes et la qualité des échanges, mais leur intégration reste limitée par des contraintes techniques et numériques. Les données non structurées, issues des notes cliniques ou comptes rendus, enrichissent les modèles prédictifs, notamment via le NLP bien que leur qualité varie selon les pratiques et logiciels. Conclusion : Les résultats montrent que les EDS permettent de repérer les besoins palliatifs non identifiés mais des études sur la population ambulatoire, les maladies chroniques et les données non structurées sont nécessaires."
  },
  {
    "title_s": [
      "A Study about Explainability and Fairness in Machine Learning and Knowledge Discovery"
    ],
    "keyword_s": [
      "Machine learning",
      "Explainability",
      "Fairness",
      "Antibiotic classification",
      "Machine learning",
      "Explicabilité",
      "Fairness",
      "Classification antibiotique"
    ],
    "abstract_s": [
      "Progress in machine learning, more recently accelerated by deep learning, now makes possible to build high-performance models for recognition or generation tasks. However, these models are complex, making it difficult to justify their predictions. To avoid a \"black box\" effect and for an interpretability reason, the Orpailleur team is interested in model explanations, a research field that is still recent. These explainers manage to spot biased predictions of some models (e.g., decisions mainly based on skin color). The discoveries of the Orpailleur team show that it is possible to make predictions fairer (we generally talk about \"fairness\") thanks to ensemble methods and variable dropping, without altering the models performance. We particularly focus here on the specific case of textual data, as well as on an adaptation to deep learning models which hold a prominent place in natural language processing. Secondly, we will use these explainers on antibiotic classification models in order to determine how such a classifier reaches the conclusion that a molecule has antibiotic properties. This work will lead to the creation of an interactive web interface to highlight the important patterns learned by these models, through visualizations.",
      "Les progrès en machine learning, plus récemment accélérés par le deep learning, permettent désormais de construire des modèles performants dans des tâches de reconnaissance ou de génération. Ces modèles sont cependant complexes, si bien qu'il devient difficile de justifier leurs prédictions. Pour éviter un effet « boîte noire » et dans un soucis d'interprétabilité, l'équipe Orpailleur s'intéresse aux explicateurs de modèles, un domaine de recherche encore récent. Ces explicateurs permettent, entre autres, de mettre en lumière les prédictions biaisées de certains modèles (exemple : décision principalement basée sur la couleur de peau). Les découvertes de l'équipe Orpailleur montrent qu'il est possible de rendre les prédictions plus justes (on parle généralement de « fairness ») grâce à des méthodes ensemblistes et de suppression de variables, sans altérer les performances des modèles. Nous nous intéressons plus particulièrement ici au cas spécifique des données textuelles, ainsi qu'à une adaptation aux modèles de deep learning qui occupent une place de premier choix dans le traitement automatique du langage. Dans un second temps, nous utilisons ces explicateurs sur des modèles de classification d'antibiotiques afin de déterminer comment un tel classificateur parvient à la conclusion qu'une molécule possède des propriétés antibiotiques. Ces travaux conduiront à la création d'une interface web interactive permettant de mettre en valeur les motifs importants appris par ces modèles, au travers de visualisations."
    ],
    "authFullName_s": [
      "Fabien Bernier"
    ],
    "halId_s": "hal-03371070",
    "producedDateY_i": 2021,
    "texte_nettoye": "Progress in machine learning, more recently accelerated by deep learning, now makes possible to build high-performance models for recognition or generation tasks. However, these models are complex, making it difficult to justify their predictions. To avoid a \"black box\" effect and for an interpretability reason, the Orpailleur team is interested in model explanations, a research field that is still recent. These explainers manage to spot biased predictions of some models (e.g., decisions mainly based on skin color). The discoveries of the Orpailleur team show that it is possible to make predictions fairer (we generally talk about \"fairness\") thanks to ensemble methods and variable dropping, without altering the models performance. We particularly focus here on the specific case of textual data, as well as on an adaptation to deep learning models which hold a prominent place in natural language processing. Secondly, we will use these explainers on antibiotic classification models in order to determine how such a classifier reaches the conclusion that a molecule has antibiotic properties. This work will lead to the creation of an interactive web interface to highlight the important patterns learned by these models, through visualizations."
  },
  {
    "title_s": [
      "Intérêt de l'intelligence artificielle pour l'extraction automatisée des patients présentant une douleur thoracique notifiée dans le dossier de régulation médicale du SAMU 33",
      "Interest of artificial intelligence for the automated extraction of patients with chest pain notified in the medical regulation file of the EMS 33"
    ],
    "keyword_s": [
      "Myocardial infarction",
      "Health",
      "EMS",
      "Emergency",
      "Classification",
      "Algorithm",
      "Deep learning",
      "Artificial intelligence",
      "Chest pain",
      "Medical regulation",
      "Santé",
      "SAMU",
      "Urgences",
      "Classification",
      "Algorithme",
      "Apprentissage profond",
      "Infarctus du myocarde",
      "Régulation médicale",
      "Intelligence artificielle",
      "Douleur thoracique"
    ],
    "abstract_s": [
      "Introduction : la douleur thoracique correspond à l’un des motifs fréquents de recours au centre 15. Depuis quelques années, l’intelligence artificielle s’ancre progressivement dans le milieu de la santé dans un but d’améliorer les performances et les prises de décisions des médecins. L’objectif principal de ce travail de thèse était d’évaluer les performances d’un modèle de classification type « transformer » dans l’extraction automatisée des informations contenues dans un dossier de régulation médicale. Méthode : ce travail a été mené sur le CHU de Bordeaux. Un échantillon de l’ensemble des dossiers de 2009 à 2021, a été soumis à un transformer correspondant à un logiciel de traitement automatique du langage. Après un entrainement préalable, la précision du modèle a été obtenue par comparaison de la prédiction faite du modèle versus l’annotation individuelle (gold standard) de 1000 dossiers. Résultat : 2237 dossiers ont été utilisés pour l’entrainement supervisé du modèle. Au décours, la comparaison des prédictions par le modèle sur 1000 nouveaux dossiers versus notre propre analyse personnelle, a permis d’objectiver une sensibilité de 0,90 (95% IC, 0,88-0,91) et une spécificité de 0,98 (95% IC, 0,97-0,98). La précision du modèle était de 0,99 (95% IC, 0,98-0,99) pour le score « autre » et de 0,85 (95% IC, 0,78-0,92) pour le score « douleur thoracique ». L’application du modèle sur l’ensemble des données a permis d’isoler 360 000 douleurs thoraciques (soit 9% des données) avec des indices de confiance très corrects. Discussion : l’étude révèle une nette performance du transformer dans l’extraction des douleurs thoraciques au sein des dossiers de régulation. Ce projet pourrait rendre exploitable la base de données non structurées du SAMU 33 à des fins épidémiologiques. La liberté d’écriture du médecin régulateur représente une des limites de l’étude. La validité du gold standard peut poser problème. Les similitudes syntaxiques, la négation et la saisie des présentations atypiques restent une voie d’apprentissage. L’approche de l’intelligence artificielle dans ce secteur représente une véritable voie d’avenir.",
      "Introduction: chest pain is one of the most frequent reasons for calling the 15 center. In recent years, artificial intelligence has been progressively introduced in the health sector with the aim of improving the performance and decision making of physicians. The main objective of this thesis work was to evaluate the performance of a \"transformer\" type classification model in the automated classification of information contained in a medical regulation file. Method: this study was conducted in the Bordeaux University Hospital. A sample of all the files from 2009 to 2021 was subjected to a transformer corresponding to an automatic language processing software. After prior training, the accuracy of the model was obtained by comparing the prediction made by the model with the individual annotation (gold standard) of 1000 records. Result: 2237 records were used for supervised training of the model. In the course of this, the comparison of the model's predictions on 1000 new records versus our own personal analysis resulted in a sensitivity of 0.90 (95% CI, 0,88-0,91) and a specificity of 0.98 (95% CI, 0,97-0,98). The accuracy was 0.99 (95% CI, 0,98-0,99) for the \"other\" score and 0.85 (95% CI, 0,78-0,92) for the \"chest pain\" score. Applying the model to the entire data set allowed us to isolate 360,000 chest pains (i.e. 9% of the data) with very good confidence indices. Discussion: the study shows a clear performance of the transformer in the extraction of chest pain from the regulation files. This project could make the unstructured database of the EMS 33 usable for epidemiological purposes. The freedom of writing of the regulating doctor represents one of the limits of the study. The validity of the gold standard can pose a problem. Syntactic similarities, negation and the capture of atypical presentations remain a learning curve. The artificial intelligence approach in this area is a real way forward."
    ],
    "authFullName_s": [
      "Guillaume Gnyp"
    ],
    "halId_s": "dumas-03849849",
    "producedDateY_i": 2022,
    "texte_nettoye": "Introduction : la douleur thoracique correspond à l’un des motifs fréquents de recours au centre 15. Depuis quelques années, l’intelligence artificielle s’ancre progressivement dans le milieu de la santé dans un but d’améliorer les performances et les prises de décisions des médecins. L’objectif principal de ce travail de thèse était d’évaluer les performances d’un modèle de classification type « transformer » dans l’extraction automatisée des informations contenues dans un dossier de régulation médicale. Méthode : ce travail a été mené sur le CHU de Bordeaux. Un échantillon de l’ensemble des dossiers de 2009 à 2021, a été soumis à un transformer correspondant à un logiciel de traitement automatique du langage. Après un entrainement préalable, la précision du modèle a été obtenue par comparaison de la prédiction faite du modèle versus l’annotation individuelle (gold standard) de 1000 dossiers. Résultat : 2237 dossiers ont été utilisés pour l’entrainement supervisé du modèle. Au décours, la comparaison des prédictions par le modèle sur 1000 nouveaux dossiers versus notre propre analyse personnelle, a permis d’objectiver une sensibilité de 0,90 (95% IC, 0,88-0,91) et une spécificité de 0,98 (95% IC, 0,97-0,98). La précision du modèle était de 0,99 (95% IC, 0,98-0,99) pour le score « autre » et de 0,85 (95% IC, 0,78-0,92) pour le score « douleur thoracique ». L’application du modèle sur l’ensemble des données a permis d’isoler 360 000 douleurs thoraciques (soit 9% des données) avec des indices de confiance très corrects. Discussion : l’étude révèle une nette performance du transformer dans l’extraction des douleurs thoraciques au sein des dossiers de régulation. Ce projet pourrait rendre exploitable la base de données non structurées du SAMU 33 à des fins épidémiologiques. La liberté d’écriture du médecin régulateur représente une des limites de l’étude. La validité du gold standard peut poser problème. Les similitudes syntaxiques, la négation et la saisie des présentations atypiques restent une voie d’apprentissage. L’approche de l’intelligence artificielle dans ce secteur représente une véritable voie d’avenir."
  }
]